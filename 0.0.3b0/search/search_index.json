{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>FRAM data package is a part of FRAM modelling framework developed by the Norwegian Water Resources and Energy Directorate (NVE). The package is responsible for connection between the database and the FRAM core package. It contains NVEs implementation of the Populator() class that populates the core model by translating the data from NVEs database into generic components so that the data can be further manipulated and sent to the power market model. </p> <p></p>"},{"location":"#installation","title":"Installation","text":"<p>To install the data package, run <code>pip install fram-data</code>. But we recommend  that you rather start by installing our simple demo to better understand how FRAM works.</p>"},{"location":"#nves-database","title":"NVEs database","text":"<p>NVEs database is a folder with data files with folder structure, file format and table format defined by NVE. See this database for the example. NVEs database is model-neutral meaning that the same data can be used to build different energy market models.</p> <p>FRAM is designed so that different users can use their own databses with the format that suits them best.</p>"},{"location":"#nves-populator","title":"NVEs populator","text":"<p>NVEs implementation of the populator supports file formats and table formats in NVEs database. Populator is implementes using the populator interface defined in FRAM core. </p> <p>If you want to use your own database together with FRAM, you should create your own implementation of the populator. Use this package as an example.</p>"},{"location":"reference/","title":"Code Reference","text":""},{"location":"reference/#framdata","title":"<code>framdata</code>","text":""},{"location":"reference/#framdata.database_names","title":"<code>database_names</code>","text":""},{"location":"reference/#framdata.database_names.DatabaseNames","title":"<code>DatabaseNames</code>","text":"<p>Container for names and locations of files and folders in the NVE database.</p>"},{"location":"reference/#framdata.database_names.DatabaseNames.DatabaseNames","title":"<code>DatabaseNames</code>","text":"<p>               Bases: <code>Base</code></p> <p>Define names of files and folders in the NVE database and map files to folders.</p> Source code in <code>framdata/database_names/DatabaseNames.py</code> <pre><code>class DatabaseNames(Base):\n    \"\"\"Define names of files and folders in the NVE database and map files to folders.\"\"\"\n\n    # ---------- FILE EXTENSIONS ---------- #\n    ext_excel = \".xlsx\"\n    ext_h5 = \".h5\"\n    ext_parquet = \".parquet\"\n    ext_yaml = \".yaml\"\n\n    # ---------- SHEETS ---------- #\n    data_sheet = \"Data\"\n    metadata_sheet = \"Metadata\"\n\n    # ---------- SUFFIXES ---------- #\n    capacity = \".capacity\"\n    prices = \".prices\"\n    profiles = \".profiles\"\n    curves = \".curves\"\n\n    # ---------- DATABASE FOLDERs MAP ---------- #\n    db00 = \"db00_nodes\"\n    db01 = \"db01_nodes_profiles\"\n    db10 = \"db10_wind_solar\"\n    db11 = \"db11_wind_solar_profiles\"\n    db20 = \"db20_hydropower\"\n    db21 = \"db21_hydropower_profiles\"\n    db22 = \"db22_hydropower_curves\"\n    db30 = \"db30_thermal\"\n    db31 = \"db31_thermal_profiles\"\n    db40 = \"db40_demand\"\n    db41 = \"db41_demand_profiles\"\n    db50 = \"db50_transmission\"\n    db51 = \"db51_transmission_profiles\"\n\n    db_folder_list: ClassVar[list] = [db00, db01, db10, db11, db20, db21, db22, db30, db31, db40, db41, db50, db51]\n\n    # ---------- FILENAMES ---------- #\n    # ==== NODES ====\n    power_nodes = \"Power.Nodes\"\n    power_nodes_prices = \"Power.Nodes.prices\"\n    power_nodes_profiles = \"Power.Nodes.profiles\"\n\n    fuel_nodes = \"Fuel.Nodes\"\n    fuel_nodes_prices = \"Fuel.Nodes.prices\"\n    fuel_nodes_profiles = \"Fuel.Nodes.profiles\"\n\n    emission_nodes = \"Emission.Nodes\"\n    emission_nodes_prices = \"Emission.Nodes.prices\"\n    emission_nodes_profiles = \"Emission.Nodes.profiles\"\n\n    # ==== THERMAL ====\n    thermal_generators = \"Thermal.Generators\"\n    thermal_generators_capacity = \"Thermal.Generators.capacity\"\n    thermal_generators_profiles = \"Thermal.Generators.profiles\"\n\n    # ==== HYDROPOWER ====\n    # hydro attribute tables\n    hydro_modules = \"Hydropower.Modules\"\n    hydro_modules_volumecapacity = \"Hydropower.Modules.VolumeCapacity\"\n    hydro_modules_enekv_global_derived = \"Hydropower.Modules.enekv_global_derived\"\n    hydro_modules_reggrad_glob_derived = \"Hydropower.Modules.reggrad_glob_derived\"\n    hydro_modules_reggrad_lok_derived = \"Hydropower.Modules.reggrad_lok_derived\"\n    hydro_bypass = \"Hydropower.Bypass\"\n    hydro_generators = \"Hydropower.Generators\"\n    hydro_inflow = \"Hydropower.Inflow\"\n    hydro_inflow_yearvolume = \"Hydropower.Inflow.YearVolume\"\n    hydro_inflow_upstream_inflow_derived = \"Hydropower.Inflow.upstream_inflow_derived\"\n    hydro_pumps = \"Hydropower.Pumps\"\n    hydro_reservoirs = \"Hydropower.Reservoirs\"\n\n    # hydro time series\n    hydro_inflow_profiles = \"Hydropower.Inflow.profiles\"\n    hydro_bypass_operationalbounds_restrictions = \"Hydropower.Bypass.OperationalBounds.Restrictions\"\n    hydro_modules_operationalbounds_restrictions = \"Hydropower.Modules.OperationalBounds.Restrictions\"\n    hydro_reservoirs_operationalbounds_restrictions = \"Hydropower.Reservoirs.OperationalBounds.Restrictions\"\n    hydro_generators_energyeq_mid = \"Hydropower.Generators.EnergyEq_mid\"\n\n    # hydro curves\n    hydro_curves = \"Hydropower.curves\"\n    hydro_pqcurves = \"Hydropower.pqcurves\"\n\n    # ==== DEMAND ====\n    demand_consumers = \"Demand.Consumers\"\n    demand_consumers_capacity = \"Demand.Consumers.capacity\"\n    demand_consumers_normalprices = \"Demand.Consumers.normalprices\"\n    demand_consumers_profiles_weatheryears = \"Demand.Consumers.profiles.weatheryears\"\n    demand_consumers_profiles_oneyear = \"Demand.Consumers.profiles\"\n\n    # ==== WIND AND SOLAR ====\n    wind_generators = \"Wind.Generators\"\n    wind_generators_capacity = \"Wind.Generators.capacity\"\n    wind_generators_profiles = \"Wind.Generators.profiles\"\n    solar_generators = \"Solar.Generators\"\n    solar_generators_capacity = \"Solar.Generators.capacity\"\n    solar_generators_profiles = \"Solar.Generators.profiles\"\n\n    # ==== Transmission ====\n    transmission_grid = \"Transmission.Grid\"\n    transmission_capacity = transmission_grid + \".capacity\"\n    transmission_loss = transmission_grid + \".loss\"\n    transmission_profiles = transmission_grid + \".profiles\"\n\n    # ---------- DATABASE FOLDER MAP ---------- #\n    db_folder_map: ClassVar[dict[str, list[str]]] = {\n        # ===: NODES ====,\n        power_nodes: db00,\n        fuel_nodes: db00,\n        emission_nodes: db00,\n        power_nodes_prices: db01,\n        fuel_nodes_prices: db01,\n        emission_nodes_prices: db01,\n        power_nodes_profiles: db01,\n        fuel_nodes_profiles: db01,\n        emission_nodes_profiles: db01,\n        # ===: HYDROPOWER ====,\n        # hydro attribute tables\n        hydro_modules: db20,\n        hydro_modules_volumecapacity: db20,\n        hydro_modules_enekv_global_derived: db20,\n        hydro_modules_reggrad_glob_derived: db20,\n        hydro_modules_reggrad_lok_derived: db20,\n        hydro_bypass: db20,\n        hydro_generators: db20,\n        hydro_inflow: db20,\n        hydro_inflow_yearvolume: db20,\n        hydro_inflow_upstream_inflow_derived: db20,\n        hydro_pumps: db20,\n        hydro_reservoirs: db20,\n        # hydro time series\n        hydro_inflow_profiles: db21,\n        hydro_bypass_operationalbounds_restrictions: db21,\n        hydro_modules_operationalbounds_restrictions: db21,\n        hydro_reservoirs_operationalbounds_restrictions: db21,\n        hydro_generators_energyeq_mid: db21,\n        # hydro curves\n        hydro_curves: db22,\n        hydro_pqcurves: db22,\n        # ===: THERMAL ====,\n        thermal_generators: db30,\n        thermal_generators_capacity: db30,\n        thermal_generators_profiles: db31,\n        # ===: DEMAND ====,\n        demand_consumers: db40,\n        demand_consumers_capacity: db40,\n        demand_consumers_normalprices: db40,\n        demand_consumers_profiles_weatheryears: db41,\n        demand_consumers_profiles_oneyear: db41,\n        # ===: WIND AND SOLAR ====,\n        wind_generators: db10,\n        wind_generators_capacity: db11,\n        wind_generators_profiles: db11,\n        solar_generators: db10,\n        solar_generators_capacity: db11,\n        solar_generators_profiles: db11,\n        # ==== Transmission ====\n        transmission_grid: db50,\n        transmission_capacity: db51,\n        transmission_loss: db51,\n        transmission_profiles: db51,\n    }\n\n    @classmethod\n    def get_relative_folder_path(cls, file_id: str) -&gt; Path:\n        \"\"\"\n        Get the relative database folder path for a given file_id.\n\n        The relative path consists of database folder and file name.\n\n        Args:\n            file_id (str): Identifier for the file to retrieve.\n\n        Returns:\n            Path: The database folder name.\n\n        \"\"\"\n        try:\n            return Path(cls.db_folder_map[file_id])\n        except KeyError as e:\n            message = f\"File id '{file_id}' not found in database folder map.\"\n\n            raise KeyError(message) from e\n\n    @classmethod\n    def get_file_name(cls, source: Path, db_folder: str, file_id: str) -&gt; str | None:\n        \"\"\"\n        Get the name of a file, with extension, from a file ID and a path.\n\n        Args:\n            source (Path): Root path of the database.\n            db_folder (str): Database folder to look for the file in.\n            file_id (str): ID of file, i.e the name of the file without extension.\n\n        Raises:\n            RuntimeError: If multiple files with the same ID but different extensions are found.\n\n        Returns:\n            str | None: File ID and extension combined. If file is not found, return None.\n\n        \"\"\"\n        db_path = source / db_folder\n        if not db_path.exists():\n            message = f\"The database folder {db_path} does not exist.\"\n            raise FileNotFoundError(message)\n        candidate_extentions = set()\n        for file_path in db_path.iterdir():\n            if file_path.is_file() and file_path.stem == file_id:\n                candidate_extentions.add(file_path.suffix)\n        if len(candidate_extentions) &gt; 1:  # Multiple files of same ID. Ambiguous\n            message = (\n                f\"Found multiple files with ID {file_id} (with different extensions: {candidate_extentions}) in database folder {db_path}.\"\n                \" File names must be unique.\"\n            )\n            raise RuntimeError(message)\n        if len(candidate_extentions) == 0:  # No matching files.\n            return None\n            # message = f\"Found no file with ID {file_id} in database folder {db_path}.\"\n            # raise FileNotFoundError(message)\n\n        (extension,) = candidate_extentions  # We have only one candidate, so we extract it.\n        return file_id + extension\n</code></pre>"},{"location":"reference/#framdata.database_names.DatabaseNames.DatabaseNames.get_file_name","title":"<code>get_file_name(source: Path, db_folder: str, file_id: str) -&gt; str | None</code>  <code>classmethod</code>","text":"<p>Get the name of a file, with extension, from a file ID and a path.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Path</code> <p>Root path of the database.</p> required <code>db_folder</code> <code>str</code> <p>Database folder to look for the file in.</p> required <code>file_id</code> <code>str</code> <p>ID of file, i.e the name of the file without extension.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If multiple files with the same ID but different extensions are found.</p> <p>Returns:</p> Type Description <code>str | None</code> <p>str | None: File ID and extension combined. If file is not found, return None.</p> Source code in <code>framdata/database_names/DatabaseNames.py</code> <pre><code>@classmethod\ndef get_file_name(cls, source: Path, db_folder: str, file_id: str) -&gt; str | None:\n    \"\"\"\n    Get the name of a file, with extension, from a file ID and a path.\n\n    Args:\n        source (Path): Root path of the database.\n        db_folder (str): Database folder to look for the file in.\n        file_id (str): ID of file, i.e the name of the file without extension.\n\n    Raises:\n        RuntimeError: If multiple files with the same ID but different extensions are found.\n\n    Returns:\n        str | None: File ID and extension combined. If file is not found, return None.\n\n    \"\"\"\n    db_path = source / db_folder\n    if not db_path.exists():\n        message = f\"The database folder {db_path} does not exist.\"\n        raise FileNotFoundError(message)\n    candidate_extentions = set()\n    for file_path in db_path.iterdir():\n        if file_path.is_file() and file_path.stem == file_id:\n            candidate_extentions.add(file_path.suffix)\n    if len(candidate_extentions) &gt; 1:  # Multiple files of same ID. Ambiguous\n        message = (\n            f\"Found multiple files with ID {file_id} (with different extensions: {candidate_extentions}) in database folder {db_path}.\"\n            \" File names must be unique.\"\n        )\n        raise RuntimeError(message)\n    if len(candidate_extentions) == 0:  # No matching files.\n        return None\n        # message = f\"Found no file with ID {file_id} in database folder {db_path}.\"\n        # raise FileNotFoundError(message)\n\n    (extension,) = candidate_extentions  # We have only one candidate, so we extract it.\n    return file_id + extension\n</code></pre>"},{"location":"reference/#framdata.database_names.DatabaseNames.DatabaseNames.get_relative_folder_path","title":"<code>get_relative_folder_path(file_id: str) -&gt; Path</code>  <code>classmethod</code>","text":"<p>Get the relative database folder path for a given file_id.</p> <p>The relative path consists of database folder and file name.</p> <p>Parameters:</p> Name Type Description Default <code>file_id</code> <code>str</code> <p>Identifier for the file to retrieve.</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>The database folder name.</p> Source code in <code>framdata/database_names/DatabaseNames.py</code> <pre><code>@classmethod\ndef get_relative_folder_path(cls, file_id: str) -&gt; Path:\n    \"\"\"\n    Get the relative database folder path for a given file_id.\n\n    The relative path consists of database folder and file name.\n\n    Args:\n        file_id (str): Identifier for the file to retrieve.\n\n    Returns:\n        Path: The database folder name.\n\n    \"\"\"\n    try:\n        return Path(cls.db_folder_map[file_id])\n    except KeyError as e:\n        message = f\"File id '{file_id}' not found in database folder map.\"\n\n        raise KeyError(message) from e\n</code></pre>"},{"location":"reference/#framdata.database_names.DemandNames","title":"<code>DemandNames</code>","text":"<p>Contains classes defining the demand table and validations.</p>"},{"location":"reference/#framdata.database_names.DemandNames.DemandMetadataSchema","title":"<code>DemandMetadataSchema</code>","text":"<p>               Bases: <code>_AttributeMetadataSchema</code></p> <p>Pandera DataFrameModel schema for metadata in the Demand.Consumers file.</p> Source code in <code>framdata/database_names/DemandNames.py</code> <pre><code>class DemandMetadataSchema(_AttributeMetadataSchema):\n    \"\"\"Pandera DataFrameModel schema for metadata in the Demand.Consumers file.\"\"\"\n\n    @pa.dataframe_check\n    @classmethod\n    def check_unit_is_str_for_attributes(cls, df: pd.DataFrame) -&gt; Series[bool]:\n        \"\"\"\n        Check that the 'unit' value is a string for the row where 'attribute' is 'Capacity'.\n\n        Args:\n            df (Dataframe): DataFrame used to check value for \"unit\".\n\n        Returns:\n            Series[bool]: Series of boolean values detonating if each element has passed the check.\n\n        \"\"\"\n        return check_unit_is_str_for_attributes(df, [DemandNames.capacity_col])\n</code></pre>"},{"location":"reference/#framdata.database_names.DemandNames.DemandMetadataSchema.check_unit_is_str_for_attributes","title":"<code>check_unit_is_str_for_attributes(df: pd.DataFrame) -&gt; Series[bool]</code>  <code>classmethod</code>","text":"<p>Check that the 'unit' value is a string for the row where 'attribute' is 'Capacity'.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Dataframe</code> <p>DataFrame used to check value for \"unit\".</p> required <p>Returns:</p> Type Description <code>Series[bool]</code> <p>Series[bool]: Series of boolean values detonating if each element has passed the check.</p> Source code in <code>framdata/database_names/DemandNames.py</code> <pre><code>@pa.dataframe_check\n@classmethod\ndef check_unit_is_str_for_attributes(cls, df: pd.DataFrame) -&gt; Series[bool]:\n    \"\"\"\n    Check that the 'unit' value is a string for the row where 'attribute' is 'Capacity'.\n\n    Args:\n        df (Dataframe): DataFrame used to check value for \"unit\".\n\n    Returns:\n        Series[bool]: Series of boolean values detonating if each element has passed the check.\n\n    \"\"\"\n    return check_unit_is_str_for_attributes(df, [DemandNames.capacity_col])\n</code></pre>"},{"location":"reference/#framdata.database_names.DemandNames.DemandNames","title":"<code>DemandNames</code>","text":"<p>               Bases: <code>_BaseComponentsNames</code></p> <p>Container class for describing the demand attribute table's names, structure, and convertion to Demand Component.</p> Source code in <code>framdata/database_names/DemandNames.py</code> <pre><code>class DemandNames(_BaseComponentsNames):\n    \"\"\"Container class for describing the demand attribute table's names, structure, and convertion to Demand Component.\"\"\"\n\n    id_col = \"ConsumerID\"\n    node_col = \"PowerNode\"\n    reserve_price_col = \"ReservePrice\"\n    price_elasticity_col = \"PriceElasticity\"\n    min_price_col = \"MinPriceLimit\"\n    max_price_col = \"MaxPriceLimit\"\n    normal_price_col = \"NormalPrice\"\n    capacity_profile_col = \"CapacityProfile\"\n    temperature_profile_col = \"TemperatureProfile\"\n    capacity_col = \"Capacity\"\n\n    columns: ClassVar[list[str]] = [\n        id_col,\n        node_col,\n        reserve_price_col,\n        price_elasticity_col,\n        min_price_col,\n        max_price_col,\n        normal_price_col,\n        capacity_profile_col,\n        temperature_profile_col,\n        capacity_col,\n    ]\n\n    ref_columns: ClassVar[list[str]] = [\n        node_col,\n        reserve_price_col,\n        price_elasticity_col,\n        min_price_col,\n        max_price_col,\n        normal_price_col,\n        capacity_profile_col,\n        temperature_profile_col,\n        capacity_col,\n    ]\n\n    @staticmethod\n    def create_component(\n        row: NDArray,\n        indices: dict[str, int],\n        meta_columns: set[str],\n        meta_data: pd.DataFrame,\n        attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n    ) -&gt; dict[str, Demand]:\n        \"\"\"\n        Create a Demand component from a table row in the Demand.Consumers file.\n\n        Args:\n            row (NDArray): Array containing the values of one table row, represeting one Demand object.\n            indices (list[str, int]): Mapping of table's Column names to the array's indices.\n            meta_columns (list[str]): Set of columns which defines memberships in meta groups for aggregation.\n            meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n            attribute_objects (dict[str, tuple[object, dict[str, Meta]]], optional): NOT USED\n\n        Returns:\n            dict[str, Demand]: A dictionary with the consumer_id as key and the demand component as value.\n\n        \"\"\"\n        elastic_demand_cols = [\n            DemandNames.price_elasticity_col,\n            DemandNames.min_price_col,\n            DemandNames.max_price_col,\n            DemandNames.normal_price_col,\n        ]\n        columns_to_parse = [\n            DemandNames.reserve_price_col,\n            DemandNames.capacity_profile_col,\n            DemandNames.temperature_profile_col,\n            DemandNames.capacity_col,\n        ]\n        columns_to_parse.extend(elastic_demand_cols)\n\n        arg_user_code = DemandNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n        elastic_demand_values = [value for key, value in arg_user_code.items() if key in elastic_demand_cols]\n        if all(value is not None for value in elastic_demand_values):\n            elastic_demand = ElasticDemand(\n                price_elasticity=Elasticity(level=arg_user_code[DemandNames.price_elasticity_col]),\n                min_price=Price(level=arg_user_code[DemandNames.min_price_col]),\n                normal_price=Price(level=arg_user_code[DemandNames.normal_price_col]),\n                max_price=Price(level=arg_user_code[DemandNames.max_price_col]),\n            )\n            reserve_price = None\n        elif arg_user_code[DemandNames.reserve_price_col] is not None:\n            elastic_demand = None\n            reserve_price = ReservePrice(level=arg_user_code[DemandNames.reserve_price_col])\n        else:\n            elastic_demand = None\n            reserve_price = None\n        demand = Demand(\n            node=row[indices[DemandNames.node_col]],\n            capacity=MaxFlowVolume(\n                level=arg_user_code[DemandNames.capacity_col],\n                profile=arg_user_code[DemandNames.capacity_profile_col],\n            ),\n            reserve_price=reserve_price,\n            elastic_demand=elastic_demand,\n            temperature_profile=arg_user_code[DemandNames.temperature_profile_col],\n        )\n        DemandNames._add_meta(demand, row, indices, meta_columns)\n\n        return {row[indices[DemandNames.id_col]]: demand}\n\n    @staticmethod\n    def get_attribute_data_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for attribute data in the Demand.Consumers file.\n\n        Returns:\n            DemandSchema (pa.DataFrameModel): Pandera DataFrameModel schema for Demand attribute data.\n\n        \"\"\"\n        return DemandSchema\n\n    @staticmethod\n    def get_metadata_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for metadata in the Demand.Consumers file.\n\n        Returns:\n            DemandMetadataSchema (pa.DataFrameModel): Pandera DataFrameModel schema for Demand metadata.\n\n        \"\"\"\n        return DemandMetadataSchema\n\n    @staticmethod\n    def _get_unique_check_descriptions() -&gt; dict[str, tuple[str, bool]]:\n        \"\"\"\n        Retrieve a dictionary with descriptons of validation checks that are specific to the Demand schemas.\n\n        Returns:\n            dict[str, tuple[str, bool]]: A dictionary where:\n                - Keys (str): The name of the validation check method.\n                - Values (tuple[str, bool]):\n                    - The first element (str) provides a concise and user-friendly description of the check. E.g. what\n                      caused the validation error or what is required for the check to pass.\n                    - The second element (bool) indicates whether the check is a warning (True) or an error (False).\n\n        \"\"\"\n        return {\n            \"check_elastic_demand\": (\"Missing elastic demand value.\", True),\n        }\n\n    @staticmethod\n    def _format_unique_checks(errors: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Format the error DataFrame according to the validation checks that are specific to the Demand schemas.\n\n        This method processes validation errors that come from a dataframe-level check on elastic demand columns in the\n        attribute data schema. The default reporting on failed dataframe-level checks in Pandera's standard error\n        reports DataFrame (errors) is not very user-friendly. It can contain uneccassary rows about columns that are not\n        relevant to the check and will not include rows about the columns relevant to the check if those columns have\n        missing values. This method removes uneccassary rows from the error dataframe and ensures that rows with\n        information abot the elastic demand columns that fail the check are included.\n\n        Args:\n            errors (pd.DataFrame): DataFrame containing validation errors. Pandera's standard error reports DataFrame.\n\n        Returns:\n            pd.DataFrame: The updated error DataFrame with formatted rows for unique validation checks.\n\n        \"\"\"\n        if \"check_elastic_demand\" in errors[DemandNames.COL_CHECK].to_numpy():\n            check_rows = errors.loc[errors[DemandNames.COL_CHECK] == \"check_elastic_demand\"]\n            errors = errors[~(errors[DemandNames.COL_CHECK] == \"check_elastic_demand\")]\n            elastic_demand_columns = [\n                DemandNames.price_elasticity_col,\n                DemandNames.min_price_col,\n                DemandNames.max_price_col,\n                DemandNames.normal_price_col,\n            ]\n            check_description_str = check_rows[DemandNames.COL_CHECK_DESC].unique()[0]\n            elastic_demand_rows = []\n            for idx in check_rows[DemandNames.COL_IDX].unique():\n                check_case = check_rows[check_rows[DemandNames.COL_IDX] == idx]\n                for col in elastic_demand_columns:\n                    if col not in list(check_case[DemandNames.COL_COLUMN].unique()):\n                        elastic_demand_rows.append(\n                            [\n                                col,\n                                \"check_elastic_demand\",\n                                None,\n                                idx,\n                                check_description_str,\n                                True,\n                            ],\n                        )\n            errors = pd.concat([errors, pd.DataFrame(elastic_demand_rows, columns=errors.columns)], ignore_index=True)\n        return errors\n</code></pre>"},{"location":"reference/#framdata.database_names.DemandNames.DemandNames.create_component","title":"<code>create_component(row: NDArray, indices: dict[str, int], meta_columns: set[str], meta_data: pd.DataFrame, attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None) -&gt; dict[str, Demand]</code>  <code>staticmethod</code>","text":"<p>Create a Demand component from a table row in the Demand.Consumers file.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>NDArray</code> <p>Array containing the values of one table row, represeting one Demand object.</p> required <code>indices</code> <code>list[str, int]</code> <p>Mapping of table's Column names to the array's indices.</p> required <code>meta_columns</code> <code>list[str]</code> <p>Set of columns which defines memberships in meta groups for aggregation.</p> required <code>meta_data</code> <code>DataFrame</code> <p>Dictionary containing at least unit of every column.</p> required <code>attribute_objects</code> <code>dict[str, tuple[object, dict[str, Meta]]]</code> <p>NOT USED</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Demand]</code> <p>dict[str, Demand]: A dictionary with the consumer_id as key and the demand component as value.</p> Source code in <code>framdata/database_names/DemandNames.py</code> <pre><code>@staticmethod\ndef create_component(\n    row: NDArray,\n    indices: dict[str, int],\n    meta_columns: set[str],\n    meta_data: pd.DataFrame,\n    attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n) -&gt; dict[str, Demand]:\n    \"\"\"\n    Create a Demand component from a table row in the Demand.Consumers file.\n\n    Args:\n        row (NDArray): Array containing the values of one table row, represeting one Demand object.\n        indices (list[str, int]): Mapping of table's Column names to the array's indices.\n        meta_columns (list[str]): Set of columns which defines memberships in meta groups for aggregation.\n        meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n        attribute_objects (dict[str, tuple[object, dict[str, Meta]]], optional): NOT USED\n\n    Returns:\n        dict[str, Demand]: A dictionary with the consumer_id as key and the demand component as value.\n\n    \"\"\"\n    elastic_demand_cols = [\n        DemandNames.price_elasticity_col,\n        DemandNames.min_price_col,\n        DemandNames.max_price_col,\n        DemandNames.normal_price_col,\n    ]\n    columns_to_parse = [\n        DemandNames.reserve_price_col,\n        DemandNames.capacity_profile_col,\n        DemandNames.temperature_profile_col,\n        DemandNames.capacity_col,\n    ]\n    columns_to_parse.extend(elastic_demand_cols)\n\n    arg_user_code = DemandNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n    elastic_demand_values = [value for key, value in arg_user_code.items() if key in elastic_demand_cols]\n    if all(value is not None for value in elastic_demand_values):\n        elastic_demand = ElasticDemand(\n            price_elasticity=Elasticity(level=arg_user_code[DemandNames.price_elasticity_col]),\n            min_price=Price(level=arg_user_code[DemandNames.min_price_col]),\n            normal_price=Price(level=arg_user_code[DemandNames.normal_price_col]),\n            max_price=Price(level=arg_user_code[DemandNames.max_price_col]),\n        )\n        reserve_price = None\n    elif arg_user_code[DemandNames.reserve_price_col] is not None:\n        elastic_demand = None\n        reserve_price = ReservePrice(level=arg_user_code[DemandNames.reserve_price_col])\n    else:\n        elastic_demand = None\n        reserve_price = None\n    demand = Demand(\n        node=row[indices[DemandNames.node_col]],\n        capacity=MaxFlowVolume(\n            level=arg_user_code[DemandNames.capacity_col],\n            profile=arg_user_code[DemandNames.capacity_profile_col],\n        ),\n        reserve_price=reserve_price,\n        elastic_demand=elastic_demand,\n        temperature_profile=arg_user_code[DemandNames.temperature_profile_col],\n    )\n    DemandNames._add_meta(demand, row, indices, meta_columns)\n\n    return {row[indices[DemandNames.id_col]]: demand}\n</code></pre>"},{"location":"reference/#framdata.database_names.DemandNames.DemandNames.get_attribute_data_schema","title":"<code>get_attribute_data_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for attribute data in the Demand.Consumers file.</p> <p>Returns:</p> Name Type Description <code>DemandSchema</code> <code>DataFrameModel</code> <p>Pandera DataFrameModel schema for Demand attribute data.</p> Source code in <code>framdata/database_names/DemandNames.py</code> <pre><code>@staticmethod\ndef get_attribute_data_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for attribute data in the Demand.Consumers file.\n\n    Returns:\n        DemandSchema (pa.DataFrameModel): Pandera DataFrameModel schema for Demand attribute data.\n\n    \"\"\"\n    return DemandSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.DemandNames.DemandNames.get_metadata_schema","title":"<code>get_metadata_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for metadata in the Demand.Consumers file.</p> <p>Returns:</p> Name Type Description <code>DemandMetadataSchema</code> <code>DataFrameModel</code> <p>Pandera DataFrameModel schema for Demand metadata.</p> Source code in <code>framdata/database_names/DemandNames.py</code> <pre><code>@staticmethod\ndef get_metadata_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for metadata in the Demand.Consumers file.\n\n    Returns:\n        DemandMetadataSchema (pa.DataFrameModel): Pandera DataFrameModel schema for Demand metadata.\n\n    \"\"\"\n    return DemandMetadataSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.DemandNames.DemandSchema","title":"<code>DemandSchema</code>","text":"<p>               Bases: <code>DataFrameModel</code></p> <p>Pandera DataFrameModel schema for attribute data in the Demand.Consumers file.</p> Source code in <code>framdata/database_names/DemandNames.py</code> <pre><code>class DemandSchema(pa.DataFrameModel):\n    \"\"\"Pandera DataFrameModel schema for attribute data in the Demand.Consumers file.\"\"\"\n\n    ConsumerID: Series[str] = pa.Field(unique=True, nullable=False)\n    PowerNode: Series[str] = pa.Field(nullable=False)\n    ReservePrice: Series[Any] = pa.Field(nullable=True)\n    PriceElasticity: Series[Any] = pa.Field(nullable=True)\n    MinPriceLimit: Series[Any] = pa.Field(nullable=True)\n    MaxPriceLimit: Series[Any] = pa.Field(nullable=True)\n    NormalPrice: Series[Any] = pa.Field(nullable=True)\n    CapacityProfile: Series[Any] = pa.Field(nullable=True)\n    TemperatureProfile: Series[Any] = pa.Field(nullable=True)\n    Capacity: Series[Any] = pa.Field(nullable=False)\n\n    @pa.check(DemandNames.capacity_col)\n    @classmethod\n    def dtype_str_int_float(cls, series: Series[Any]) -&gt; Series[bool]:\n        \"\"\"Check if values in the series are of datatype: str, int or float.\"\"\"\n        return dtype_str_int_float(series)\n\n    @pa.check(\n        DemandNames.reserve_price_col,\n        DemandNames.price_elasticity_col,\n        DemandNames.min_price_col,\n        DemandNames.max_price_col,\n        DemandNames.normal_price_col,\n        DemandNames.capacity_profile_col,\n        DemandNames.temperature_profile_col,\n    )\n    @classmethod\n    def dtype_str_int_float_none(cls, series: Series[Any]) -&gt; Series[bool]:\n        \"\"\"Check if values in the series are of datatype: str, int, float or None.\"\"\"\n        return dtype_str_int_float_none(series)\n\n    @pa.check(DemandNames.price_elasticity_col)\n    @classmethod\n    def numeric_values_less_than_or_equal_to_0(cls, series: Series[Any]) -&gt; Series[bool]:\n        \"\"\"Check if numeric values in the series are less than or equal to zero.\"\"\"\n        return numeric_values_less_than_or_equal_to(series, 0)\n\n    @pa.check(\n        DemandNames.reserve_price_col,\n        DemandNames.min_price_col,\n        DemandNames.max_price_col,\n        DemandNames.normal_price_col,\n        DemandNames.capacity_col,\n    )\n    @classmethod\n    def numeric_values_greater_than_or_equal_to_0(cls, series: Series[Any]) -&gt; Series[bool]:\n        \"\"\"Check if numeric values in the series are greater than or equal to zero.\"\"\"\n        return numeric_values_greater_than_or_equal_to(series, 0)\n\n    @pa.check(DemandNames.capacity_profile_col)\n    @classmethod\n    def numeric_values_are_between_or_equal_to_0_and_1(cls, series: Series[Any]) -&gt; Series[bool]:\n        \"\"\"Check if numeric values in the series are between zero and one or equal to zero and one.\"\"\"\n        return numeric_values_are_between_or_equal_to(series, 0, 1)\n\n    @pa.dataframe_check\n    @classmethod\n    def check_elastic_demand(cls, df: DataFrame) -&gt; Series[bool]:\n        \"\"\"Check that all elastic demand values are present if one or more is.\"\"\"\n        elastic_demand = df[\n            [\n                DemandNames.price_elasticity_col,\n                DemandNames.min_price_col,\n                DemandNames.max_price_col,\n                DemandNames.normal_price_col,\n            ]\n        ]\n\n        check = elastic_demand.apply(\n            lambda row: all(value is not None for value in row) if any(value is not None for value in row) else True,\n            axis=1,\n        ).tolist()\n        return pd.Series(check)\n\n    class Config:\n        \"\"\"Schema-wide configuration for the DemandSchema class.\"\"\"\n\n        unique_column_names = True\n</code></pre>"},{"location":"reference/#framdata.database_names.DemandNames.DemandSchema.Config","title":"<code>Config</code>","text":"<p>Schema-wide configuration for the DemandSchema class.</p> Source code in <code>framdata/database_names/DemandNames.py</code> <pre><code>class Config:\n    \"\"\"Schema-wide configuration for the DemandSchema class.\"\"\"\n\n    unique_column_names = True\n</code></pre>"},{"location":"reference/#framdata.database_names.DemandNames.DemandSchema.check_elastic_demand","title":"<code>check_elastic_demand(df: DataFrame) -&gt; Series[bool]</code>  <code>classmethod</code>","text":"<p>Check that all elastic demand values are present if one or more is.</p> Source code in <code>framdata/database_names/DemandNames.py</code> <pre><code>@pa.dataframe_check\n@classmethod\ndef check_elastic_demand(cls, df: DataFrame) -&gt; Series[bool]:\n    \"\"\"Check that all elastic demand values are present if one or more is.\"\"\"\n    elastic_demand = df[\n        [\n            DemandNames.price_elasticity_col,\n            DemandNames.min_price_col,\n            DemandNames.max_price_col,\n            DemandNames.normal_price_col,\n        ]\n    ]\n\n    check = elastic_demand.apply(\n        lambda row: all(value is not None for value in row) if any(value is not None for value in row) else True,\n        axis=1,\n    ).tolist()\n    return pd.Series(check)\n</code></pre>"},{"location":"reference/#framdata.database_names.DemandNames.DemandSchema.dtype_str_int_float","title":"<code>dtype_str_int_float(series: Series[Any]) -&gt; Series[bool]</code>  <code>classmethod</code>","text":"<p>Check if values in the series are of datatype: str, int or float.</p> Source code in <code>framdata/database_names/DemandNames.py</code> <pre><code>@pa.check(DemandNames.capacity_col)\n@classmethod\ndef dtype_str_int_float(cls, series: Series[Any]) -&gt; Series[bool]:\n    \"\"\"Check if values in the series are of datatype: str, int or float.\"\"\"\n    return dtype_str_int_float(series)\n</code></pre>"},{"location":"reference/#framdata.database_names.DemandNames.DemandSchema.dtype_str_int_float_none","title":"<code>dtype_str_int_float_none(series: Series[Any]) -&gt; Series[bool]</code>  <code>classmethod</code>","text":"<p>Check if values in the series are of datatype: str, int, float or None.</p> Source code in <code>framdata/database_names/DemandNames.py</code> <pre><code>@pa.check(\n    DemandNames.reserve_price_col,\n    DemandNames.price_elasticity_col,\n    DemandNames.min_price_col,\n    DemandNames.max_price_col,\n    DemandNames.normal_price_col,\n    DemandNames.capacity_profile_col,\n    DemandNames.temperature_profile_col,\n)\n@classmethod\ndef dtype_str_int_float_none(cls, series: Series[Any]) -&gt; Series[bool]:\n    \"\"\"Check if values in the series are of datatype: str, int, float or None.\"\"\"\n    return dtype_str_int_float_none(series)\n</code></pre>"},{"location":"reference/#framdata.database_names.DemandNames.DemandSchema.numeric_values_are_between_or_equal_to_0_and_1","title":"<code>numeric_values_are_between_or_equal_to_0_and_1(series: Series[Any]) -&gt; Series[bool]</code>  <code>classmethod</code>","text":"<p>Check if numeric values in the series are between zero and one or equal to zero and one.</p> Source code in <code>framdata/database_names/DemandNames.py</code> <pre><code>@pa.check(DemandNames.capacity_profile_col)\n@classmethod\ndef numeric_values_are_between_or_equal_to_0_and_1(cls, series: Series[Any]) -&gt; Series[bool]:\n    \"\"\"Check if numeric values in the series are between zero and one or equal to zero and one.\"\"\"\n    return numeric_values_are_between_or_equal_to(series, 0, 1)\n</code></pre>"},{"location":"reference/#framdata.database_names.DemandNames.DemandSchema.numeric_values_greater_than_or_equal_to_0","title":"<code>numeric_values_greater_than_or_equal_to_0(series: Series[Any]) -&gt; Series[bool]</code>  <code>classmethod</code>","text":"<p>Check if numeric values in the series are greater than or equal to zero.</p> Source code in <code>framdata/database_names/DemandNames.py</code> <pre><code>@pa.check(\n    DemandNames.reserve_price_col,\n    DemandNames.min_price_col,\n    DemandNames.max_price_col,\n    DemandNames.normal_price_col,\n    DemandNames.capacity_col,\n)\n@classmethod\ndef numeric_values_greater_than_or_equal_to_0(cls, series: Series[Any]) -&gt; Series[bool]:\n    \"\"\"Check if numeric values in the series are greater than or equal to zero.\"\"\"\n    return numeric_values_greater_than_or_equal_to(series, 0)\n</code></pre>"},{"location":"reference/#framdata.database_names.DemandNames.DemandSchema.numeric_values_less_than_or_equal_to_0","title":"<code>numeric_values_less_than_or_equal_to_0(series: Series[Any]) -&gt; Series[bool]</code>  <code>classmethod</code>","text":"<p>Check if numeric values in the series are less than or equal to zero.</p> Source code in <code>framdata/database_names/DemandNames.py</code> <pre><code>@pa.check(DemandNames.price_elasticity_col)\n@classmethod\ndef numeric_values_less_than_or_equal_to_0(cls, series: Series[Any]) -&gt; Series[bool]:\n    \"\"\"Check if numeric values in the series are less than or equal to zero.\"\"\"\n    return numeric_values_less_than_or_equal_to(series, 0)\n</code></pre>"},{"location":"reference/#framdata.database_names.H5Names","title":"<code>H5Names</code>","text":"<p>Define names and fields used in H5 files.</p>"},{"location":"reference/#framdata.database_names.H5Names.H5Names","title":"<code>H5Names</code>","text":"<p>Container class for names used in H5 files.</p> Source code in <code>framdata/database_names/H5Names.py</code> <pre><code>class H5Names:\n    \"\"\"Container class for names used in H5 files.\"\"\"\n\n    INDEX_GROUP = \"index\"\n    METADATA_GROUP = \"metadata\"\n    VECTORS_GROUP = \"vectors\"\n    COMMON_PREFIX = \"common_\"\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroBypassNames","title":"<code>HydroBypassNames</code>","text":"<p>Contain the BypassNames class and related Pandera schemas for handling hydropower bypass data.</p> <p>Includes attribute and metadata schemas.</p>"},{"location":"reference/#framdata.database_names.HydroBypassNames.HydroBypassMetadataSchema","title":"<code>HydroBypassMetadataSchema</code>","text":"<p>               Bases: <code>_AttributeMetadataSchema</code></p> <p>Pandera DataFrameModel schema for metadata in the Hydropower.Bypass file.</p> Source code in <code>framdata/database_names/HydroBypassNames.py</code> <pre><code>class HydroBypassMetadataSchema(_AttributeMetadataSchema):\n    \"\"\"Pandera DataFrameModel schema for metadata in the Hydropower.Bypass file.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroBypassNames.HydroBypassNames","title":"<code>HydroBypassNames</code>","text":"<p>               Bases: <code>_BaseComponentsNames</code></p> <p>Define naming conventions and attribute object creation for HydroBypass object, which is an attribute of the HydroModule.</p> <p>Provides methods for creating generator components, retrieving Pandera schemas for attribute and metadata tables, and formatting validation errors specific to generator schemas.</p> Source code in <code>framdata/database_names/HydroBypassNames.py</code> <pre><code>class HydroBypassNames(_BaseComponentsNames):\n    \"\"\"\n    Define naming conventions and attribute object creation for HydroBypass object, which is an attribute of the HydroModule.\n\n    Provides methods for creating generator components, retrieving Pandera schemas for attribute and metadata tables,\n    and formatting validation errors specific to generator schemas.\n\n    \"\"\"\n\n    id_col = \"BypassID\"\n    to_col = \"BypassTo\"\n    cap_col = \"Capacity\"\n    min_bnd_col = \"MinOperationalBypass\"\n    min_penalty_col = \"MinViolationPenalty\"\n\n    columns: ClassVar[list[str]] = [id_col, to_col, cap_col, min_bnd_col, min_penalty_col]\n\n    ref_columns: ClassVar[list[str]] = [to_col, cap_col, min_bnd_col, min_penalty_col]\n\n    @staticmethod\n    def create_component(\n        row: NDArray,\n        indices: dict[str, int],\n        meta_columns: set[str],\n        meta_data: pd.DataFrame,\n        attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n    ) -&gt; dict[str, HydroBypass]:\n        \"\"\"\n        Create a HydroBypass object.\n\n        Args:\n            row (NDArray): Array containing the values of one table row, represeting one HydroModule object.\n            indices (list[str, int]): Mapping of table's Column names to the array's indices.\n            meta_columns (set[str]): Set of columns used to tag object with memberships.\n            meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n            attribute_objects (dict[str, tuple[object, dict[str, Meta]]], optional): NOT USED, currently only used in HydroModulesNames.\n\n        Returns:\n            dict[str, HydroBypass]: A dictionary with the bypass ID as key and the module unit as value.\n\n        \"\"\"\n        columns_to_parse = [\n            HydroBypassNames.id_col,\n            HydroBypassNames.to_col,\n            HydroBypassNames.cap_col,\n            HydroBypassNames.min_bnd_col,\n            HydroBypassNames.min_penalty_col,\n        ]\n\n        arg_user_code = HydroBypassNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n        bypass = HydroBypass(\n            to_module=row[indices[HydroBypassNames.to_col]],\n            # capacity=SoftFlowCapacity(\n            #     level_input=arg_user_code[BypassNames.cap_col],\n            #     min_profile_input=arg_user_code[BypassNames.min_bnd_col],\n            #     min_penalty=arg_user_code[BypassNames.min_penalty_col],\n            # ),\n            capacity=MaxFlowVolume(level=arg_user_code[HydroBypassNames.cap_col]),\n        )\n\n        meta = {}\n        HydroBypassNames._add_meta(meta, row, indices, meta_columns)\n\n        return {row[indices[HydroBypassNames.id_col]]: (bypass, meta)}\n\n    @staticmethod\n    def get_attribute_data_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for attribute data in the Hydropower.Bypass file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for the Bypass attribute data.\n\n        \"\"\"\n        return HydroBypassSchema\n\n    @staticmethod\n    def get_metadata_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for the metadata table in the Hydropower.Bypass file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for the Bypass metadata.\n\n        \"\"\"\n        return HydroBypassMetadataSchema\n\n    @staticmethod\n    def _get_unique_check_descriptions() -&gt; dict[str, tuple[str, bool]]:\n        \"\"\"\n        Retrieve a dictionary with descriptons of validation checks that are specific to the Bypass schemas.\n\n        Returns:\n            dict[str, tuple[str, bool]]: A dictionary where:\n                - Keys (str): The name of the validation check method.\n                - Values (tuple[str, bool]):\n                    - The first element (str) provides a concise and user-friendly description of the check. E.g. what\n                      caused the validation error or what is required for the check to pass.\n                    - The second element (bool) indicates whether the check is a warning (True) or an error (False).\n\n\n        \"\"\"\n        return None\n\n    @staticmethod\n    def _format_unique_checks(errors: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Format the error DataFrame according to the validation checks that are specific to the Bypass schemas.\n\n        Args:\n            errors (pd.DataFrame): The error DataFrame containing validation errors.\n\n        Returns:\n            pd.DataFrame: The updated error DataFrame with formatted rows for unique validation checks.\n\n        \"\"\"\n        return None\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroBypassNames.HydroBypassNames.create_component","title":"<code>create_component(row: NDArray, indices: dict[str, int], meta_columns: set[str], meta_data: pd.DataFrame, attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None) -&gt; dict[str, HydroBypass]</code>  <code>staticmethod</code>","text":"<p>Create a HydroBypass object.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>NDArray</code> <p>Array containing the values of one table row, represeting one HydroModule object.</p> required <code>indices</code> <code>list[str, int]</code> <p>Mapping of table's Column names to the array's indices.</p> required <code>meta_columns</code> <code>set[str]</code> <p>Set of columns used to tag object with memberships.</p> required <code>meta_data</code> <code>DataFrame</code> <p>Dictionary containing at least unit of every column.</p> required <code>attribute_objects</code> <code>dict[str, tuple[object, dict[str, Meta]]]</code> <p>NOT USED, currently only used in HydroModulesNames.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, HydroBypass]</code> <p>dict[str, HydroBypass]: A dictionary with the bypass ID as key and the module unit as value.</p> Source code in <code>framdata/database_names/HydroBypassNames.py</code> <pre><code>@staticmethod\ndef create_component(\n    row: NDArray,\n    indices: dict[str, int],\n    meta_columns: set[str],\n    meta_data: pd.DataFrame,\n    attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n) -&gt; dict[str, HydroBypass]:\n    \"\"\"\n    Create a HydroBypass object.\n\n    Args:\n        row (NDArray): Array containing the values of one table row, represeting one HydroModule object.\n        indices (list[str, int]): Mapping of table's Column names to the array's indices.\n        meta_columns (set[str]): Set of columns used to tag object with memberships.\n        meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n        attribute_objects (dict[str, tuple[object, dict[str, Meta]]], optional): NOT USED, currently only used in HydroModulesNames.\n\n    Returns:\n        dict[str, HydroBypass]: A dictionary with the bypass ID as key and the module unit as value.\n\n    \"\"\"\n    columns_to_parse = [\n        HydroBypassNames.id_col,\n        HydroBypassNames.to_col,\n        HydroBypassNames.cap_col,\n        HydroBypassNames.min_bnd_col,\n        HydroBypassNames.min_penalty_col,\n    ]\n\n    arg_user_code = HydroBypassNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n    bypass = HydroBypass(\n        to_module=row[indices[HydroBypassNames.to_col]],\n        # capacity=SoftFlowCapacity(\n        #     level_input=arg_user_code[BypassNames.cap_col],\n        #     min_profile_input=arg_user_code[BypassNames.min_bnd_col],\n        #     min_penalty=arg_user_code[BypassNames.min_penalty_col],\n        # ),\n        capacity=MaxFlowVolume(level=arg_user_code[HydroBypassNames.cap_col]),\n    )\n\n    meta = {}\n    HydroBypassNames._add_meta(meta, row, indices, meta_columns)\n\n    return {row[indices[HydroBypassNames.id_col]]: (bypass, meta)}\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroBypassNames.HydroBypassNames.get_attribute_data_schema","title":"<code>get_attribute_data_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for attribute data in the Hydropower.Bypass file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for the Bypass attribute data.</p> Source code in <code>framdata/database_names/HydroBypassNames.py</code> <pre><code>@staticmethod\ndef get_attribute_data_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for attribute data in the Hydropower.Bypass file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for the Bypass attribute data.\n\n    \"\"\"\n    return HydroBypassSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroBypassNames.HydroBypassNames.get_metadata_schema","title":"<code>get_metadata_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for the metadata table in the Hydropower.Bypass file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for the Bypass metadata.</p> Source code in <code>framdata/database_names/HydroBypassNames.py</code> <pre><code>@staticmethod\ndef get_metadata_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for the metadata table in the Hydropower.Bypass file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for the Bypass metadata.\n\n    \"\"\"\n    return HydroBypassMetadataSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroBypassNames.HydroBypassSchema","title":"<code>HydroBypassSchema</code>","text":"<p>               Bases: <code>DataFrameModel</code></p> <p>Pandera DataFrameModel schema for attribute data in the Hydropower.Bypass file.</p> Source code in <code>framdata/database_names/HydroBypassNames.py</code> <pre><code>class HydroBypassSchema(pa.DataFrameModel):\n    \"\"\"Pandera DataFrameModel schema for attribute data in the Hydropower.Bypass file.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroGeneratorNames","title":"<code>HydroGeneratorNames</code>","text":"<p>Define the GeneratorNames class and related Pandera schemas for hydropower generator data.</p> <p>Provides: - GeneratorNames: class for handling generator component names and schema validation. - GeneratorSchema: Pandera schema for generator attribute data. - GeneratorMetadataSchema: Pandera schema for generator metadata.</p>"},{"location":"reference/#framdata.database_names.HydroGeneratorNames.GeneratorMetadataSchema","title":"<code>GeneratorMetadataSchema</code>","text":"<p>               Bases: <code>_AttributeMetadataSchema</code></p> <p>Pandera DataFrameModel schema for metadata in the Hydropower.Generators file.</p> Source code in <code>framdata/database_names/HydroGeneratorNames.py</code> <pre><code>class GeneratorMetadataSchema(_AttributeMetadataSchema):\n    \"\"\"Pandera DataFrameModel schema for metadata in the Hydropower.Generators file.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroGeneratorNames.GeneratorSchema","title":"<code>GeneratorSchema</code>","text":"<p>               Bases: <code>DataFrameModel</code></p> <p>Pandera DataFrameModel schema for attribute data in the Hydropower.Generators file.</p> Source code in <code>framdata/database_names/HydroGeneratorNames.py</code> <pre><code>class GeneratorSchema(pa.DataFrameModel):\n    \"\"\"Pandera DataFrameModel schema for attribute data in the Hydropower.Generators file.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroGeneratorNames.HydroGeneratorNames","title":"<code>HydroGeneratorNames</code>","text":"<p>               Bases: <code>_BaseComponentsNames</code></p> <p>Handles generator component names and schema validation for hydropower generator data.</p> <p>Provides methods for creating generator components, retrieving Pandera schemas for attribute and metadata tables, and formatting validation errors specific to generator schemas.</p> Source code in <code>framdata/database_names/HydroGeneratorNames.py</code> <pre><code>class HydroGeneratorNames(_BaseComponentsNames):\n    \"\"\"\n    Handles generator component names and schema validation for hydropower generator data.\n\n    Provides methods for creating generator components, retrieving Pandera schemas for attribute and metadata tables,\n    and formatting validation errors specific to generator schemas.\n    \"\"\"\n\n    id_col = \"GeneratorID\"\n    node_col = \"PowerNode\"\n    pq_curve_col = \"PQCurve\"\n    tailw_elev_col = \"TailwaterElevation\"\n    head_nom_col = \"NominalHead\"\n    en_eq_col = \"EnergyEq\"\n\n    columns: ClassVar[list[str]] = [\n        id_col,\n        node_col,\n        pq_curve_col,\n        tailw_elev_col,\n        head_nom_col,\n        en_eq_col,\n    ]\n\n    ref_columns: ClassVar[list[str]] = [\n        node_col,\n        pq_curve_col,\n        tailw_elev_col,\n        head_nom_col,\n        en_eq_col,\n    ]\n\n    @staticmethod\n    def create_component(\n        row: NDArray,\n        indices: dict[str, int],\n        meta_columns: set[str],\n        meta_data: pd.DataFrame,\n        attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n    ) -&gt; dict[str, tuple[HydroGenerator, dict[str, Meta]]]:\n        \"\"\"\n        Create a hydro generator attribute object.\n\n        Args:\n            row (NDArray): Array containing the values of one table row, represeting one HydroModule object.\n            indices (list[str, int]): Mapping of table's Column names to the array's indices.\n            meta_columns (set[str]): Set of columns used to tag object with memberships.\n            meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n            attribute_objects (dict[str, tuple[object, dict[str, Meta]]], optional): NOT USED, currently only used in HydroModulesNames.\n\n        Returns:\n            dict[str, dict[str, Meta]]: A dictionary with the generator ID as key and the attribute object and metadata as value.\n\n        \"\"\"\n        columns_to_parse = [\n            HydroGeneratorNames.pq_curve_col,\n            HydroGeneratorNames.tailw_elev_col,\n            HydroGeneratorNames.head_nom_col,\n            HydroGeneratorNames.en_eq_col,\n        ]\n\n        arg_user_code = HydroGeneratorNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n        generator = HydroGenerator(\n            power_node=row[indices[HydroGeneratorNames.node_col]],\n            energy_eq=Conversion(level=arg_user_code[HydroGeneratorNames.en_eq_col]),\n            pq_curve=arg_user_code[HydroGeneratorNames.pq_curve_col],\n            tailwater_elevation=arg_user_code[HydroGeneratorNames.tailw_elev_col],\n            nominal_head=arg_user_code[HydroGeneratorNames.head_nom_col],\n        )\n\n        meta = {}\n        HydroGeneratorNames._add_meta(meta, row, indices, meta_columns)\n\n        return {row[indices[HydroGeneratorNames.id_col]]: (generator, meta)}\n\n    @staticmethod\n    def get_attribute_data_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for attribute data in the Hydropower.Generators file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for the Generator attribute data.\n\n        \"\"\"\n        return GeneratorSchema\n\n    @staticmethod\n    def get_metadata_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for the metadata table in the Hydropower.Generators file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for the Generator metadata.\n\n        \"\"\"\n        return GeneratorMetadataSchema\n\n    @staticmethod\n    def _get_unique_check_descriptions() -&gt; dict[str, tuple[str, bool]]:\n        \"\"\"\n        Retrieve a dictionary with descriptons of validation checks that are specific to the Generator schemas.\n\n        Returns:\n            dict[str, tuple[str, bool]]: A dictionary where:\n                - Keys (str): The name of the validation check method.\n                - Values (tuple[str, bool]):\n                    - The first element (str) provides a concise and user-friendly description of the check. E.g. what\n                      caused the validation error or what is required for the check to pass.\n                    - The second element (bool) indicates whether the check is a warning (True) or an error (False).\n\n\n        \"\"\"\n        return None\n\n    @staticmethod\n    def _format_unique_checks(errors: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Format the error DataFrame according to the validation checks that are specific to the Generator schemas.\n\n        Args:\n            errors (pd.DataFrame): The error DataFrame containing validation errors.\n\n        Returns:\n            pd.DataFrame: The updated error DataFrame with formatted rows for unique validation checks.\n\n        \"\"\"\n        return None\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroGeneratorNames.HydroGeneratorNames.create_component","title":"<code>create_component(row: NDArray, indices: dict[str, int], meta_columns: set[str], meta_data: pd.DataFrame, attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None) -&gt; dict[str, tuple[HydroGenerator, dict[str, Meta]]]</code>  <code>staticmethod</code>","text":"<p>Create a hydro generator attribute object.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>NDArray</code> <p>Array containing the values of one table row, represeting one HydroModule object.</p> required <code>indices</code> <code>list[str, int]</code> <p>Mapping of table's Column names to the array's indices.</p> required <code>meta_columns</code> <code>set[str]</code> <p>Set of columns used to tag object with memberships.</p> required <code>meta_data</code> <code>DataFrame</code> <p>Dictionary containing at least unit of every column.</p> required <code>attribute_objects</code> <code>dict[str, tuple[object, dict[str, Meta]]]</code> <p>NOT USED, currently only used in HydroModulesNames.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, tuple[HydroGenerator, dict[str, Meta]]]</code> <p>dict[str, dict[str, Meta]]: A dictionary with the generator ID as key and the attribute object and metadata as value.</p> Source code in <code>framdata/database_names/HydroGeneratorNames.py</code> <pre><code>@staticmethod\ndef create_component(\n    row: NDArray,\n    indices: dict[str, int],\n    meta_columns: set[str],\n    meta_data: pd.DataFrame,\n    attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n) -&gt; dict[str, tuple[HydroGenerator, dict[str, Meta]]]:\n    \"\"\"\n    Create a hydro generator attribute object.\n\n    Args:\n        row (NDArray): Array containing the values of one table row, represeting one HydroModule object.\n        indices (list[str, int]): Mapping of table's Column names to the array's indices.\n        meta_columns (set[str]): Set of columns used to tag object with memberships.\n        meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n        attribute_objects (dict[str, tuple[object, dict[str, Meta]]], optional): NOT USED, currently only used in HydroModulesNames.\n\n    Returns:\n        dict[str, dict[str, Meta]]: A dictionary with the generator ID as key and the attribute object and metadata as value.\n\n    \"\"\"\n    columns_to_parse = [\n        HydroGeneratorNames.pq_curve_col,\n        HydroGeneratorNames.tailw_elev_col,\n        HydroGeneratorNames.head_nom_col,\n        HydroGeneratorNames.en_eq_col,\n    ]\n\n    arg_user_code = HydroGeneratorNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n    generator = HydroGenerator(\n        power_node=row[indices[HydroGeneratorNames.node_col]],\n        energy_eq=Conversion(level=arg_user_code[HydroGeneratorNames.en_eq_col]),\n        pq_curve=arg_user_code[HydroGeneratorNames.pq_curve_col],\n        tailwater_elevation=arg_user_code[HydroGeneratorNames.tailw_elev_col],\n        nominal_head=arg_user_code[HydroGeneratorNames.head_nom_col],\n    )\n\n    meta = {}\n    HydroGeneratorNames._add_meta(meta, row, indices, meta_columns)\n\n    return {row[indices[HydroGeneratorNames.id_col]]: (generator, meta)}\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroGeneratorNames.HydroGeneratorNames.get_attribute_data_schema","title":"<code>get_attribute_data_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for attribute data in the Hydropower.Generators file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for the Generator attribute data.</p> Source code in <code>framdata/database_names/HydroGeneratorNames.py</code> <pre><code>@staticmethod\ndef get_attribute_data_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for attribute data in the Hydropower.Generators file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for the Generator attribute data.\n\n    \"\"\"\n    return GeneratorSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroGeneratorNames.HydroGeneratorNames.get_metadata_schema","title":"<code>get_metadata_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for the metadata table in the Hydropower.Generators file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for the Generator metadata.</p> Source code in <code>framdata/database_names/HydroGeneratorNames.py</code> <pre><code>@staticmethod\ndef get_metadata_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for the metadata table in the Hydropower.Generators file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for the Generator metadata.\n\n    \"\"\"\n    return GeneratorMetadataSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroInflowNames","title":"<code>HydroInflowNames</code>","text":"<p>Define the InflowNames class and related Pandera schemas for handling hydropower inflow data.</p> <p>Includes attribute and metadata schemas.</p>"},{"location":"reference/#framdata.database_names.HydroInflowNames.HydroInflowNames","title":"<code>HydroInflowNames</code>","text":"<p>               Bases: <code>_BaseComponentsNames</code></p> <p>Convert hydropower inflow data to attribute objects for HydroModules. Handle attribute and metadata schema validation.</p> Source code in <code>framdata/database_names/HydroInflowNames.py</code> <pre><code>class HydroInflowNames(_BaseComponentsNames):\n    \"\"\"Convert hydropower inflow data to attribute objects for HydroModules. Handle attribute and metadata schema validation.\"\"\"\n\n    id_col = \"InflowID\"\n    yr_vol_col = \"YearlyVolume\"\n    profile_col = \"InflowProfileID\"\n\n    columns: ClassVar[list[str]] = [\n        id_col,\n        yr_vol_col,\n        profile_col,\n    ]\n\n    ref_columns: ClassVar[list[str]] = [\n        yr_vol_col,\n        profile_col,\n    ]\n\n    @staticmethod\n    def create_component(\n        row: NDArray,\n        indices: dict[str, int],\n        meta_columns: set[str],\n        meta_data: pd.DataFrame,\n        attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n    ) -&gt; dict[str, AvgFlowVolume]:\n        \"\"\"\n        Create a hydro inflow component.\n\n        Args:\n            row (NDArray): Array containing the values of one table row, represeting one HydroModule object.\n            indices (list[str, int]): Mapping of table's Column names to the array's indices.\n            meta_columns (set[str]): Set of columns used to tag object with memberships.\n            meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n            attribute_objects (dict[str, tuple[object, dict[str, Meta]]], optional): NOT USED, currently only used in HydroModulesNames.\n\n        Returns:\n            dict[str, Component]: A dictionary with the inflow ID as key and the module unit as value.\n\n        \"\"\"\n        if HydroInflowNames._ref_period_lacks_profiles(row, indices, [HydroInflowNames.profile_col], meta_data):\n            return {row[indices[HydroInflowNames.id_col]]: None}\n        columns_to_parse = [\n            HydroInflowNames.yr_vol_col,\n            HydroInflowNames.profile_col,\n        ]\n\n        arg_user_code = HydroInflowNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n        inflow = AvgFlowVolume(\n            level=arg_user_code[HydroInflowNames.yr_vol_col],\n            profile=arg_user_code[HydroInflowNames.profile_col],\n        )\n\n        meta = {}\n        HydroInflowNames._add_meta(meta, row, indices, meta_columns)\n\n        return {row[indices[HydroInflowNames.id_col]]: (inflow, meta)}\n\n    @staticmethod\n    def get_attribute_data_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for attribute data in the Hydropower.Inflow file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for the Inflow attribute data.\n\n        \"\"\"\n        return InflowSchema\n\n    @staticmethod\n    def get_metadata_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for the metadata table in the Hydropower.Inflow file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for the Inflow metadata.\n\n        \"\"\"\n        return InflowMetadataSchema\n\n    @staticmethod\n    def _get_unique_check_descriptions() -&gt; dict[str, tuple[str, bool]]:\n        \"\"\"\n        Retrieve a dictionary with descriptons of validation checks that are specific to the Inflow schemas.\n\n        Returns:\n            dict[str, tuple[str, bool]]: A dictionary where:\n                - Keys (str): The name of the validation check method.\n                - Values (tuple[str, bool]):\n                    - The first element (str) provides a concise and user-friendly description of the check. E.g. what\n                      caused the validation error or what is required for the check to pass.\n                    - The second element (bool) indicates whether the check is a warning (True) or an error (False).\n\n\n        \"\"\"\n        return None\n\n    @staticmethod\n    def _format_unique_checks(errors: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Format the error DataFrame according to the validation checks that are specific to the Inflow schemas.\n\n        Args:\n            errors (pd.DataFrame): The error DataFrame containing validation errors.\n\n        Returns:\n            pd.DataFrame: The updated error DataFrame with formatted rows for unique validation checks.\n\n        \"\"\"\n        return None\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroInflowNames.HydroInflowNames.create_component","title":"<code>create_component(row: NDArray, indices: dict[str, int], meta_columns: set[str], meta_data: pd.DataFrame, attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None) -&gt; dict[str, AvgFlowVolume]</code>  <code>staticmethod</code>","text":"<p>Create a hydro inflow component.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>NDArray</code> <p>Array containing the values of one table row, represeting one HydroModule object.</p> required <code>indices</code> <code>list[str, int]</code> <p>Mapping of table's Column names to the array's indices.</p> required <code>meta_columns</code> <code>set[str]</code> <p>Set of columns used to tag object with memberships.</p> required <code>meta_data</code> <code>DataFrame</code> <p>Dictionary containing at least unit of every column.</p> required <code>attribute_objects</code> <code>dict[str, tuple[object, dict[str, Meta]]]</code> <p>NOT USED, currently only used in HydroModulesNames.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, AvgFlowVolume]</code> <p>dict[str, Component]: A dictionary with the inflow ID as key and the module unit as value.</p> Source code in <code>framdata/database_names/HydroInflowNames.py</code> <pre><code>@staticmethod\ndef create_component(\n    row: NDArray,\n    indices: dict[str, int],\n    meta_columns: set[str],\n    meta_data: pd.DataFrame,\n    attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n) -&gt; dict[str, AvgFlowVolume]:\n    \"\"\"\n    Create a hydro inflow component.\n\n    Args:\n        row (NDArray): Array containing the values of one table row, represeting one HydroModule object.\n        indices (list[str, int]): Mapping of table's Column names to the array's indices.\n        meta_columns (set[str]): Set of columns used to tag object with memberships.\n        meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n        attribute_objects (dict[str, tuple[object, dict[str, Meta]]], optional): NOT USED, currently only used in HydroModulesNames.\n\n    Returns:\n        dict[str, Component]: A dictionary with the inflow ID as key and the module unit as value.\n\n    \"\"\"\n    if HydroInflowNames._ref_period_lacks_profiles(row, indices, [HydroInflowNames.profile_col], meta_data):\n        return {row[indices[HydroInflowNames.id_col]]: None}\n    columns_to_parse = [\n        HydroInflowNames.yr_vol_col,\n        HydroInflowNames.profile_col,\n    ]\n\n    arg_user_code = HydroInflowNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n    inflow = AvgFlowVolume(\n        level=arg_user_code[HydroInflowNames.yr_vol_col],\n        profile=arg_user_code[HydroInflowNames.profile_col],\n    )\n\n    meta = {}\n    HydroInflowNames._add_meta(meta, row, indices, meta_columns)\n\n    return {row[indices[HydroInflowNames.id_col]]: (inflow, meta)}\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroInflowNames.HydroInflowNames.get_attribute_data_schema","title":"<code>get_attribute_data_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for attribute data in the Hydropower.Inflow file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for the Inflow attribute data.</p> Source code in <code>framdata/database_names/HydroInflowNames.py</code> <pre><code>@staticmethod\ndef get_attribute_data_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for attribute data in the Hydropower.Inflow file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for the Inflow attribute data.\n\n    \"\"\"\n    return InflowSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroInflowNames.HydroInflowNames.get_metadata_schema","title":"<code>get_metadata_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for the metadata table in the Hydropower.Inflow file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for the Inflow metadata.</p> Source code in <code>framdata/database_names/HydroInflowNames.py</code> <pre><code>@staticmethod\ndef get_metadata_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for the metadata table in the Hydropower.Inflow file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for the Inflow metadata.\n\n    \"\"\"\n    return InflowMetadataSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroInflowNames.InflowMetadataSchema","title":"<code>InflowMetadataSchema</code>","text":"<p>               Bases: <code>_AttributeMetadataSchema</code></p> <p>Pandera DataFrameModel schema for metadata in the Hydropower.Inflow file.</p> Source code in <code>framdata/database_names/HydroInflowNames.py</code> <pre><code>class InflowMetadataSchema(_AttributeMetadataSchema):\n    \"\"\"Pandera DataFrameModel schema for metadata in the Hydropower.Inflow file.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroInflowNames.InflowSchema","title":"<code>InflowSchema</code>","text":"<p>               Bases: <code>DataFrameModel</code></p> <p>Pandera DataFrameModel schema for attribute data in the Hydropower.Inflow file.</p> Source code in <code>framdata/database_names/HydroInflowNames.py</code> <pre><code>class InflowSchema(pa.DataFrameModel):\n    \"\"\"Pandera DataFrameModel schema for attribute data in the Hydropower.Inflow file.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroModulesNames","title":"<code>HydroModulesNames</code>","text":"<p>Defines schema, names, and component creation logic for hydropower modules.</p> <p>This module provides: - HydroModulesNames: class for column names and component creation for hydropower modules. - HydroModuleSchema: Pandera schema for attribute data. - HydroModuleMetadataSchema: Pandera schema for metadata.</p>"},{"location":"reference/#framdata.database_names.HydroModulesNames.HydroModuleMetadataSchema","title":"<code>HydroModuleMetadataSchema</code>","text":"<p>               Bases: <code>_AttributeMetadataSchema</code></p> <p>Pandera DataFrameModel schema for metadata in the Hydropower.Modules file.</p> Source code in <code>framdata/database_names/HydroModulesNames.py</code> <pre><code>class HydroModuleMetadataSchema(_AttributeMetadataSchema):\n    \"\"\"Pandera DataFrameModel schema for metadata in the Hydropower.Modules file.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroModulesNames.HydroModuleSchema","title":"<code>HydroModuleSchema</code>","text":"<p>               Bases: <code>DataFrameModel</code></p> <p>Pandera DataFrameModel schema for attribute data in the Hydropower.Modules file.</p> Source code in <code>framdata/database_names/HydroModulesNames.py</code> <pre><code>class HydroModuleSchema(pa.DataFrameModel):\n    \"\"\"Pandera DataFrameModel schema for attribute data in the Hydropower.Modules file.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroModulesNames.HydroModulesNames","title":"<code>HydroModulesNames</code>","text":"<p>               Bases: <code>_BaseComponentsNames</code></p> <p>Provides column names, schema accessors, and component creation logic for hydropower modules.</p> <p>This class defines constants for column names, methods for creating HydroModule components from data rows, and accessors for Pandera schemas used for validation of attribute and metadata tables.</p> Source code in <code>framdata/database_names/HydroModulesNames.py</code> <pre><code>class HydroModulesNames(_BaseComponentsNames):\n    \"\"\"\n    Provides column names, schema accessors, and component creation logic for hydropower modules.\n\n    This class defines constants for column names, methods for creating HydroModule components from data rows,\n    and accessors for Pandera schemas used for validation of attribute and metadata tables.\n    \"\"\"\n\n    filename = \"Hydropower.Modules\"\n\n    id_col = \"ModuleID\"\n    pump_col = \"PumpID\"\n    gen_col = \"GeneratorID\"\n    res_col = \"ReservoirID\"\n    byp_col = \"BypassID\"\n    hyd_code_col = \"HydraulicCoupling\"\n    inflow_col = \"InflowID\"\n    rel_to_col = \"ReleaseTo\"\n    spill_to_col = \"SpillTo\"\n    rel_cap_col = \"CapacityRelease\"\n    min_bnd_col = \"MinOperationalRelease\"\n    max_bnd_col = \"MaxOperationalRelease\"\n    min_penalty_col = \"MinViolationPenalty\"\n    max_penalty_col = \"MaxViolationPenalty\"\n\n    columns: ClassVar[list[str]] = [\n        id_col,\n        pump_col,\n        gen_col,\n        res_col,\n        byp_col,\n        hyd_code_col,\n        inflow_col,\n        rel_to_col,\n        spill_to_col,\n        rel_cap_col,\n        min_bnd_col,\n        max_bnd_col,\n        min_penalty_col,\n        max_penalty_col,\n    ]\n\n    ref_columns: ClassVar[list[str]] = [\n        rel_to_col,\n        spill_to_col,\n        rel_cap_col,\n        min_bnd_col,\n        max_bnd_col,\n        min_penalty_col,\n        max_penalty_col,\n    ]\n\n    @staticmethod\n    def create_component(\n        row: NDArray,\n        indices: dict[str, int],\n        meta_columns: set[str],\n        meta_data: pd.DataFrame,\n        attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n    ) -&gt; dict[str, Component]:\n        \"\"\"\n        Create a hydro module component.\n\n        Args:\n            row (NDArray): Array containing the values of one table row, represeting one HydroModule object.\n            indices (list[str, int]): Mapping of table's Column names to the array's indices.\n            meta_columns (set[str]): Set of columns used to tag object with memberships.\n            meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n            attribute_objects (dict[str, tuple[object, dict[str, Meta]]], optional): Dictionary of attributes to link to the HydroModule.\n\n        Returns:\n            dict[str, Component]: A dictionary with the module_id as key and the module unit as value.\n\n        \"\"\"\n        columns_to_parse = [\n            HydroModulesNames.rel_cap_col,\n            HydroModulesNames.min_bnd_col,\n            HydroModulesNames.max_bnd_col,\n            HydroModulesNames.min_penalty_col,\n            HydroModulesNames.max_penalty_col,\n        ]\n        name = row[indices[HydroModulesNames.id_col]]\n        inflow_name = indices[HydroModulesNames.inflow_col]\n        pump_name = indices[HydroModulesNames.pump_col]\n        gen_name = indices[HydroModulesNames.gen_col]\n        res_name = indices[HydroModulesNames.res_col]\n        byp_name = indices[HydroModulesNames.byp_col]\n        arg_user_code = HydroModulesNames._parse_args(row, indices, columns_to_parse, meta_data)\n        inflow, inflow_meta = HydroModulesNames._get_attribute_object(\n            attribute_objects,\n            row[inflow_name],\n            name,\n            HydroModule,\n            AvgFlowVolume,\n        )\n        pump, pump_meta = HydroModulesNames._get_attribute_object(\n            attribute_objects,\n            row[pump_name],\n            name,\n            HydroModule,\n            HydroPump,\n        )\n        generator, generator_meta = HydroModulesNames._get_attribute_object(\n            attribute_objects,\n            row[gen_name],\n            name,\n            HydroModule,\n            HydroGenerator,\n        )\n        reservoir, reservoir_meta = HydroModulesNames._get_attribute_object(\n            attribute_objects,\n            row[res_name],\n            name,\n            HydroModule,\n            HydroReservoir,\n        )\n        bypass, bypass_meta = HydroModulesNames._get_attribute_object(\n            attribute_objects,\n            row[byp_name],\n            name,\n            HydroModule,\n            HydroBypass,\n        )\n        module = HydroModule(\n            release_capacity=MaxFlowVolume(level=arg_user_code[HydroModulesNames.rel_cap_col]),\n            hydraulic_coupling=row[indices[HydroModulesNames.hyd_code_col]],\n            inflow=inflow,\n            pump=pump,\n            generator=generator,\n            reservoir=reservoir,\n            bypass=bypass,\n            release_to=row[indices[HydroModulesNames.rel_to_col]],\n            spill_to=row[indices[HydroModulesNames.spill_to_col]],\n        )\n\n        if \"EnergyEqDownstream\" in meta_columns:\n            HydroModulesNames._add_meta(module, row, indices, [\"EnergyEqDownstream\"], unit=\"kWh/m3\")\n\n        meta_columns = [c for c in meta_columns if c != \"EnergyEqDownstream\"]\n        HydroModulesNames._add_meta(module, row, indices, meta_columns)  # fails because Modules want floats in Meta.\n\n        attr_meta = {\n            inflow_name: inflow_meta,\n            pump_name: pump_meta,\n            gen_name: generator_meta,\n            res_name: reservoir_meta,\n            byp_name: bypass_meta,\n        }\n        HydroModulesNames._merge_attribute_meta(\n            name,\n            module,\n            {k: v for k, v in attr_meta.items() if k and v},\n        )\n\n        return {name: module}\n\n    @staticmethod\n    def get_attribute_data_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for attribute data in the Hydropower.Modules file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for the HydroModule attribute data.\n\n        \"\"\"\n        return HydroModuleSchema\n\n    @staticmethod\n    def get_metadata_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for the metadata table in the Hydropower.Modules file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for the HydroModule metadata.\n\n        \"\"\"\n        return HydroModuleMetadataSchema\n\n    @staticmethod\n    def _get_unique_check_descriptions() -&gt; dict[str, tuple[str, bool]]:\n        \"\"\"\n        Retrieve a dictionary with descriptons of validation checks that are specific to the HydroModule schemas.\n\n        Returns:\n            dict[str, tuple[str, bool]]: A dictionary where:\n                - Keys (str): The name of the validation check method.\n                - Values (tuple[str, bool]):\n                    - The first element (str) provides a concise and user-friendly description of the check. E.g. what\n                      caused the validation error or what is required for the check to pass.\n                    - The second element (bool) indicates whether the check is a warning (True) or an error (False).\n\n\n        \"\"\"\n        return None\n\n    @staticmethod\n    def _format_unique_checks(errors: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Format the error DataFrame according to the validation checks that are specific to the HydroModule schemas.\n\n        Args:\n            errors (pd.DataFrame): The error DataFrame containing validation errors.\n\n        Returns:\n            pd.DataFrame: The updated error DataFrame with formatted rows for unique validation checks.\n\n        \"\"\"\n        return None\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroModulesNames.HydroModulesNames.create_component","title":"<code>create_component(row: NDArray, indices: dict[str, int], meta_columns: set[str], meta_data: pd.DataFrame, attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None) -&gt; dict[str, Component]</code>  <code>staticmethod</code>","text":"<p>Create a hydro module component.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>NDArray</code> <p>Array containing the values of one table row, represeting one HydroModule object.</p> required <code>indices</code> <code>list[str, int]</code> <p>Mapping of table's Column names to the array's indices.</p> required <code>meta_columns</code> <code>set[str]</code> <p>Set of columns used to tag object with memberships.</p> required <code>meta_data</code> <code>DataFrame</code> <p>Dictionary containing at least unit of every column.</p> required <code>attribute_objects</code> <code>dict[str, tuple[object, dict[str, Meta]]]</code> <p>Dictionary of attributes to link to the HydroModule.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Component]</code> <p>dict[str, Component]: A dictionary with the module_id as key and the module unit as value.</p> Source code in <code>framdata/database_names/HydroModulesNames.py</code> <pre><code>@staticmethod\ndef create_component(\n    row: NDArray,\n    indices: dict[str, int],\n    meta_columns: set[str],\n    meta_data: pd.DataFrame,\n    attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n) -&gt; dict[str, Component]:\n    \"\"\"\n    Create a hydro module component.\n\n    Args:\n        row (NDArray): Array containing the values of one table row, represeting one HydroModule object.\n        indices (list[str, int]): Mapping of table's Column names to the array's indices.\n        meta_columns (set[str]): Set of columns used to tag object with memberships.\n        meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n        attribute_objects (dict[str, tuple[object, dict[str, Meta]]], optional): Dictionary of attributes to link to the HydroModule.\n\n    Returns:\n        dict[str, Component]: A dictionary with the module_id as key and the module unit as value.\n\n    \"\"\"\n    columns_to_parse = [\n        HydroModulesNames.rel_cap_col,\n        HydroModulesNames.min_bnd_col,\n        HydroModulesNames.max_bnd_col,\n        HydroModulesNames.min_penalty_col,\n        HydroModulesNames.max_penalty_col,\n    ]\n    name = row[indices[HydroModulesNames.id_col]]\n    inflow_name = indices[HydroModulesNames.inflow_col]\n    pump_name = indices[HydroModulesNames.pump_col]\n    gen_name = indices[HydroModulesNames.gen_col]\n    res_name = indices[HydroModulesNames.res_col]\n    byp_name = indices[HydroModulesNames.byp_col]\n    arg_user_code = HydroModulesNames._parse_args(row, indices, columns_to_parse, meta_data)\n    inflow, inflow_meta = HydroModulesNames._get_attribute_object(\n        attribute_objects,\n        row[inflow_name],\n        name,\n        HydroModule,\n        AvgFlowVolume,\n    )\n    pump, pump_meta = HydroModulesNames._get_attribute_object(\n        attribute_objects,\n        row[pump_name],\n        name,\n        HydroModule,\n        HydroPump,\n    )\n    generator, generator_meta = HydroModulesNames._get_attribute_object(\n        attribute_objects,\n        row[gen_name],\n        name,\n        HydroModule,\n        HydroGenerator,\n    )\n    reservoir, reservoir_meta = HydroModulesNames._get_attribute_object(\n        attribute_objects,\n        row[res_name],\n        name,\n        HydroModule,\n        HydroReservoir,\n    )\n    bypass, bypass_meta = HydroModulesNames._get_attribute_object(\n        attribute_objects,\n        row[byp_name],\n        name,\n        HydroModule,\n        HydroBypass,\n    )\n    module = HydroModule(\n        release_capacity=MaxFlowVolume(level=arg_user_code[HydroModulesNames.rel_cap_col]),\n        hydraulic_coupling=row[indices[HydroModulesNames.hyd_code_col]],\n        inflow=inflow,\n        pump=pump,\n        generator=generator,\n        reservoir=reservoir,\n        bypass=bypass,\n        release_to=row[indices[HydroModulesNames.rel_to_col]],\n        spill_to=row[indices[HydroModulesNames.spill_to_col]],\n    )\n\n    if \"EnergyEqDownstream\" in meta_columns:\n        HydroModulesNames._add_meta(module, row, indices, [\"EnergyEqDownstream\"], unit=\"kWh/m3\")\n\n    meta_columns = [c for c in meta_columns if c != \"EnergyEqDownstream\"]\n    HydroModulesNames._add_meta(module, row, indices, meta_columns)  # fails because Modules want floats in Meta.\n\n    attr_meta = {\n        inflow_name: inflow_meta,\n        pump_name: pump_meta,\n        gen_name: generator_meta,\n        res_name: reservoir_meta,\n        byp_name: bypass_meta,\n    }\n    HydroModulesNames._merge_attribute_meta(\n        name,\n        module,\n        {k: v for k, v in attr_meta.items() if k and v},\n    )\n\n    return {name: module}\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroModulesNames.HydroModulesNames.get_attribute_data_schema","title":"<code>get_attribute_data_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for attribute data in the Hydropower.Modules file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for the HydroModule attribute data.</p> Source code in <code>framdata/database_names/HydroModulesNames.py</code> <pre><code>@staticmethod\ndef get_attribute_data_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for attribute data in the Hydropower.Modules file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for the HydroModule attribute data.\n\n    \"\"\"\n    return HydroModuleSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroModulesNames.HydroModulesNames.get_metadata_schema","title":"<code>get_metadata_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for the metadata table in the Hydropower.Modules file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for the HydroModule metadata.</p> Source code in <code>framdata/database_names/HydroModulesNames.py</code> <pre><code>@staticmethod\ndef get_metadata_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for the metadata table in the Hydropower.Modules file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for the HydroModule metadata.\n\n    \"\"\"\n    return HydroModuleMetadataSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroPumpNames","title":"<code>HydroPumpNames</code>","text":"<p>Define the PumpNames class and related Pandera schemas for handling hydropower pump data.</p> <p>Includes attribute and metadata validation for the Hydropower.Pumps file.</p>"},{"location":"reference/#framdata.database_names.HydroPumpNames.HydroPumpNames","title":"<code>HydroPumpNames</code>","text":"<p>               Bases: <code>_BaseComponentsNames</code></p> <p>Handle naming conventions, schema definitions, and component creation for hydropower pump data.</p> Source code in <code>framdata/database_names/HydroPumpNames.py</code> <pre><code>class HydroPumpNames(_BaseComponentsNames):\n    \"\"\"Handle naming conventions, schema definitions, and component creation for hydropower pump data.\"\"\"\n\n    id_col = \"PumpID\"\n    node_col = \"PowerNode\"\n    pump_from_col = \"PumpFrom\"\n    pump_to_col = \"PumpTo\"\n    power_capacity_col = \"PowerCapacity\"\n    vol_capacity_col = \"Capacity\"\n    energy_equiv_col = \"EnergyEq\"\n    h_min_col = \"HeadMin\"\n    h_max_col = \"HeadMax\"\n    q_min_col = \"QMin\"\n    q_max_col = \"QMax\"\n\n    columns: ClassVar[list[str]] = [\n        id_col,\n        node_col,\n        pump_from_col,\n        pump_to_col,\n        power_capacity_col,\n        vol_capacity_col,\n        energy_equiv_col,\n        h_min_col,\n        h_max_col,\n        q_min_col,\n        q_max_col,\n    ]\n\n    ref_columns: ClassVar[list[str]] = [\n        node_col,\n        pump_from_col,\n        pump_to_col,\n        power_capacity_col,\n        vol_capacity_col,\n        energy_equiv_col,\n        h_min_col,\n        h_max_col,\n        q_min_col,\n        q_max_col,\n    ]\n\n    @staticmethod\n    def create_component(\n        row: NDArray,\n        indices: dict[str, int],\n        meta_columns: set[str],\n        meta_data: pd.DataFrame,\n        attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n    ) -&gt; dict[str, HydroPump]:\n        \"\"\"\n        Create a HydroPump object from a row in the Hydropower.Pumps table.\n\n        Args:\n            row (NDArray): Array containing the values of one table row, represeting one HydroModule object.\n            indices (list[str, int]): Mapping of table's Column names to the array's indices.\n            meta_columns (set[str]): Set of columns used to tag object with memberships.\n            meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n            attribute_objects (dict[str, tuple[object, dict[str, Meta]]], optional): NOT USED, currently only used in HydroModulesNames.\n\n        Returns:\n            dict[str, HydroPump]: A dictionary with the pump ID as key and the module unit as value.\n\n        \"\"\"\n        columns_to_parse = [\n            HydroPumpNames.power_capacity_col,\n            HydroPumpNames.vol_capacity_col,\n            HydroPumpNames.energy_equiv_col,\n            HydroPumpNames.h_min_col,\n            HydroPumpNames.h_max_col,\n            HydroPumpNames.q_min_col,\n            HydroPumpNames.q_max_col,\n        ]\n\n        arg_user_code = HydroPumpNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n        pump = HydroPump(\n            power_node=row[indices[HydroPumpNames.node_col]],\n            from_module=row[indices[HydroPumpNames.pump_from_col]],\n            to_module=row[indices[HydroPumpNames.pump_to_col]],\n            water_capacity=MaxFlowVolume(level=arg_user_code[HydroPumpNames.vol_capacity_col]),\n            energy_eq=Conversion(level=arg_user_code[HydroPumpNames.energy_equiv_col]),\n            power_capacity=MaxFlowVolume(level=arg_user_code[HydroPumpNames.power_capacity_col]),\n            head_max=arg_user_code[HydroPumpNames.h_max_col],\n            head_min=arg_user_code[HydroPumpNames.h_min_col],\n            q_max=arg_user_code[HydroPumpNames.q_max_col],\n            q_min=arg_user_code[HydroPumpNames.q_min_col],\n        )\n\n        meta = {}\n        HydroPumpNames._add_meta(meta, row, indices, meta_columns)\n\n        return {row[indices[HydroPumpNames.id_col]]: (pump, meta)}\n\n    @staticmethod\n    def get_attribute_data_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for attribute data in the Hydropower.Pumps file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for the Pump attribute data.\n\n        \"\"\"\n        return PumpSchema\n\n    @staticmethod\n    def get_metadata_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for the metadata table in the Hydropower.Pumps file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for the Pump metadata.\n\n        \"\"\"\n        return PumpMetadataSchema\n\n    @staticmethod\n    def _get_unique_check_descriptions() -&gt; dict[str, tuple[str, bool]]:\n        \"\"\"\n        Retrieve a dictionary with descriptons of validation checks that are specific to the Pump schemas.\n\n        Returns:\n            dict[str, tuple[str, bool]]: A dictionary where:\n                - Keys (str): The name of the validation check method.\n                - Values (tuple[str, bool]):\n                    - The first element (str) provides a concise and user-friendly description of the check. E.g. what\n                      caused the validation error or what is required for the check to pass.\n                    - The second element (bool) indicates whether the check is a warning (True) or an error (False).\n\n\n        \"\"\"\n        return None\n\n    @staticmethod\n    def _format_unique_checks(errors: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Format the error DataFrame according to the validation checks that are specific to the Pump schemas.\n\n        Args:\n            errors (pd.DataFrame): The error DataFrame containing validation errors.\n\n        Returns:\n            pd.DataFrame: The updated error DataFrame with formatted rows for unique validation checks.\n\n        \"\"\"\n        return None\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroPumpNames.HydroPumpNames.create_component","title":"<code>create_component(row: NDArray, indices: dict[str, int], meta_columns: set[str], meta_data: pd.DataFrame, attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None) -&gt; dict[str, HydroPump]</code>  <code>staticmethod</code>","text":"<p>Create a HydroPump object from a row in the Hydropower.Pumps table.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>NDArray</code> <p>Array containing the values of one table row, represeting one HydroModule object.</p> required <code>indices</code> <code>list[str, int]</code> <p>Mapping of table's Column names to the array's indices.</p> required <code>meta_columns</code> <code>set[str]</code> <p>Set of columns used to tag object with memberships.</p> required <code>meta_data</code> <code>DataFrame</code> <p>Dictionary containing at least unit of every column.</p> required <code>attribute_objects</code> <code>dict[str, tuple[object, dict[str, Meta]]]</code> <p>NOT USED, currently only used in HydroModulesNames.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, HydroPump]</code> <p>dict[str, HydroPump]: A dictionary with the pump ID as key and the module unit as value.</p> Source code in <code>framdata/database_names/HydroPumpNames.py</code> <pre><code>@staticmethod\ndef create_component(\n    row: NDArray,\n    indices: dict[str, int],\n    meta_columns: set[str],\n    meta_data: pd.DataFrame,\n    attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n) -&gt; dict[str, HydroPump]:\n    \"\"\"\n    Create a HydroPump object from a row in the Hydropower.Pumps table.\n\n    Args:\n        row (NDArray): Array containing the values of one table row, represeting one HydroModule object.\n        indices (list[str, int]): Mapping of table's Column names to the array's indices.\n        meta_columns (set[str]): Set of columns used to tag object with memberships.\n        meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n        attribute_objects (dict[str, tuple[object, dict[str, Meta]]], optional): NOT USED, currently only used in HydroModulesNames.\n\n    Returns:\n        dict[str, HydroPump]: A dictionary with the pump ID as key and the module unit as value.\n\n    \"\"\"\n    columns_to_parse = [\n        HydroPumpNames.power_capacity_col,\n        HydroPumpNames.vol_capacity_col,\n        HydroPumpNames.energy_equiv_col,\n        HydroPumpNames.h_min_col,\n        HydroPumpNames.h_max_col,\n        HydroPumpNames.q_min_col,\n        HydroPumpNames.q_max_col,\n    ]\n\n    arg_user_code = HydroPumpNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n    pump = HydroPump(\n        power_node=row[indices[HydroPumpNames.node_col]],\n        from_module=row[indices[HydroPumpNames.pump_from_col]],\n        to_module=row[indices[HydroPumpNames.pump_to_col]],\n        water_capacity=MaxFlowVolume(level=arg_user_code[HydroPumpNames.vol_capacity_col]),\n        energy_eq=Conversion(level=arg_user_code[HydroPumpNames.energy_equiv_col]),\n        power_capacity=MaxFlowVolume(level=arg_user_code[HydroPumpNames.power_capacity_col]),\n        head_max=arg_user_code[HydroPumpNames.h_max_col],\n        head_min=arg_user_code[HydroPumpNames.h_min_col],\n        q_max=arg_user_code[HydroPumpNames.q_max_col],\n        q_min=arg_user_code[HydroPumpNames.q_min_col],\n    )\n\n    meta = {}\n    HydroPumpNames._add_meta(meta, row, indices, meta_columns)\n\n    return {row[indices[HydroPumpNames.id_col]]: (pump, meta)}\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroPumpNames.HydroPumpNames.get_attribute_data_schema","title":"<code>get_attribute_data_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for attribute data in the Hydropower.Pumps file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for the Pump attribute data.</p> Source code in <code>framdata/database_names/HydroPumpNames.py</code> <pre><code>@staticmethod\ndef get_attribute_data_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for attribute data in the Hydropower.Pumps file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for the Pump attribute data.\n\n    \"\"\"\n    return PumpSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroPumpNames.HydroPumpNames.get_metadata_schema","title":"<code>get_metadata_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for the metadata table in the Hydropower.Pumps file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for the Pump metadata.</p> Source code in <code>framdata/database_names/HydroPumpNames.py</code> <pre><code>@staticmethod\ndef get_metadata_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for the metadata table in the Hydropower.Pumps file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for the Pump metadata.\n\n    \"\"\"\n    return PumpMetadataSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroPumpNames.PumpMetadataSchema","title":"<code>PumpMetadataSchema</code>","text":"<p>               Bases: <code>_AttributeMetadataSchema</code></p> <p>Pandera DataFrameModel schema for metadata in the Hydropower.Pumps file.</p> Source code in <code>framdata/database_names/HydroPumpNames.py</code> <pre><code>class PumpMetadataSchema(_AttributeMetadataSchema):\n    \"\"\"Pandera DataFrameModel schema for metadata in the Hydropower.Pumps file.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroPumpNames.PumpSchema","title":"<code>PumpSchema</code>","text":"<p>               Bases: <code>DataFrameModel</code></p> <p>Pandera DataFrameModel schema for attribute data in the Hydropower.Pumps file.</p> Source code in <code>framdata/database_names/HydroPumpNames.py</code> <pre><code>class PumpSchema(pa.DataFrameModel):\n    \"\"\"Pandera DataFrameModel schema for attribute data in the Hydropower.Pumps file.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroReservoirNames","title":"<code>HydroReservoirNames</code>","text":"<p>Module for handling reservoir names and schemas in hydropower data.</p> <p>This module defines the ReservoirNames class for managing reservoir attributes, and provides Pandera schemas for validating reservoir attribute and metadata tables.</p>"},{"location":"reference/#framdata.database_names.HydroReservoirNames.HydroReservoirMetadataSchema","title":"<code>HydroReservoirMetadataSchema</code>","text":"<p>               Bases: <code>_AttributeMetadataSchema</code></p> <p>Pandera DataFrameModel schema for metadata in the Hydropower.Reservoirs file.</p> Source code in <code>framdata/database_names/HydroReservoirNames.py</code> <pre><code>class HydroReservoirMetadataSchema(_AttributeMetadataSchema):\n    \"\"\"Pandera DataFrameModel schema for metadata in the Hydropower.Reservoirs file.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroReservoirNames.HydroReservoirNames","title":"<code>HydroReservoirNames</code>","text":"<p>               Bases: <code>_BaseComponentsNames</code></p> <p>Class for managing reservoir attribute names and providing methods for schema validation and component creation.</p> <p>This class defines column names for reservoir attributes, methods for creating HydroReservoir components, and functions to retrieve Pandera schemas for validating reservoir attribute and metadata tables.</p> Source code in <code>framdata/database_names/HydroReservoirNames.py</code> <pre><code>class HydroReservoirNames(_BaseComponentsNames):\n    \"\"\"\n    Class for managing reservoir attribute names and providing methods for schema validation and component creation.\n\n    This class defines column names for reservoir attributes, methods for creating HydroReservoir components,\n    and functions to retrieve Pandera schemas for validating reservoir attribute and metadata tables.\n    \"\"\"\n\n    id_col = \"ReservoirID\"\n    capacity_col = \"Capacity\"\n    res_curve_col = \"ReservoirCurve\"\n    min_res_col = \"MinOperationalFilling\"\n    min_penalty_col = \"MinViolationPenalty\"\n    max_res_col = \"MaxOperationalFilling\"\n    max_penalty_col = \"MaxViolationPenalty\"\n    res_buf_col = \"TargetFilling\"\n    buf_penalty_col = \"TargetViolationPenalty\"\n\n    columns: ClassVar[list[str]] = [\n        id_col,\n        capacity_col,\n        res_curve_col,\n        min_res_col,\n        max_res_col,\n        res_buf_col,\n        min_penalty_col,\n        max_penalty_col,\n        buf_penalty_col,\n    ]\n\n    ref_columns: ClassVar[list[str]] = [\n        capacity_col,\n        res_curve_col,\n        min_res_col,\n        max_res_col,\n        res_buf_col,\n        min_penalty_col,\n        max_penalty_col,\n        buf_penalty_col,\n    ]\n\n    @staticmethod\n    def create_component(\n        row: NDArray,\n        indices: dict[str, int],\n        meta_columns: set[str],\n        meta_data: pd.DataFrame,\n        attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n    ) -&gt; dict[str, HydroReservoir]:\n        \"\"\"\n        Create a HydroReservoir object.\n\n        Args:\n            row (NDArray): Array containing the values of one table row, represeting one HydroModule object.\n            indices (list[str, int]): Mapping of table's Column names to the array's indices.\n            meta_columns (set[str]): Set of columns used to tag object with memberships.\n            meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n            attribute_objects (dict[str, tuple[object, dict[str, Meta]]], optional): NOT USED, currently only used in HydroModulesNames.\n\n        Returns:\n            dict[str, HydroReservoir]: A dictionary with the inflow ID as key and the module unit as value.\n\n        \"\"\"\n        columns_to_parse = [\n            HydroReservoirNames.capacity_col,\n            HydroReservoirNames.res_curve_col,\n            HydroReservoirNames.min_res_col,\n            HydroReservoirNames.max_res_col,\n            HydroReservoirNames.res_buf_col,\n            HydroReservoirNames.min_penalty_col,\n            HydroReservoirNames.max_penalty_col,\n            HydroReservoirNames.buf_penalty_col,\n        ]\n\n        arg_user_code = HydroReservoirNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n        reservoir_curve = ReservoirCurve(arg_user_code[HydroReservoirNames.res_curve_col])\n\n        reservoir = HydroReservoir(\n            capacity=StockVolume(level=arg_user_code[HydroReservoirNames.capacity_col]),\n            reservoir_curve=reservoir_curve,\n        )\n\n        meta = {}\n        HydroReservoirNames._add_meta(meta, row, indices, meta_columns)\n\n        return {row[indices[HydroReservoirNames.id_col]]: (reservoir, meta)}\n\n    @staticmethod\n    def get_attribute_data_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for attribute data in the Hydropower.Reservoirs file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for the Reservoir attribute data.\n\n        \"\"\"\n        return HydroReservoirSchema\n\n    @staticmethod\n    def get_metadata_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for the metadata table in the Hydropower.Reservoirs file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for the Reservoir metadata.\n\n        \"\"\"\n        return HydroReservoirMetadataSchema\n\n    @staticmethod\n    def _get_unique_check_descriptions() -&gt; dict[str, tuple[str, bool]]:\n        \"\"\"\n        Retrieve a dictionary with descriptons of validation checks that are specific to the Reservoir schemas.\n\n        Returns:\n            dict[str, tuple[str, bool]]: A dictionary where:\n                - Keys (str): The name of the validation check method.\n                - Values (tuple[str, bool]):\n                    - The first element (str) provides a concise and user-friendly description of the check. E.g. what\n                      caused the validation error or what is required for the check to pass.\n                    - The second element (bool) indicates whether the check is a warning (True) or an error (False).\n\n\n        \"\"\"\n        return None\n\n    @staticmethod\n    def _format_unique_checks(errors: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Format the error DataFrame according to the validation checks that are specific to the Reservoir schemas.\n\n        Args:\n            errors (pd.DataFrame): The error DataFrame containing validation errors.\n\n        Returns:\n            pd.DataFrame: The updated error DataFrame with formatted rows for unique validation checks.\n\n        \"\"\"\n        return None\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroReservoirNames.HydroReservoirNames.create_component","title":"<code>create_component(row: NDArray, indices: dict[str, int], meta_columns: set[str], meta_data: pd.DataFrame, attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None) -&gt; dict[str, HydroReservoir]</code>  <code>staticmethod</code>","text":"<p>Create a HydroReservoir object.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>NDArray</code> <p>Array containing the values of one table row, represeting one HydroModule object.</p> required <code>indices</code> <code>list[str, int]</code> <p>Mapping of table's Column names to the array's indices.</p> required <code>meta_columns</code> <code>set[str]</code> <p>Set of columns used to tag object with memberships.</p> required <code>meta_data</code> <code>DataFrame</code> <p>Dictionary containing at least unit of every column.</p> required <code>attribute_objects</code> <code>dict[str, tuple[object, dict[str, Meta]]]</code> <p>NOT USED, currently only used in HydroModulesNames.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, HydroReservoir]</code> <p>dict[str, HydroReservoir]: A dictionary with the inflow ID as key and the module unit as value.</p> Source code in <code>framdata/database_names/HydroReservoirNames.py</code> <pre><code>@staticmethod\ndef create_component(\n    row: NDArray,\n    indices: dict[str, int],\n    meta_columns: set[str],\n    meta_data: pd.DataFrame,\n    attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n) -&gt; dict[str, HydroReservoir]:\n    \"\"\"\n    Create a HydroReservoir object.\n\n    Args:\n        row (NDArray): Array containing the values of one table row, represeting one HydroModule object.\n        indices (list[str, int]): Mapping of table's Column names to the array's indices.\n        meta_columns (set[str]): Set of columns used to tag object with memberships.\n        meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n        attribute_objects (dict[str, tuple[object, dict[str, Meta]]], optional): NOT USED, currently only used in HydroModulesNames.\n\n    Returns:\n        dict[str, HydroReservoir]: A dictionary with the inflow ID as key and the module unit as value.\n\n    \"\"\"\n    columns_to_parse = [\n        HydroReservoirNames.capacity_col,\n        HydroReservoirNames.res_curve_col,\n        HydroReservoirNames.min_res_col,\n        HydroReservoirNames.max_res_col,\n        HydroReservoirNames.res_buf_col,\n        HydroReservoirNames.min_penalty_col,\n        HydroReservoirNames.max_penalty_col,\n        HydroReservoirNames.buf_penalty_col,\n    ]\n\n    arg_user_code = HydroReservoirNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n    reservoir_curve = ReservoirCurve(arg_user_code[HydroReservoirNames.res_curve_col])\n\n    reservoir = HydroReservoir(\n        capacity=StockVolume(level=arg_user_code[HydroReservoirNames.capacity_col]),\n        reservoir_curve=reservoir_curve,\n    )\n\n    meta = {}\n    HydroReservoirNames._add_meta(meta, row, indices, meta_columns)\n\n    return {row[indices[HydroReservoirNames.id_col]]: (reservoir, meta)}\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroReservoirNames.HydroReservoirNames.get_attribute_data_schema","title":"<code>get_attribute_data_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for attribute data in the Hydropower.Reservoirs file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for the Reservoir attribute data.</p> Source code in <code>framdata/database_names/HydroReservoirNames.py</code> <pre><code>@staticmethod\ndef get_attribute_data_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for attribute data in the Hydropower.Reservoirs file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for the Reservoir attribute data.\n\n    \"\"\"\n    return HydroReservoirSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroReservoirNames.HydroReservoirNames.get_metadata_schema","title":"<code>get_metadata_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for the metadata table in the Hydropower.Reservoirs file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for the Reservoir metadata.</p> Source code in <code>framdata/database_names/HydroReservoirNames.py</code> <pre><code>@staticmethod\ndef get_metadata_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for the metadata table in the Hydropower.Reservoirs file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for the Reservoir metadata.\n\n    \"\"\"\n    return HydroReservoirMetadataSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.HydroReservoirNames.HydroReservoirSchema","title":"<code>HydroReservoirSchema</code>","text":"<p>               Bases: <code>DataFrameModel</code></p> <p>Pandera DataFrameModel schema for attribute data in the Hydropower.Reservoirs file.</p> Source code in <code>framdata/database_names/HydroReservoirNames.py</code> <pre><code>class HydroReservoirSchema(pa.DataFrameModel):\n    \"\"\"Pandera DataFrameModel schema for attribute data in the Hydropower.Reservoirs file.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/#framdata.database_names.ThermalNames","title":"<code>ThermalNames</code>","text":"<p>Classes defining Thermal tables.</p>"},{"location":"reference/#framdata.database_names.ThermalNames.ThermalMetadataSchema","title":"<code>ThermalMetadataSchema</code>","text":"<p>               Bases: <code>_AttributeMetadataSchema</code></p> <p>Pandera DataFrameModel schema for metadata in the Thermal.Generators file.</p> Source code in <code>framdata/database_names/ThermalNames.py</code> <pre><code>class ThermalMetadataSchema(_AttributeMetadataSchema):\n    \"\"\"Pandera DataFrameModel schema for metadata in the Thermal.Generators file.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/#framdata.database_names.ThermalNames.ThermalNames","title":"<code>ThermalNames</code>","text":"<p>               Bases: <code>_BaseComponentsNames</code></p> <p>Container class for describing the Thermal attribute table's names and structure.</p> Source code in <code>framdata/database_names/ThermalNames.py</code> <pre><code>class ThermalNames(_BaseComponentsNames):\n    \"\"\"Container class for describing the Thermal attribute table's names and structure.\"\"\"\n\n    id_col = \"ThermalID\"\n    main_unit_col = \"MainUnit\"\n    nice_name_col = \"NiceName\"\n    power_node_col = \"PowerNode\"\n    fuel_node_col = \"FuelNode\"\n    emission_node_col = \"EmissionNode\"\n    emission_coeff_col = \"EmissionCoefficient\"\n    type_col = \"Type\"\n    capacity_col = \"Capacity\"\n    full_load_col = \"FullLoadEfficiency\"\n    part_load_col = \"PartLoadEfficiency\"\n    voc_col = \"VOC\"\n    start_costs_col = \"StartCosts\"\n    start_hours_col = \"StartHours\"\n    min_stable_load_col = \"MinStableLoad\"\n    min_op_bound_col = \"MinOperationalBound\"\n    max_op_bound_col = \"MaxOperationalBound\"\n    ramp_up_col = \"RampUp\"\n    ramp_down_col = \"RampDown\"\n\n    # Should include rampup/down data in Thermal, when we get data for this\n    columns: ClassVar[list[str]] = [\n        id_col,\n        nice_name_col,\n        type_col,\n        main_unit_col,\n        power_node_col,\n        fuel_node_col,\n        emission_node_col,\n        capacity_col,\n        full_load_col,\n        part_load_col,\n        voc_col,\n        start_costs_col,\n        start_hours_col,\n        min_stable_load_col,\n        min_op_bound_col,\n        max_op_bound_col,\n        emission_coeff_col,\n    ]\n\n    ref_columns: ClassVar[list[str]] = [\n        power_node_col,\n        fuel_node_col,\n        emission_node_col,\n        capacity_col,\n        full_load_col,\n        part_load_col,\n        voc_col,\n        start_costs_col,\n        start_hours_col,\n        min_stable_load_col,\n        min_op_bound_col,\n        max_op_bound_col,\n        emission_coeff_col,\n    ]\n\n    @staticmethod\n    def create_component(\n        row: NDArray,\n        indices: dict[str, int],\n        meta_columns: set[str],\n        meta_data: pd.DataFrame,\n        attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n    ) -&gt; dict[str, Thermal]:\n        \"\"\"\n        Create a thermal unit component.\n\n        Args:\n            row (NDArray): Array containing the values of one table row, represeting one Thermal object.\n            indices (list[str, int]): Mapping of table's Column names to the array's indices.\n            meta_columns (set[str]): Set of columns used to tag object with memberships.\n            meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n            attribute_objects (dict[str, tuple[object, dict[str, Meta]]], optional): NOT USED\n\n        Returns:\n            dict[str, Thermal]: A dictionary with the thermal_id as key and the thermal unit as value.\n\n        \"\"\"\n        columns_to_parse = [\n            ThermalNames.emission_node_col,\n            ThermalNames.capacity_col,\n            ThermalNames.full_load_col,\n            ThermalNames.part_load_col,\n            ThermalNames.voc_col,\n            ThermalNames.start_costs_col,\n            ThermalNames.start_hours_col,\n            ThermalNames.min_stable_load_col,\n            ThermalNames.min_op_bound_col,\n            ThermalNames.max_op_bound_col,\n            ThermalNames.emission_coeff_col,\n        ]\n\n        arg_user_code = ThermalNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n        no_start_up_costs_condition = (\n            (arg_user_code[ThermalNames.start_costs_col] is None)\n            or (arg_user_code[ThermalNames.min_stable_load_col] is None)\n            or (arg_user_code[ThermalNames.start_hours_col] is None)\n            or (arg_user_code[ThermalNames.part_load_col] is None)\n        )\n        start_up_cost = (\n            None\n            if no_start_up_costs_condition\n            else StartUpCost(\n                startup_cost=Cost(level=arg_user_code[ThermalNames.start_costs_col]),\n                min_stable_load=Proportion(level=arg_user_code[ThermalNames.min_stable_load_col]),\n                start_hours=Hours(level=arg_user_code[ThermalNames.start_hours_col]),\n                part_load_efficiency=Efficiency(level=arg_user_code[ThermalNames.part_load_col]),\n            )\n        )\n\n        voc = (\n            None\n            if arg_user_code[ThermalNames.voc_col] is None\n            else Cost(\n                level=arg_user_code[ThermalNames.voc_col],\n                profile=None,\n            )\n        )\n\n        min_capacity = (\n            None\n            if arg_user_code[ThermalNames.min_op_bound_col] is None\n            else MaxFlowVolume(\n                level=arg_user_code[ThermalNames.capacity_col],\n                profile=arg_user_code[ThermalNames.min_op_bound_col],\n            )\n        )\n\n        thermal = Thermal(\n            power_node=row[indices[ThermalNames.power_node_col]],\n            fuel_node=row[indices[ThermalNames.fuel_node_col]],\n            efficiency=Efficiency(level=arg_user_code[ThermalNames.full_load_col]),\n            emission_node=row[indices[ThermalNames.emission_node_col]],\n            emission_coefficient=Conversion(level=arg_user_code[FuelNodesNames.emission_coefficient_col]),\n            max_capacity=MaxFlowVolume(\n                level=arg_user_code[ThermalNames.capacity_col],\n                profile=arg_user_code[ThermalNames.max_op_bound_col],\n            ),\n            min_capacity=min_capacity,\n            voc=voc,\n            startupcost=start_up_cost,\n        )\n        ThermalNames._add_meta(thermal, row, indices, meta_columns)\n\n        return {row[indices[ThermalNames.id_col]]: thermal}\n\n    @staticmethod\n    def get_attribute_data_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for attribute data in the Thermal.Generators file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for Thermal attribute data.\n\n        \"\"\"\n        return ThermalSchema\n\n    @staticmethod\n    def get_metadata_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for the metadata table in the Thermal.Generators file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for the Thermal metadata.\n\n        \"\"\"\n        return ThermalMetadataSchema\n\n    @staticmethod\n    def _get_unique_check_descriptions() -&gt; dict[str, tuple[str, bool]]:\n        \"\"\"\n        Retrieve a dictionary with descriptons of validation checks that are specific to the Thermal schemas.\n\n        Returns:\n            dict[str, tuple[str, bool]]: A dictionary where:\n                - Keys (str): The name of the validation check method.\n                - Values (tuple[str, bool]):\n                    - The first element (str) provides a concise and user-friendly description of the check. E.g. what\n                      caused the validation error or what is required for the check to pass.\n                    - The second element (bool) indicates whether the check is a warning (True) or an error (False).\n\n\n        \"\"\"\n        return None\n\n    @staticmethod\n    def _format_unique_checks(errors: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Format the error DataFrame according to the validation checks that are specific to the Thermal schemas.\n\n        Args:\n            errors (pd.DataFrame): The error DataFrame containing validation errors.\n\n        Returns:\n            pd.DataFrame: The updated error DataFrame with formatted rows for unique validation checks.\n\n        \"\"\"\n        return None\n</code></pre>"},{"location":"reference/#framdata.database_names.ThermalNames.ThermalNames.create_component","title":"<code>create_component(row: NDArray, indices: dict[str, int], meta_columns: set[str], meta_data: pd.DataFrame, attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None) -&gt; dict[str, Thermal]</code>  <code>staticmethod</code>","text":"<p>Create a thermal unit component.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>NDArray</code> <p>Array containing the values of one table row, represeting one Thermal object.</p> required <code>indices</code> <code>list[str, int]</code> <p>Mapping of table's Column names to the array's indices.</p> required <code>meta_columns</code> <code>set[str]</code> <p>Set of columns used to tag object with memberships.</p> required <code>meta_data</code> <code>DataFrame</code> <p>Dictionary containing at least unit of every column.</p> required <code>attribute_objects</code> <code>dict[str, tuple[object, dict[str, Meta]]]</code> <p>NOT USED</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Thermal]</code> <p>dict[str, Thermal]: A dictionary with the thermal_id as key and the thermal unit as value.</p> Source code in <code>framdata/database_names/ThermalNames.py</code> <pre><code>@staticmethod\ndef create_component(\n    row: NDArray,\n    indices: dict[str, int],\n    meta_columns: set[str],\n    meta_data: pd.DataFrame,\n    attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n) -&gt; dict[str, Thermal]:\n    \"\"\"\n    Create a thermal unit component.\n\n    Args:\n        row (NDArray): Array containing the values of one table row, represeting one Thermal object.\n        indices (list[str, int]): Mapping of table's Column names to the array's indices.\n        meta_columns (set[str]): Set of columns used to tag object with memberships.\n        meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n        attribute_objects (dict[str, tuple[object, dict[str, Meta]]], optional): NOT USED\n\n    Returns:\n        dict[str, Thermal]: A dictionary with the thermal_id as key and the thermal unit as value.\n\n    \"\"\"\n    columns_to_parse = [\n        ThermalNames.emission_node_col,\n        ThermalNames.capacity_col,\n        ThermalNames.full_load_col,\n        ThermalNames.part_load_col,\n        ThermalNames.voc_col,\n        ThermalNames.start_costs_col,\n        ThermalNames.start_hours_col,\n        ThermalNames.min_stable_load_col,\n        ThermalNames.min_op_bound_col,\n        ThermalNames.max_op_bound_col,\n        ThermalNames.emission_coeff_col,\n    ]\n\n    arg_user_code = ThermalNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n    no_start_up_costs_condition = (\n        (arg_user_code[ThermalNames.start_costs_col] is None)\n        or (arg_user_code[ThermalNames.min_stable_load_col] is None)\n        or (arg_user_code[ThermalNames.start_hours_col] is None)\n        or (arg_user_code[ThermalNames.part_load_col] is None)\n    )\n    start_up_cost = (\n        None\n        if no_start_up_costs_condition\n        else StartUpCost(\n            startup_cost=Cost(level=arg_user_code[ThermalNames.start_costs_col]),\n            min_stable_load=Proportion(level=arg_user_code[ThermalNames.min_stable_load_col]),\n            start_hours=Hours(level=arg_user_code[ThermalNames.start_hours_col]),\n            part_load_efficiency=Efficiency(level=arg_user_code[ThermalNames.part_load_col]),\n        )\n    )\n\n    voc = (\n        None\n        if arg_user_code[ThermalNames.voc_col] is None\n        else Cost(\n            level=arg_user_code[ThermalNames.voc_col],\n            profile=None,\n        )\n    )\n\n    min_capacity = (\n        None\n        if arg_user_code[ThermalNames.min_op_bound_col] is None\n        else MaxFlowVolume(\n            level=arg_user_code[ThermalNames.capacity_col],\n            profile=arg_user_code[ThermalNames.min_op_bound_col],\n        )\n    )\n\n    thermal = Thermal(\n        power_node=row[indices[ThermalNames.power_node_col]],\n        fuel_node=row[indices[ThermalNames.fuel_node_col]],\n        efficiency=Efficiency(level=arg_user_code[ThermalNames.full_load_col]),\n        emission_node=row[indices[ThermalNames.emission_node_col]],\n        emission_coefficient=Conversion(level=arg_user_code[FuelNodesNames.emission_coefficient_col]),\n        max_capacity=MaxFlowVolume(\n            level=arg_user_code[ThermalNames.capacity_col],\n            profile=arg_user_code[ThermalNames.max_op_bound_col],\n        ),\n        min_capacity=min_capacity,\n        voc=voc,\n        startupcost=start_up_cost,\n    )\n    ThermalNames._add_meta(thermal, row, indices, meta_columns)\n\n    return {row[indices[ThermalNames.id_col]]: thermal}\n</code></pre>"},{"location":"reference/#framdata.database_names.ThermalNames.ThermalNames.get_attribute_data_schema","title":"<code>get_attribute_data_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for attribute data in the Thermal.Generators file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for Thermal attribute data.</p> Source code in <code>framdata/database_names/ThermalNames.py</code> <pre><code>@staticmethod\ndef get_attribute_data_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for attribute data in the Thermal.Generators file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for Thermal attribute data.\n\n    \"\"\"\n    return ThermalSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.ThermalNames.ThermalNames.get_metadata_schema","title":"<code>get_metadata_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for the metadata table in the Thermal.Generators file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for the Thermal metadata.</p> Source code in <code>framdata/database_names/ThermalNames.py</code> <pre><code>@staticmethod\ndef get_metadata_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for the metadata table in the Thermal.Generators file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for the Thermal metadata.\n\n    \"\"\"\n    return ThermalMetadataSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.ThermalNames.ThermalSchema","title":"<code>ThermalSchema</code>","text":"<p>               Bases: <code>DataFrameModel</code></p> <p>Pandera DataFrameModel schema for attribute data in the Thermal.Generators file.</p> Source code in <code>framdata/database_names/ThermalNames.py</code> <pre><code>class ThermalSchema(pa.DataFrameModel):\n    \"\"\"Pandera DataFrameModel schema for attribute data in the Thermal.Generators file.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/#framdata.database_names.TimeVectorMetadataNames","title":"<code>TimeVectorMetadataNames</code>","text":"<p>Contains names of fields in time vector metadata.</p>"},{"location":"reference/#framdata.database_names.TimeVectorMetadataNames.TimeVectorMetadataNames","title":"<code>TimeVectorMetadataNames</code>","text":"<p>Denote available fields in time vector metadata, and provide functionality for time vector metadata processing.</p> <p>The processing is concerned with casting the metadata fields to correct types and decoding the fields and/or values if they are stored as bytes.</p> Source code in <code>framdata/database_names/TimeVectorMetadataNames.py</code> <pre><code>class TimeVectorMetadataNames:\n    \"\"\"\n    Denote available fields in time vector metadata, and provide functionality for time vector metadata processing.\n\n    The processing is concerned with casting the metadata fields to correct types and decoding the fields and/or values if they are stored as bytes.\n\n    \"\"\"\n\n    ENCODING = \"utf-8\"\n\n    DATETIME_COL = \"DateTime\"\n    # OBS! when adding new metadata entries, you also have to parse them in FileHandler.get_parquet_metadata\n    # otherwise they will not be read.\n    # Metadata fields\n\n    # Id column name\n    ID_COLUMN_NAME = \"ID\"\n\n    # Required bools\n    IS_MAX_LEVEL = \"IsMaxLevel\"\n    IS_ZERO_ONE_PROFILE = \"IsZeroOneProfile\"\n    IS_52_WEEK_YEARS = \"Is52WeekYears\"\n    EXTRAPOLATE_FISRT_POINT = \"ExtrapolateFirstPoint\"\n    EXTRAPOLATE_LAST_POINT = \"ExtrapolateLastPoint\"\n\n    # reference period\n    REF_PERIOD_START_YEAR = \"RefPeriodStartYear\"\n    REF_PERIOD_NUM_YEARS = \"RefPeriodNumberOfYears\"\n\n    START = \"StartDateTime\"\n    FREQUENCY = \"Frequency\"\n    NUM_POINTS = \"NumberOfPoints\"\n    TIMEZONE = \"TimeZone\"\n\n    UNIT = \"Unit\"\n    CURRENCY = \"Currency\"\n\n    # reference_period = \"ReferencePeriod\"\n\n    B_IS_MAX_LEVEL = IS_MAX_LEVEL.encode(ENCODING)\n    B_IS_ZERO_ONE_PROFILE = IS_ZERO_ONE_PROFILE.encode(ENCODING)\n    B_IS_52_WEEK_YEARS = IS_52_WEEK_YEARS.encode(ENCODING)\n    B_ID_COLUMN_NAME = ID_COLUMN_NAME.encode(ENCODING)\n    B_EXTRAPOLATE_FISRT_POINT = EXTRAPOLATE_FISRT_POINT.encode(ENCODING)\n    B_EXTRAPOLATE_LAST_POINT = EXTRAPOLATE_LAST_POINT.encode(ENCODING)\n\n    # reference period\n    B_REF_PERIOD_START_YEAR = REF_PERIOD_START_YEAR.encode(ENCODING)\n    B_REF_PERIOD_NUM_YEARS = REF_PERIOD_NUM_YEARS.encode(ENCODING)\n\n    B_START = START.encode(ENCODING)\n    B_FREQUENCY = FREQUENCY.encode(ENCODING)\n    B_NUM_POINTS = NUM_POINTS.encode(ENCODING)\n    B_TIMEZONE = TIMEZONE.encode(ENCODING)\n    B_UNIT = UNIT.encode(ENCODING)\n    B_CURRENCY = CURRENCY.encode(ENCODING)\n\n    str_keys_to_bytes_map: ClassVar[dict[str, bytes]] = {\n        ID_COLUMN_NAME: B_ID_COLUMN_NAME,\n        IS_MAX_LEVEL: B_IS_MAX_LEVEL,\n        IS_ZERO_ONE_PROFILE: B_IS_ZERO_ONE_PROFILE,\n        IS_52_WEEK_YEARS: B_IS_52_WEEK_YEARS,\n        EXTRAPOLATE_FISRT_POINT: B_EXTRAPOLATE_FISRT_POINT,\n        EXTRAPOLATE_LAST_POINT: B_EXTRAPOLATE_LAST_POINT,\n        REF_PERIOD_START_YEAR: B_REF_PERIOD_START_YEAR,\n        REF_PERIOD_NUM_YEARS: B_REF_PERIOD_NUM_YEARS,\n        START: B_START,\n        FREQUENCY: B_FREQUENCY,\n        NUM_POINTS: B_NUM_POINTS,\n        TIMEZONE: B_TIMEZONE,\n        UNIT: B_UNIT,\n        CURRENCY: B_CURRENCY,\n    }\n\n    strict_bools_cast: ClassVar[set[str]] = {\n        IS_52_WEEK_YEARS,\n        EXTRAPOLATE_FISRT_POINT,\n        EXTRAPOLATE_LAST_POINT,\n    }\n    keys_cast_methods: ClassVar[dict[str, Callable | type]] = {\n        ID_COLUMN_NAME: str,\n        IS_MAX_LEVEL: bool,\n        IS_ZERO_ONE_PROFILE: bool,\n        REF_PERIOD_START_YEAR: int,\n        REF_PERIOD_NUM_YEARS: int,\n        START: pd.to_datetime,\n        FREQUENCY: pd.to_timedelta,\n        NUM_POINTS: int,\n        TIMEZONE: pytz.timezone,\n        UNIT: str,\n        CURRENCY: str,\n    }\n\n    @staticmethod\n    def cast_meta(\n        raw_meta: dict[str | bytes, str | bytes | int | bool | None],\n    ) -&gt; tuple[dict[str, str, bool | int | str | datetime | timedelta | tzinfo | None], set[str]]:\n        \"\"\"\n        Decode possible binary keys and values and cast values of metadata dict to their defined types.\n\n        Args:\n            raw_meta (dict[str  |  bytes, str  |  bytes  |  int  |  bool  |  None]): Dictionary to decode and cast.\n\n        Returns:\n            tuple[dict[str, Any], set[str]]: Decoded and cast dictionary, set of missing keys.\n\n        \"\"\"\n        tvmn = TimeVectorMetadataNames\n        str_bytes_map = tvmn.str_keys_to_bytes_map\n        cast_meta = {key: raw_meta[key] for key in set(str_bytes_map.keys()) | set(str_bytes_map.values()) if key in raw_meta}\n        str_to_bytes_meta = tvmn.bytes_keys_to_str(cast_meta)\n        cast_meta = str_to_bytes_meta if str_to_bytes_meta else cast_meta  # Keys were bytes and we decode to str.\n\n        missing_keys: set[str] = {key for key in str_bytes_map if key not in cast_meta}\n\n        # Update with cast values for strict bools and others.\n        cast_meta.update({key: tvmn.cast_strict_bool_value(cast_meta[key]) for key in tvmn.strict_bools_cast if key in cast_meta})\n        cast_meta.update({key: tvmn.cast_value(cast_meta[key], cast_method) for key, cast_method in tvmn.keys_cast_methods.items() if key in cast_meta})\n\n        return cast_meta, missing_keys\n\n    @staticmethod\n    def str_keys_to_bytes(raw_meta: dict[str, bytes]) -&gt; dict[bytes, bytes]:\n        return {bytes_name: raw_meta[str_name] for str_name, bytes_name in TimeVectorMetadataNames.str_keys_to_bytes_map.items() if str_name in raw_meta}\n\n    @staticmethod\n    def bytes_keys_to_str(raw_meta: dict[bytes, bytes]) -&gt; dict[str, bytes]:\n        return {str_name: raw_meta[bytes_name] for str_name, bytes_name in TimeVectorMetadataNames.str_keys_to_bytes_map.items() if bytes_name in raw_meta}\n\n    @staticmethod\n    def cast_value(value: str | bytes | None, cast_function: Callable | type) -&gt; object | None:\n        \"\"\"\n        Cast a string value into new type, but always return None if value is None or \"None\".\n\n        Args:\n            value (str | None): A string value or None.\n            cast_function (Union[Callable, type]): Function or type with which to cast the value into.\n\n        Raises:\n            RuntimeError: If anything goes wrong in the cast_function.\n\n        Returns:\n            object|None: Value as new type or None.\n\n        \"\"\"\n        if isinstance(value, bytes):\n            if cast_function is bool:\n                return None if value == b\"None\" else value == b\"True\"\n            value = value.decode(encoding=TimeVectorMetadataNames.ENCODING)\n\n        if value is None or value in {\"None\", \"\"}:  # Handle missing values\n            return None\n        try:\n            return cast_function(value)\n        except Exception as e:\n            msg = f\"Could not cast metadata value: {value}. Casting method: {cast_function}\"\n            raise RuntimeError(msg) from e\n\n    @staticmethod\n    def cast_strict_bool_value(value: str | bool | bytes) -&gt; bool:\n        if isinstance(value, bytes):\n            return value == b\"True\"\n        return bool(value)\n</code></pre>"},{"location":"reference/#framdata.database_names.TimeVectorMetadataNames.TimeVectorMetadataNames.cast_meta","title":"<code>cast_meta(raw_meta: dict[str | bytes, str | bytes | int | bool | None]) -&gt; tuple[dict[str, str, bool | int | str | datetime | timedelta | tzinfo | None], set[str]]</code>  <code>staticmethod</code>","text":"<p>Decode possible binary keys and values and cast values of metadata dict to their defined types.</p> <p>Parameters:</p> Name Type Description Default <code>raw_meta</code> <code>dict[str | bytes, str | bytes | int | bool | None]</code> <p>Dictionary to decode and cast.</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, str, bool | int | str | datetime | timedelta | tzinfo | None], set[str]]</code> <p>tuple[dict[str, Any], set[str]]: Decoded and cast dictionary, set of missing keys.</p> Source code in <code>framdata/database_names/TimeVectorMetadataNames.py</code> <pre><code>@staticmethod\ndef cast_meta(\n    raw_meta: dict[str | bytes, str | bytes | int | bool | None],\n) -&gt; tuple[dict[str, str, bool | int | str | datetime | timedelta | tzinfo | None], set[str]]:\n    \"\"\"\n    Decode possible binary keys and values and cast values of metadata dict to their defined types.\n\n    Args:\n        raw_meta (dict[str  |  bytes, str  |  bytes  |  int  |  bool  |  None]): Dictionary to decode and cast.\n\n    Returns:\n        tuple[dict[str, Any], set[str]]: Decoded and cast dictionary, set of missing keys.\n\n    \"\"\"\n    tvmn = TimeVectorMetadataNames\n    str_bytes_map = tvmn.str_keys_to_bytes_map\n    cast_meta = {key: raw_meta[key] for key in set(str_bytes_map.keys()) | set(str_bytes_map.values()) if key in raw_meta}\n    str_to_bytes_meta = tvmn.bytes_keys_to_str(cast_meta)\n    cast_meta = str_to_bytes_meta if str_to_bytes_meta else cast_meta  # Keys were bytes and we decode to str.\n\n    missing_keys: set[str] = {key for key in str_bytes_map if key not in cast_meta}\n\n    # Update with cast values for strict bools and others.\n    cast_meta.update({key: tvmn.cast_strict_bool_value(cast_meta[key]) for key in tvmn.strict_bools_cast if key in cast_meta})\n    cast_meta.update({key: tvmn.cast_value(cast_meta[key], cast_method) for key, cast_method in tvmn.keys_cast_methods.items() if key in cast_meta})\n\n    return cast_meta, missing_keys\n</code></pre>"},{"location":"reference/#framdata.database_names.TimeVectorMetadataNames.TimeVectorMetadataNames.cast_value","title":"<code>cast_value(value: str | bytes | None, cast_function: Callable | type) -&gt; object | None</code>  <code>staticmethod</code>","text":"<p>Cast a string value into new type, but always return None if value is None or \"None\".</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str | None</code> <p>A string value or None.</p> required <code>cast_function</code> <code>Union[Callable, type]</code> <p>Function or type with which to cast the value into.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If anything goes wrong in the cast_function.</p> <p>Returns:</p> Type Description <code>object | None</code> <p>object|None: Value as new type or None.</p> Source code in <code>framdata/database_names/TimeVectorMetadataNames.py</code> <pre><code>@staticmethod\ndef cast_value(value: str | bytes | None, cast_function: Callable | type) -&gt; object | None:\n    \"\"\"\n    Cast a string value into new type, but always return None if value is None or \"None\".\n\n    Args:\n        value (str | None): A string value or None.\n        cast_function (Union[Callable, type]): Function or type with which to cast the value into.\n\n    Raises:\n        RuntimeError: If anything goes wrong in the cast_function.\n\n    Returns:\n        object|None: Value as new type or None.\n\n    \"\"\"\n    if isinstance(value, bytes):\n        if cast_function is bool:\n            return None if value == b\"None\" else value == b\"True\"\n        value = value.decode(encoding=TimeVectorMetadataNames.ENCODING)\n\n    if value is None or value in {\"None\", \"\"}:  # Handle missing values\n        return None\n    try:\n        return cast_function(value)\n    except Exception as e:\n        msg = f\"Could not cast metadata value: {value}. Casting method: {cast_function}\"\n        raise RuntimeError(msg) from e\n</code></pre>"},{"location":"reference/#framdata.database_names.TransmissionNames","title":"<code>TransmissionNames</code>","text":"<p>Defines the TransmissionNames class and related Pandera schemas.</p> <p>These describe validate Transmission attributes and metadata tables in the energy model database.</p>"},{"location":"reference/#framdata.database_names.TransmissionNames.TransmissionMetadataSchema","title":"<code>TransmissionMetadataSchema</code>","text":"<p>               Bases: <code>_AttributeMetadataSchema</code></p> <p>Pandera DataFrameModel schema for metadata in the Transmission.Grid file.</p> Source code in <code>framdata/database_names/TransmissionNames.py</code> <pre><code>class TransmissionMetadataSchema(_AttributeMetadataSchema):\n    \"\"\"Pandera DataFrameModel schema for metadata in the Transmission.Grid file.\"\"\"\n\n    @pa.dataframe_check\n    @classmethod\n    def check_unit_is_str_for_attributes(cls, df: pd.DataFrame) -&gt; Series[bool]:\n        \"\"\"\n        Check that the 'unit' value is a string for the rows where 'attribute' is 'Capacity' and 'Loss'.\n\n        Args:\n            df (Dataframe): DataFrame used to check value for \"unit\".\n\n        Returns:\n            Series[bool]: Series of boolean values detonating if each element has passed the check.\n\n        \"\"\"\n        return check_unit_is_str_for_attributes(df, [TransmissionNames.capacity_col, TransmissionNames.tariff_col])\n</code></pre>"},{"location":"reference/#framdata.database_names.TransmissionNames.TransmissionMetadataSchema.check_unit_is_str_for_attributes","title":"<code>check_unit_is_str_for_attributes(df: pd.DataFrame) -&gt; Series[bool]</code>  <code>classmethod</code>","text":"<p>Check that the 'unit' value is a string for the rows where 'attribute' is 'Capacity' and 'Loss'.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Dataframe</code> <p>DataFrame used to check value for \"unit\".</p> required <p>Returns:</p> Type Description <code>Series[bool]</code> <p>Series[bool]: Series of boolean values detonating if each element has passed the check.</p> Source code in <code>framdata/database_names/TransmissionNames.py</code> <pre><code>@pa.dataframe_check\n@classmethod\ndef check_unit_is_str_for_attributes(cls, df: pd.DataFrame) -&gt; Series[bool]:\n    \"\"\"\n    Check that the 'unit' value is a string for the rows where 'attribute' is 'Capacity' and 'Loss'.\n\n    Args:\n        df (Dataframe): DataFrame used to check value for \"unit\".\n\n    Returns:\n        Series[bool]: Series of boolean values detonating if each element has passed the check.\n\n    \"\"\"\n    return check_unit_is_str_for_attributes(df, [TransmissionNames.capacity_col, TransmissionNames.tariff_col])\n</code></pre>"},{"location":"reference/#framdata.database_names.TransmissionNames.TransmissionNames","title":"<code>TransmissionNames</code>","text":"<p>               Bases: <code>_BaseComponentsNames</code></p> <p>Container class for describing the Transmission attribute table's names and structure.</p> Source code in <code>framdata/database_names/TransmissionNames.py</code> <pre><code>class TransmissionNames(_BaseComponentsNames):\n    \"\"\"Container class for describing the Transmission attribute table's names and structure.\"\"\"\n\n    id_col = \"TransmissionID\"\n    from_node_col = \"FromNode\"\n    to_node_col = \"ToNode\"\n    capacity_col = \"Capacity\"\n    loss_col = \"Loss\"\n    tariff_col = \"Tariff\"\n    max_op_bound_col = \"MaxOperationalBound\"\n    min_op_bound_col = \"MinOperationalBound\"\n    ramp_up_col = \"RampUp\"\n    ramp_down_col = \"RampDown\"\n\n    columns: ClassVar[list[str]] = [\n        id_col,\n        from_node_col,\n        to_node_col,\n        capacity_col,\n        loss_col,\n        tariff_col,\n        max_op_bound_col,\n        min_op_bound_col,\n        ramp_up_col,\n        ramp_down_col,\n    ]\n\n    ref_columns: ClassVar[list[str]] = [\n        from_node_col,\n        to_node_col,\n        capacity_col,\n        loss_col,\n        tariff_col,\n        max_op_bound_col,\n        min_op_bound_col,\n        ramp_up_col,\n        ramp_down_col,\n    ]\n\n    @staticmethod\n    def create_component(\n        row: NDArray,\n        indices: dict[str, int],\n        meta_columns: set[str],\n        meta_data: pd.DataFrame,\n        attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n    ) -&gt; dict[str, Transmission]:\n        \"\"\"\n        Create a transmission unit component.\n\n        Args:\n            row (NDArray): Array containing the values of one table row, represeting one Transmission object.\n            indices (list[str, int]): Mapping of table's Column names to the array's indices.\n            meta_columns (set[str]): Set of columns used to tag object with memberships.\n            meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n            attribute_objects (dict[str, tuple[object, dict[str, Meta]]] | None, optional): NOT USED\n\n        Returns:\n            dict[str, Transmission]: A dictionary with the transmission_id as key and the transmission unit as value.\n\n        \"\"\"\n        columns_to_parse = [\n            TransmissionNames.capacity_col,\n            TransmissionNames.loss_col,\n            TransmissionNames.tariff_col,\n            TransmissionNames.max_op_bound_col,\n            TransmissionNames.min_op_bound_col,\n            TransmissionNames.ramp_up_col,\n            TransmissionNames.ramp_down_col,\n        ]\n\n        arg_user_code = TransmissionNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n        ramp_up = None if arg_user_code[TransmissionNames.ramp_up_col] is None else Proportion(level=arg_user_code[TransmissionNames.ramp_up_col])\n        ramp_down = None if arg_user_code[TransmissionNames.ramp_down_col] is None else Proportion(level=arg_user_code[TransmissionNames.ramp_down_col])\n        loss = None if arg_user_code[TransmissionNames.loss_col] is None else Loss(level=arg_user_code[TransmissionNames.loss_col])\n\n        tariff = None if arg_user_code[TransmissionNames.tariff_col] is None else Cost(level=arg_user_code[TransmissionNames.tariff_col])\n\n        min_capacity = (\n            None\n            if arg_user_code[TransmissionNames.min_op_bound_col] is None\n            else MaxFlowVolume(\n                level=arg_user_code[TransmissionNames.capacity_col],\n                profile=arg_user_code[TransmissionNames.min_op_bound_col],\n            )\n        )\n\n        transmission = Transmission(\n            from_node=row[indices[TransmissionNames.from_node_col]],\n            to_node=row[indices[TransmissionNames.to_node_col]],\n            max_capacity=MaxFlowVolume(\n                level=arg_user_code[TransmissionNames.capacity_col],\n                profile=arg_user_code[TransmissionNames.max_op_bound_col],\n            ),\n            min_capacity=min_capacity,\n            loss=loss,\n            tariff=tariff,\n            ramp_up=ramp_up,\n            ramp_down=ramp_down,\n        )\n        TransmissionNames._add_meta(transmission, row, indices, meta_columns)\n\n        return {row[indices[TransmissionNames.id_col]]: transmission}\n\n    @staticmethod\n    def get_attribute_data_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for attribute data in the Transmission.Grid file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for Transmission attribute data.\n\n        \"\"\"\n        return TransmissionSchema\n\n    @staticmethod\n    def get_metadata_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for the metadata table in the Transmission.Grid file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for the Transmission metadata.\n\n        \"\"\"\n        return TransmissionMetadataSchema\n\n    @staticmethod\n    def _get_unique_check_descriptions() -&gt; dict[str, tuple[str, bool]]:\n        \"\"\"\n        Retrieve a dictionary with descriptons of validation checks that are specific to the Transmission schemas.\n\n        Returns:\n            dict[str, tuple[str, bool]]: A dictionary where:\n                - Keys (str): The name of the validation check method.\n                - Values (tuple[str, bool]):\n                    - The first element (str) provides a concise and user-friendly description of the check. E.g. what\n                      caused the validation error or what is required for the check to pass.\n                    - The second element (bool) indicates whether the check is a warning (True) or an error (False).\n\n\n        \"\"\"\n        return {\n            \"check_internal_line_error\": (\"Transmission line is internal (FromNode equals ToNode).\", False),\n        }\n\n    @staticmethod\n    def _format_unique_checks(errors: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Format the error DataFrame according to the validation checks that are specific to the Transmission schemas.\n\n        Args:\n            errors (pd.DataFrame): The error DataFrame containing validation errors.\n\n        Returns:\n            pd.DataFrame: The updated error DataFrame with formatted rows for unique validation checks.\n\n        \"\"\"\n        check_name = \"check_internal_line_error\"\n        if check_name in errors[TransmissionNames.COL_CHECK].to_numpy():\n            check_rows = errors.loc[\n                (errors[TransmissionNames.COL_CHECK] == check_name)\n                &amp; (\n                    errors[TransmissionNames.COL_COLUMN].isin(\n                        [TransmissionNames.from_node_col, TransmissionNames.to_node_col],\n                    )\n                )\n            ]\n            check_rows.loc[:, TransmissionNames.COL_COLUMN] = f\"{TransmissionNames.from_node_col}, {TransmissionNames.to_node_col}\"\n            check_rows = check_rows.drop_duplicates()\n            errors = errors[~(errors[TransmissionNames.COL_CHECK] == check_name)]\n            errors = pd.concat([errors, check_rows], ignore_index=True)\n\n        return errors\n</code></pre>"},{"location":"reference/#framdata.database_names.TransmissionNames.TransmissionNames.create_component","title":"<code>create_component(row: NDArray, indices: dict[str, int], meta_columns: set[str], meta_data: pd.DataFrame, attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None) -&gt; dict[str, Transmission]</code>  <code>staticmethod</code>","text":"<p>Create a transmission unit component.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>NDArray</code> <p>Array containing the values of one table row, represeting one Transmission object.</p> required <code>indices</code> <code>list[str, int]</code> <p>Mapping of table's Column names to the array's indices.</p> required <code>meta_columns</code> <code>set[str]</code> <p>Set of columns used to tag object with memberships.</p> required <code>meta_data</code> <code>DataFrame</code> <p>Dictionary containing at least unit of every column.</p> required <code>attribute_objects</code> <code>dict[str, tuple[object, dict[str, Meta]]] | None</code> <p>NOT USED</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Transmission]</code> <p>dict[str, Transmission]: A dictionary with the transmission_id as key and the transmission unit as value.</p> Source code in <code>framdata/database_names/TransmissionNames.py</code> <pre><code>@staticmethod\ndef create_component(\n    row: NDArray,\n    indices: dict[str, int],\n    meta_columns: set[str],\n    meta_data: pd.DataFrame,\n    attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n) -&gt; dict[str, Transmission]:\n    \"\"\"\n    Create a transmission unit component.\n\n    Args:\n        row (NDArray): Array containing the values of one table row, represeting one Transmission object.\n        indices (list[str, int]): Mapping of table's Column names to the array's indices.\n        meta_columns (set[str]): Set of columns used to tag object with memberships.\n        meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n        attribute_objects (dict[str, tuple[object, dict[str, Meta]]] | None, optional): NOT USED\n\n    Returns:\n        dict[str, Transmission]: A dictionary with the transmission_id as key and the transmission unit as value.\n\n    \"\"\"\n    columns_to_parse = [\n        TransmissionNames.capacity_col,\n        TransmissionNames.loss_col,\n        TransmissionNames.tariff_col,\n        TransmissionNames.max_op_bound_col,\n        TransmissionNames.min_op_bound_col,\n        TransmissionNames.ramp_up_col,\n        TransmissionNames.ramp_down_col,\n    ]\n\n    arg_user_code = TransmissionNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n    ramp_up = None if arg_user_code[TransmissionNames.ramp_up_col] is None else Proportion(level=arg_user_code[TransmissionNames.ramp_up_col])\n    ramp_down = None if arg_user_code[TransmissionNames.ramp_down_col] is None else Proportion(level=arg_user_code[TransmissionNames.ramp_down_col])\n    loss = None if arg_user_code[TransmissionNames.loss_col] is None else Loss(level=arg_user_code[TransmissionNames.loss_col])\n\n    tariff = None if arg_user_code[TransmissionNames.tariff_col] is None else Cost(level=arg_user_code[TransmissionNames.tariff_col])\n\n    min_capacity = (\n        None\n        if arg_user_code[TransmissionNames.min_op_bound_col] is None\n        else MaxFlowVolume(\n            level=arg_user_code[TransmissionNames.capacity_col],\n            profile=arg_user_code[TransmissionNames.min_op_bound_col],\n        )\n    )\n\n    transmission = Transmission(\n        from_node=row[indices[TransmissionNames.from_node_col]],\n        to_node=row[indices[TransmissionNames.to_node_col]],\n        max_capacity=MaxFlowVolume(\n            level=arg_user_code[TransmissionNames.capacity_col],\n            profile=arg_user_code[TransmissionNames.max_op_bound_col],\n        ),\n        min_capacity=min_capacity,\n        loss=loss,\n        tariff=tariff,\n        ramp_up=ramp_up,\n        ramp_down=ramp_down,\n    )\n    TransmissionNames._add_meta(transmission, row, indices, meta_columns)\n\n    return {row[indices[TransmissionNames.id_col]]: transmission}\n</code></pre>"},{"location":"reference/#framdata.database_names.TransmissionNames.TransmissionNames.get_attribute_data_schema","title":"<code>get_attribute_data_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for attribute data in the Transmission.Grid file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for Transmission attribute data.</p> Source code in <code>framdata/database_names/TransmissionNames.py</code> <pre><code>@staticmethod\ndef get_attribute_data_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for attribute data in the Transmission.Grid file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for Transmission attribute data.\n\n    \"\"\"\n    return TransmissionSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.TransmissionNames.TransmissionNames.get_metadata_schema","title":"<code>get_metadata_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for the metadata table in the Transmission.Grid file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for the Transmission metadata.</p> Source code in <code>framdata/database_names/TransmissionNames.py</code> <pre><code>@staticmethod\ndef get_metadata_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for the metadata table in the Transmission.Grid file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for the Transmission metadata.\n\n    \"\"\"\n    return TransmissionMetadataSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.TransmissionNames.TransmissionSchema","title":"<code>TransmissionSchema</code>","text":"<p>               Bases: <code>DataFrameModel</code></p> <p>Pandera DataFrameModel schema for attribute data in the Transmission.Grid file.</p> Source code in <code>framdata/database_names/TransmissionNames.py</code> <pre><code>class TransmissionSchema(pa.DataFrameModel):\n    \"\"\"Pandera DataFrameModel schema for attribute data in the Transmission.Grid file.\"\"\"\n\n    TransmissionID: Series[str] = pa.Field(unique=True, nullable=False)\n    FromNode: Series[str] = pa.Field(nullable=False)\n    ToNode: Series[str] = pa.Field(nullable=False)\n    Capacity: Series[Any] = pa.Field(nullable=False)\n    Loss: Series[Any] = pa.Field(nullable=True)\n    Tariff: Series[Any] = pa.Field(nullable=True)\n    MaxOperationalBound: Series[Any] = pa.Field(nullable=True)\n    MinOperationalBound: Series[Any] = pa.Field(nullable=True)\n    RampUp: Series[Any] = pa.Field(nullable=True)\n    RampDown: Series[Any] = pa.Field(nullable=True)\n\n    @pa.check(TransmissionNames.capacity_col)\n    @classmethod\n    def dtype_str_int_float(cls, series: Series[Any]) -&gt; Series[bool]:\n        \"\"\"Check if values in the series are of datatype: str, int or float.\"\"\"\n        return dtype_str_int_float(series)\n\n    @pa.check(\n        TransmissionNames.loss_col,\n        TransmissionNames.tariff_col,\n        TransmissionNames.max_op_bound_col,\n        TransmissionNames.min_op_bound_col,\n        TransmissionNames.ramp_up_col,\n        TransmissionNames.ramp_down_col,\n    )\n    @classmethod\n    def dtype_str_int_float_none(cls, series: Series[Any]) -&gt; Series[bool]:\n        \"\"\"Check if values in the series are of datatype: str, int, float or None.\"\"\"\n        return dtype_str_int_float_none(series)\n\n    @pa.check(TransmissionNames.capacity_col)\n    @classmethod\n    def numeric_values_greater_than_or_equal_to_0(cls, series: Series[Any]) -&gt; Series[bool]:\n        \"\"\"Check if numeric values in the series are greater than or equal to zero.\"\"\"\n        return numeric_values_greater_than_or_equal_to(series, 0)\n\n    @pa.check(TransmissionNames.loss_col)\n    @classmethod\n    def numeric_values_are_between_or_equal_to_0_and_1(cls, series: Series[Any]) -&gt; Series[bool]:\n        \"\"\"Check if numeric values in the series are between zero and one or equal to zero and one.\"\"\"\n        return numeric_values_are_between_or_equal_to(series, 0, 1)\n\n    @pa.dataframe_check\n    @classmethod\n    def check_internal_line_error(cls, dataframe: pd.DataFrame) -&gt; Series[bool]:\n        \"\"\"\n        Raise warning if origin node is the same as destination node, in which case we have an internal line.\n\n        Args:\n            dataframe (pd.DataFrame): DataFrame to check.\n\n        Returns:\n            Series[bool]: Series of boolean values denoting if each element has passed the check.\n\n        \"\"\"\n        return dataframe[TransmissionNames.from_node_col] != dataframe[TransmissionNames.to_node_col]\n\n    class Config:\n        \"\"\"Schema-wide configuration for the DemandSchema class.\"\"\"\n\n        unique_column_names = True\n</code></pre>"},{"location":"reference/#framdata.database_names.TransmissionNames.TransmissionSchema.Config","title":"<code>Config</code>","text":"<p>Schema-wide configuration for the DemandSchema class.</p> Source code in <code>framdata/database_names/TransmissionNames.py</code> <pre><code>class Config:\n    \"\"\"Schema-wide configuration for the DemandSchema class.\"\"\"\n\n    unique_column_names = True\n</code></pre>"},{"location":"reference/#framdata.database_names.TransmissionNames.TransmissionSchema.check_internal_line_error","title":"<code>check_internal_line_error(dataframe: pd.DataFrame) -&gt; Series[bool]</code>  <code>classmethod</code>","text":"<p>Raise warning if origin node is the same as destination node, in which case we have an internal line.</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>DataFrame</code> <p>DataFrame to check.</p> required <p>Returns:</p> Type Description <code>Series[bool]</code> <p>Series[bool]: Series of boolean values denoting if each element has passed the check.</p> Source code in <code>framdata/database_names/TransmissionNames.py</code> <pre><code>@pa.dataframe_check\n@classmethod\ndef check_internal_line_error(cls, dataframe: pd.DataFrame) -&gt; Series[bool]:\n    \"\"\"\n    Raise warning if origin node is the same as destination node, in which case we have an internal line.\n\n    Args:\n        dataframe (pd.DataFrame): DataFrame to check.\n\n    Returns:\n        Series[bool]: Series of boolean values denoting if each element has passed the check.\n\n    \"\"\"\n    return dataframe[TransmissionNames.from_node_col] != dataframe[TransmissionNames.to_node_col]\n</code></pre>"},{"location":"reference/#framdata.database_names.TransmissionNames.TransmissionSchema.dtype_str_int_float","title":"<code>dtype_str_int_float(series: Series[Any]) -&gt; Series[bool]</code>  <code>classmethod</code>","text":"<p>Check if values in the series are of datatype: str, int or float.</p> Source code in <code>framdata/database_names/TransmissionNames.py</code> <pre><code>@pa.check(TransmissionNames.capacity_col)\n@classmethod\ndef dtype_str_int_float(cls, series: Series[Any]) -&gt; Series[bool]:\n    \"\"\"Check if values in the series are of datatype: str, int or float.\"\"\"\n    return dtype_str_int_float(series)\n</code></pre>"},{"location":"reference/#framdata.database_names.TransmissionNames.TransmissionSchema.dtype_str_int_float_none","title":"<code>dtype_str_int_float_none(series: Series[Any]) -&gt; Series[bool]</code>  <code>classmethod</code>","text":"<p>Check if values in the series are of datatype: str, int, float or None.</p> Source code in <code>framdata/database_names/TransmissionNames.py</code> <pre><code>@pa.check(\n    TransmissionNames.loss_col,\n    TransmissionNames.tariff_col,\n    TransmissionNames.max_op_bound_col,\n    TransmissionNames.min_op_bound_col,\n    TransmissionNames.ramp_up_col,\n    TransmissionNames.ramp_down_col,\n)\n@classmethod\ndef dtype_str_int_float_none(cls, series: Series[Any]) -&gt; Series[bool]:\n    \"\"\"Check if values in the series are of datatype: str, int, float or None.\"\"\"\n    return dtype_str_int_float_none(series)\n</code></pre>"},{"location":"reference/#framdata.database_names.TransmissionNames.TransmissionSchema.numeric_values_are_between_or_equal_to_0_and_1","title":"<code>numeric_values_are_between_or_equal_to_0_and_1(series: Series[Any]) -&gt; Series[bool]</code>  <code>classmethod</code>","text":"<p>Check if numeric values in the series are between zero and one or equal to zero and one.</p> Source code in <code>framdata/database_names/TransmissionNames.py</code> <pre><code>@pa.check(TransmissionNames.loss_col)\n@classmethod\ndef numeric_values_are_between_or_equal_to_0_and_1(cls, series: Series[Any]) -&gt; Series[bool]:\n    \"\"\"Check if numeric values in the series are between zero and one or equal to zero and one.\"\"\"\n    return numeric_values_are_between_or_equal_to(series, 0, 1)\n</code></pre>"},{"location":"reference/#framdata.database_names.TransmissionNames.TransmissionSchema.numeric_values_greater_than_or_equal_to_0","title":"<code>numeric_values_greater_than_or_equal_to_0(series: Series[Any]) -&gt; Series[bool]</code>  <code>classmethod</code>","text":"<p>Check if numeric values in the series are greater than or equal to zero.</p> Source code in <code>framdata/database_names/TransmissionNames.py</code> <pre><code>@pa.check(TransmissionNames.capacity_col)\n@classmethod\ndef numeric_values_greater_than_or_equal_to_0(cls, series: Series[Any]) -&gt; Series[bool]:\n    \"\"\"Check if numeric values in the series are greater than or equal to zero.\"\"\"\n    return numeric_values_greater_than_or_equal_to(series, 0)\n</code></pre>"},{"location":"reference/#framdata.database_names.WindSolarNames","title":"<code>WindSolarNames</code>","text":"<p>Classes defining Wind and Solar tables and how to create Components from them.</p>"},{"location":"reference/#framdata.database_names.WindSolarNames.SolarNames","title":"<code>SolarNames</code>","text":"<p>               Bases: <code>WindSolarNames</code></p> <p>Class representing the names and structure of Solar tables, and method for creating Solar Component objects.</p> Source code in <code>framdata/database_names/WindSolarNames.py</code> <pre><code>class SolarNames(WindSolarNames):\n    \"\"\"Class representing the names and structure of Solar tables, and method for creating Solar Component objects.\"\"\"\n\n    @staticmethod\n    def create_component(\n        row: NDArray,\n        indices: dict[str, int],\n        meta_columns: set[str],\n        meta_data: pd.DataFrame,\n        attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n    ) -&gt; dict[str, Solar]:\n        \"\"\"\n        Create a Solar Component from a row in the Solar.Generators table.\n\n        Args:\n            row (NDArray): Array containing the values of one table row, represeting one solar object.\n            indices (list[str, int]): Mapping of table's Column names to the array's indices.\n            meta_columns (set[str]): Set of columns used to tag object with memberships.\n            meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n            attribute_objects (dict[str, tuple[object, dict[str, Meta]]] | None, optional): NOT USED\n\n        Returns:\n            dict[str, Solar]: A dictionary with the id as key and the solar unit as value.\n\n        \"\"\"\n        columns_to_parse = [\n            SolarNames.profile_col,\n            SolarNames.capacity_col,\n        ]\n\n        arg_user_code = SolarNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n        solar = Solar(\n            power_node=row[indices[SolarNames.power_node_col]],\n            max_capacity=MaxFlowVolume(\n                level=arg_user_code[SolarNames.capacity_col],\n                profile=arg_user_code[SolarNames.profile_col],\n            ),\n            voc=None,\n        )\n\n        SolarNames._add_meta(solar, row, indices, meta_columns)\n\n        return {row[indices[SolarNames.id_col]]: solar}\n</code></pre>"},{"location":"reference/#framdata.database_names.WindSolarNames.SolarNames.create_component","title":"<code>create_component(row: NDArray, indices: dict[str, int], meta_columns: set[str], meta_data: pd.DataFrame, attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None) -&gt; dict[str, Solar]</code>  <code>staticmethod</code>","text":"<p>Create a Solar Component from a row in the Solar.Generators table.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>NDArray</code> <p>Array containing the values of one table row, represeting one solar object.</p> required <code>indices</code> <code>list[str, int]</code> <p>Mapping of table's Column names to the array's indices.</p> required <code>meta_columns</code> <code>set[str]</code> <p>Set of columns used to tag object with memberships.</p> required <code>meta_data</code> <code>DataFrame</code> <p>Dictionary containing at least unit of every column.</p> required <code>attribute_objects</code> <code>dict[str, tuple[object, dict[str, Meta]]] | None</code> <p>NOT USED</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Solar]</code> <p>dict[str, Solar]: A dictionary with the id as key and the solar unit as value.</p> Source code in <code>framdata/database_names/WindSolarNames.py</code> <pre><code>@staticmethod\ndef create_component(\n    row: NDArray,\n    indices: dict[str, int],\n    meta_columns: set[str],\n    meta_data: pd.DataFrame,\n    attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n) -&gt; dict[str, Solar]:\n    \"\"\"\n    Create a Solar Component from a row in the Solar.Generators table.\n\n    Args:\n        row (NDArray): Array containing the values of one table row, represeting one solar object.\n        indices (list[str, int]): Mapping of table's Column names to the array's indices.\n        meta_columns (set[str]): Set of columns used to tag object with memberships.\n        meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n        attribute_objects (dict[str, tuple[object, dict[str, Meta]]] | None, optional): NOT USED\n\n    Returns:\n        dict[str, Solar]: A dictionary with the id as key and the solar unit as value.\n\n    \"\"\"\n    columns_to_parse = [\n        SolarNames.profile_col,\n        SolarNames.capacity_col,\n    ]\n\n    arg_user_code = SolarNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n    solar = Solar(\n        power_node=row[indices[SolarNames.power_node_col]],\n        max_capacity=MaxFlowVolume(\n            level=arg_user_code[SolarNames.capacity_col],\n            profile=arg_user_code[SolarNames.profile_col],\n        ),\n        voc=None,\n    )\n\n    SolarNames._add_meta(solar, row, indices, meta_columns)\n\n    return {row[indices[SolarNames.id_col]]: solar}\n</code></pre>"},{"location":"reference/#framdata.database_names.WindSolarNames.WindNames","title":"<code>WindNames</code>","text":"<p>               Bases: <code>WindSolarNames</code></p> <p>Class representing the names and structure of Wind tables, and method for creating Wind Component objects.</p> Source code in <code>framdata/database_names/WindSolarNames.py</code> <pre><code>class WindNames(WindSolarNames):\n    \"\"\"Class representing the names and structure of Wind tables, and method for creating Wind Component objects.\"\"\"\n\n    @staticmethod\n    def create_component(\n        row: NDArray,\n        indices: dict[str, int],\n        meta_columns: set[str],\n        meta_data: pd.DataFrame,\n        attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n    ) -&gt; dict[str, Wind]:\n        \"\"\"\n        Create a Wind Component from a row in the Wind.Generators table.\n\n        Args:\n            row (NDArray): Array containing the values of one table row, represeting one Wind object.\n            indices (list[str, int]): Mapping of table's Column names to the array's indices.\n            meta_columns (set[str]): Set of columns used to tag object with memberships.\n            meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n            attribute_objects (dict[str, tuple[object, dict[str, Meta]]] | None, optional): NOT USED\n\n        Returns:\n            dict[str, Wind]: A dictionary with the wind_id as key and the wind unit as value.\n\n        \"\"\"\n        columns_to_parse = [\n            WindNames.profile_col,\n            WindNames.capacity_col,\n        ]\n\n        arg_user_code = WindNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n        wind = Wind(\n            power_node=row[indices[WindNames.power_node_col]],\n            max_capacity=MaxFlowVolume(\n                level=arg_user_code[WindNames.capacity_col],\n                profile=arg_user_code[WindNames.profile_col],\n            ),\n            voc=None,\n        )\n        WindNames._add_meta(wind, row, indices, meta_columns)\n\n        return {row[indices[WindNames.id_col]]: wind}\n</code></pre>"},{"location":"reference/#framdata.database_names.WindSolarNames.WindNames.create_component","title":"<code>create_component(row: NDArray, indices: dict[str, int], meta_columns: set[str], meta_data: pd.DataFrame, attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None) -&gt; dict[str, Wind]</code>  <code>staticmethod</code>","text":"<p>Create a Wind Component from a row in the Wind.Generators table.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>NDArray</code> <p>Array containing the values of one table row, represeting one Wind object.</p> required <code>indices</code> <code>list[str, int]</code> <p>Mapping of table's Column names to the array's indices.</p> required <code>meta_columns</code> <code>set[str]</code> <p>Set of columns used to tag object with memberships.</p> required <code>meta_data</code> <code>DataFrame</code> <p>Dictionary containing at least unit of every column.</p> required <code>attribute_objects</code> <code>dict[str, tuple[object, dict[str, Meta]]] | None</code> <p>NOT USED</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Wind]</code> <p>dict[str, Wind]: A dictionary with the wind_id as key and the wind unit as value.</p> Source code in <code>framdata/database_names/WindSolarNames.py</code> <pre><code>@staticmethod\ndef create_component(\n    row: NDArray,\n    indices: dict[str, int],\n    meta_columns: set[str],\n    meta_data: pd.DataFrame,\n    attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n) -&gt; dict[str, Wind]:\n    \"\"\"\n    Create a Wind Component from a row in the Wind.Generators table.\n\n    Args:\n        row (NDArray): Array containing the values of one table row, represeting one Wind object.\n        indices (list[str, int]): Mapping of table's Column names to the array's indices.\n        meta_columns (set[str]): Set of columns used to tag object with memberships.\n        meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n        attribute_objects (dict[str, tuple[object, dict[str, Meta]]] | None, optional): NOT USED\n\n    Returns:\n        dict[str, Wind]: A dictionary with the wind_id as key and the wind unit as value.\n\n    \"\"\"\n    columns_to_parse = [\n        WindNames.profile_col,\n        WindNames.capacity_col,\n    ]\n\n    arg_user_code = WindNames._parse_args(row, indices, columns_to_parse, meta_data)\n\n    wind = Wind(\n        power_node=row[indices[WindNames.power_node_col]],\n        max_capacity=MaxFlowVolume(\n            level=arg_user_code[WindNames.capacity_col],\n            profile=arg_user_code[WindNames.profile_col],\n        ),\n        voc=None,\n    )\n    WindNames._add_meta(wind, row, indices, meta_columns)\n\n    return {row[indices[WindNames.id_col]]: wind}\n</code></pre>"},{"location":"reference/#framdata.database_names.WindSolarNames.WindSolarMetadataSchema","title":"<code>WindSolarMetadataSchema</code>","text":"<p>               Bases: <code>_AttributeMetadataSchema</code></p> <p>Standard Pandera DataFrameModel schema for metadata in the Wind and Solar files.</p> Source code in <code>framdata/database_names/WindSolarNames.py</code> <pre><code>class WindSolarMetadataSchema(_AttributeMetadataSchema):\n    \"\"\"Standard Pandera DataFrameModel schema for metadata in the Wind and Solar files.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/#framdata.database_names.WindSolarNames.WindSolarNames","title":"<code>WindSolarNames</code>","text":"<p>               Bases: <code>_BaseComponentsNames</code></p> <p>Class representing the names and structure of Wind and Solar tables.</p> Source code in <code>framdata/database_names/WindSolarNames.py</code> <pre><code>class WindSolarNames(_BaseComponentsNames):\n    \"\"\"Class representing the names and structure of Wind and Solar tables.\"\"\"\n\n    id_col = \"ID\"\n    power_node_col = \"PowerNode\"\n    profile_col = \"Profile\"\n    type_col = \"TechnologyType\"\n    capacity_col = \"Capacity\"\n\n    columns: ClassVar[list[str]] = [\n        id_col,\n        power_node_col,\n        profile_col,\n        capacity_col,\n    ]\n\n    ref_columns: ClassVar[list[str]] = [\n        power_node_col,\n        profile_col,\n        capacity_col,\n    ]\n\n    @staticmethod\n    def get_attribute_data_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for attribute data in a Wind and Solar file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for Wind and Solar attribute data.\n\n        \"\"\"\n        return WindSolarSchema\n\n    @staticmethod\n    def get_metadata_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for the metadata table in a Wind and Solar file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for the Thermal metadata.\n\n        \"\"\"\n        return WindSolarMetadataSchema\n\n    @staticmethod\n    def _get_unique_check_descriptions() -&gt; dict[str, tuple[str, bool]]:\n        \"\"\"\n        Retrieve a dictionary with descriptons of validation checks that are specific to the Wind and Solar schemas.\n\n        Returns:\n            dict[str, tuple[str, bool]]: A dictionary where:\n                - Keys (str): The name of the validation check method.\n                - Values (tuple[str, bool]):\n                    - The first element (str) provides a concise and user-friendly description of the check. E.g. what\n                      caused the validation error or what is required for the check to pass.\n                    - The second element (bool) indicates whether the check is a warning (True) or an error (False).\n\n\n        \"\"\"\n        return None\n\n    @staticmethod\n    def _format_unique_checks(errors: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Format the error DataFrame according to the validation checks that are specific to the Wind and Solar schemas.\n\n        Args:\n            errors (pd.DataFrame): The error DataFrame containing validation errors.\n\n        Returns:\n            pd.DataFrame: The updated error DataFrame with formatted rows for unique validation checks.\n\n        \"\"\"\n        return None\n</code></pre>"},{"location":"reference/#framdata.database_names.WindSolarNames.WindSolarNames.get_attribute_data_schema","title":"<code>get_attribute_data_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for attribute data in a Wind and Solar file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for Wind and Solar attribute data.</p> Source code in <code>framdata/database_names/WindSolarNames.py</code> <pre><code>@staticmethod\ndef get_attribute_data_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for attribute data in a Wind and Solar file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for Wind and Solar attribute data.\n\n    \"\"\"\n    return WindSolarSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.WindSolarNames.WindSolarNames.get_metadata_schema","title":"<code>get_metadata_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for the metadata table in a Wind and Solar file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for the Thermal metadata.</p> Source code in <code>framdata/database_names/WindSolarNames.py</code> <pre><code>@staticmethod\ndef get_metadata_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for the metadata table in a Wind and Solar file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for the Thermal metadata.\n\n    \"\"\"\n    return WindSolarMetadataSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.WindSolarNames.WindSolarSchema","title":"<code>WindSolarSchema</code>","text":"<p>               Bases: <code>DataFrameModel</code></p> <p>Standard Pandera DataFrameModel schema for attribute data in the Wind and Solar files.</p> Source code in <code>framdata/database_names/WindSolarNames.py</code> <pre><code>class WindSolarSchema(pa.DataFrameModel):\n    \"\"\"Standard Pandera DataFrameModel schema for attribute data in the Wind and Solar files.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/#framdata.database_names.YamlNames","title":"<code>YamlNames</code>","text":"<p>Define names and fields used in yaml files.</p>"},{"location":"reference/#framdata.database_names.YamlNames.YamlNames","title":"<code>YamlNames</code>","text":"<p>Contain names in yaml files.</p> Source code in <code>framdata/database_names/YamlNames.py</code> <pre><code>class YamlNames:\n    \"\"\"Contain names in yaml files.\"\"\"\n\n    encoding = \"utf-8\"\n\n    metadata_field = \"Metadata\"\n    x_field = \"X\"\n    y_field = \"Y\"\n\n    # ========= Metadata fields =========\n    attribute = \"Attribute\"\n    description = \"Description\"\n    dtype = \"Dtype\"\n    unit = \"Unit\"\n</code></pre>"},{"location":"reference/#framdata.database_names.nodes_names","title":"<code>nodes_names</code>","text":"<p>Define class for handling tables with Nodes.</p>"},{"location":"reference/#framdata.database_names.nodes_names.EmissionNodesNames","title":"<code>EmissionNodesNames</code>","text":"<p>               Bases: <code>NodesNames</code></p> <p>Class representing the names and structure of emission nodes tables.</p> Source code in <code>framdata/database_names/nodes_names.py</code> <pre><code>class EmissionNodesNames(NodesNames):\n    \"\"\"Class representing the names and structure of emission nodes tables.\"\"\"\n\n    filename = \"Emission.Nodes\"\n\n    tax_col = \"Tax\"  # deprecated?\n</code></pre>"},{"location":"reference/#framdata.database_names.nodes_names.FuelNodesNames","title":"<code>FuelNodesNames</code>","text":"<p>               Bases: <code>NodesNames</code></p> <p>Class representing the names and structure of fuel nodes tables.</p> Source code in <code>framdata/database_names/nodes_names.py</code> <pre><code>class FuelNodesNames(NodesNames):\n    \"\"\"Class representing the names and structure of fuel nodes tables.\"\"\"\n\n    filename = \"Fuel.Nodes\"\n\n    emission_coefficient_col = \"EmissionCoefficient\"\n    tax_col = \"Tax\"  # deprecated?\n</code></pre>"},{"location":"reference/#framdata.database_names.nodes_names.NodesMetadataSchema","title":"<code>NodesMetadataSchema</code>","text":"<p>               Bases: <code>_AttributeMetadataSchema</code></p> <p>Standard Pandera DataFrameModel schema for metadata in the Nodes files.</p> Source code in <code>framdata/database_names/nodes_names.py</code> <pre><code>class NodesMetadataSchema(_AttributeMetadataSchema):\n    \"\"\"Standard Pandera DataFrameModel schema for metadata in the Nodes files.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/#framdata.database_names.nodes_names.NodesNames","title":"<code>NodesNames</code>","text":"<p>               Bases: <code>_BaseComponentsNames</code></p> <p>Class representing the names and structure of nodes tables, and the convertion of the table to Node objects.</p> Source code in <code>framdata/database_names/nodes_names.py</code> <pre><code>class NodesNames(_BaseComponentsNames):\n    \"\"\"Class representing the names and structure of nodes tables, and the convertion of the table to Node objects.\"\"\"\n\n    id_col = \"NodeID\"\n\n    commodity_col = \"Commodity\"\n    nice_name = \"NiceName\"\n    price_col = \"ExogenPrice\"\n    profile_col = \"PriceProfile\"\n    exogenous_col = \"IsExogenous\"\n\n    columns: ClassVar[list[str]] = [id_col, nice_name, commodity_col, price_col, profile_col, exogenous_col]\n\n    ref_columns: ClassVar[list[str]] = [price_col, profile_col]\n\n    @staticmethod\n    def create_component(\n        row: NDArray,\n        indices: dict[str, int],\n        meta_columns: set[str],\n        meta_data: pd.DataFrame,\n        attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n    ) -&gt; tuple[dict[str, Node], list[str]]:\n        \"\"\"\n        Create a node object from direct parameters.\n\n        Args:\n            row (NDArray): Array containing the values of one table row, represeting one Node object.\n            indices (list[str, int]): Mapping of table's Column names to the array's indices.\n            meta_columns (list[str]): Set of columns which defines memberships in meta groups for aggregation.\n            meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n            attribute_objects (dict[str, tuple[object, dict[str, Meta]]], optional): NOT USED\n\n        Returns:\n            dict[str, Node]: Dictionary of node id and the Node object.\n\n        \"\"\"\n        columns_to_parse = [\n            NodesNames.price_col,\n            NodesNames.profile_col,\n        ]\n\n        arg_user_code = NodesNames._parse_args(row, indices, columns_to_parse, meta_data)\n        price = None\n        if arg_user_code[NodesNames.price_col] is not None:\n            price = Price(\n                level=arg_user_code[NodesNames.price_col],\n                profile=arg_user_code[NodesNames.profile_col],\n            )\n\n        node = Node(\n            row[indices[NodesNames.commodity_col]],\n            is_exogenous=row[indices[NodesNames.exogenous_col]],\n            price=price,\n        )\n        NodesNames._add_meta(node, row, indices, meta_columns)\n        return {row[indices[NodesNames.id_col]]: node}\n\n    @staticmethod\n    def get_attribute_data_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for attribute data in a Nodes file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for Nodes attribute data.\n\n        \"\"\"\n        return NodesSchema\n\n    @staticmethod\n    def get_metadata_schema() -&gt; pa.DataFrameModel:\n        \"\"\"\n        Get the Pandera DataFrameModel schema for the metadata table in a Nodes file.\n\n        Returns:\n            pa.DataFrameModel: Pandera DataFrameModel schema for the Thermal metadata.\n\n        \"\"\"\n        return NodesMetadataSchema\n\n    @staticmethod\n    def _get_unique_check_descriptions() -&gt; dict[str, tuple[str, bool]]:\n        \"\"\"\n        Retrieve a dictionary with descriptons of validation checks that are specific to the Nodes schemas.\n\n        Returns:\n            dict[str, tuple[str, bool]]: A dictionary where:\n                - Keys (str): The name of the validation check method.\n                - Values (tuple[str, bool]):\n                    - The first element (str) provides a concise and user-friendly description of the check. E.g. what\n                      caused the validation error or what is required for the check to pass.\n                    - The second element (bool) indicates whether the check is a warning (True) or an error (False).\n\n\n        \"\"\"\n        return None\n\n    @staticmethod\n    def _format_unique_checks(errors: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Format the error DataFrame according to the validation checks that are specific to the Nodes schemas.\n\n        Args:\n            errors (pd.DataFrame): The error DataFrame containing validation errors.\n\n        Returns:\n            pd.DataFrame: The updated error DataFrame with formatted rows for unique validation checks.\n\n        \"\"\"\n        return None\n</code></pre>"},{"location":"reference/#framdata.database_names.nodes_names.NodesNames.create_component","title":"<code>create_component(row: NDArray, indices: dict[str, int], meta_columns: set[str], meta_data: pd.DataFrame, attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None) -&gt; tuple[dict[str, Node], list[str]]</code>  <code>staticmethod</code>","text":"<p>Create a node object from direct parameters.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>NDArray</code> <p>Array containing the values of one table row, represeting one Node object.</p> required <code>indices</code> <code>list[str, int]</code> <p>Mapping of table's Column names to the array's indices.</p> required <code>meta_columns</code> <code>list[str]</code> <p>Set of columns which defines memberships in meta groups for aggregation.</p> required <code>meta_data</code> <code>DataFrame</code> <p>Dictionary containing at least unit of every column.</p> required <code>attribute_objects</code> <code>dict[str, tuple[object, dict[str, Meta]]]</code> <p>NOT USED</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[dict[str, Node], list[str]]</code> <p>dict[str, Node]: Dictionary of node id and the Node object.</p> Source code in <code>framdata/database_names/nodes_names.py</code> <pre><code>@staticmethod\ndef create_component(\n    row: NDArray,\n    indices: dict[str, int],\n    meta_columns: set[str],\n    meta_data: pd.DataFrame,\n    attribute_objects: dict[str, tuple[object, dict[str, Meta]]] | None = None,\n) -&gt; tuple[dict[str, Node], list[str]]:\n    \"\"\"\n    Create a node object from direct parameters.\n\n    Args:\n        row (NDArray): Array containing the values of one table row, represeting one Node object.\n        indices (list[str, int]): Mapping of table's Column names to the array's indices.\n        meta_columns (list[str]): Set of columns which defines memberships in meta groups for aggregation.\n        meta_data (pd.DataFrame): Dictionary containing at least unit of every column.\n        attribute_objects (dict[str, tuple[object, dict[str, Meta]]], optional): NOT USED\n\n    Returns:\n        dict[str, Node]: Dictionary of node id and the Node object.\n\n    \"\"\"\n    columns_to_parse = [\n        NodesNames.price_col,\n        NodesNames.profile_col,\n    ]\n\n    arg_user_code = NodesNames._parse_args(row, indices, columns_to_parse, meta_data)\n    price = None\n    if arg_user_code[NodesNames.price_col] is not None:\n        price = Price(\n            level=arg_user_code[NodesNames.price_col],\n            profile=arg_user_code[NodesNames.profile_col],\n        )\n\n    node = Node(\n        row[indices[NodesNames.commodity_col]],\n        is_exogenous=row[indices[NodesNames.exogenous_col]],\n        price=price,\n    )\n    NodesNames._add_meta(node, row, indices, meta_columns)\n    return {row[indices[NodesNames.id_col]]: node}\n</code></pre>"},{"location":"reference/#framdata.database_names.nodes_names.NodesNames.get_attribute_data_schema","title":"<code>get_attribute_data_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for attribute data in a Nodes file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for Nodes attribute data.</p> Source code in <code>framdata/database_names/nodes_names.py</code> <pre><code>@staticmethod\ndef get_attribute_data_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for attribute data in a Nodes file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for Nodes attribute data.\n\n    \"\"\"\n    return NodesSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.nodes_names.NodesNames.get_metadata_schema","title":"<code>get_metadata_schema() -&gt; pa.DataFrameModel</code>  <code>staticmethod</code>","text":"<p>Get the Pandera DataFrameModel schema for the metadata table in a Nodes file.</p> <p>Returns:</p> Type Description <code>DataFrameModel</code> <p>pa.DataFrameModel: Pandera DataFrameModel schema for the Thermal metadata.</p> Source code in <code>framdata/database_names/nodes_names.py</code> <pre><code>@staticmethod\ndef get_metadata_schema() -&gt; pa.DataFrameModel:\n    \"\"\"\n    Get the Pandera DataFrameModel schema for the metadata table in a Nodes file.\n\n    Returns:\n        pa.DataFrameModel: Pandera DataFrameModel schema for the Thermal metadata.\n\n    \"\"\"\n    return NodesMetadataSchema\n</code></pre>"},{"location":"reference/#framdata.database_names.nodes_names.NodesSchema","title":"<code>NodesSchema</code>","text":"<p>               Bases: <code>DataFrameModel</code></p> <p>Standard Pandera DataFrameModel schema for attribute data in the Nodes files.</p> Source code in <code>framdata/database_names/nodes_names.py</code> <pre><code>class NodesSchema(pa.DataFrameModel):\n    \"\"\"Standard Pandera DataFrameModel schema for attribute data in the Nodes files.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/#framdata.database_names.nodes_names.PowerNodesNames","title":"<code>PowerNodesNames</code>","text":"<p>               Bases: <code>NodesNames</code></p> <p>Class representing the names and structure of power nodes tables.</p> Source code in <code>framdata/database_names/nodes_names.py</code> <pre><code>class PowerNodesNames(NodesNames):\n    \"\"\"Class representing the names and structure of power nodes tables.\"\"\"\n\n    filename = \"Power.Nodes\"\n</code></pre>"},{"location":"reference/#framdata.database_names.validation_functions","title":"<code>validation_functions</code>","text":"<p>Module containing registered custom check functions used by Pandera schema classes.</p>"},{"location":"reference/#framdata.database_names.validation_functions.check_unit_is_str_for_attributes","title":"<code>check_unit_is_str_for_attributes(df: pd.DataFrame, attribute_names: list[str]) -&gt; Series[bool]</code>","text":"<p>Check if 'Unit' column values are strings for the rows where the 'Attribute' column matches specific attributes.</p> <p>This function checks whether the values in the 'Unit' column are strings for rows where the 'Attribute' column matches any of the specified attribute names. Rows that do not match the specified attributes are considered valid by default. This function is commonly used by subclasses of 'AttributeMetadataSchema' to validate that a unit is given for certain attributes in the metadata belonging to a Component.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame containing the columns to validate.</p> required <code>attribute_names</code> <code>list[str]</code> <p>A list with the names of the attributes to check in the 'Attribute' column.</p> required <p>Returns:</p> Type Description <code>Series[bool]</code> <p>Series[bool]: A boolean Series indicating whether each row passes the validation. Rows where the 'Attribute'</p> <code>Series[bool]</code> <p>column does not match the specified attribute are automatically marked as valid.</p> Example <p>Given the following DataFrame:</p> attribute unit Volume MWh Temperature None Capacity None <p>And <code>attribute_names = [\"Volume\", \"Capacity\"]</code>, the method will validate that the 'Unit' column contains strings for rows where 'attribute' is \"Volume\" and \"Capacity\". The resulting Series will be:</p> validation_result True True False Source code in <code>framdata/database_names/validation_functions.py</code> <pre><code>@extensions.register_check_method()\ndef check_unit_is_str_for_attributes(df: pd.DataFrame, attribute_names: list[str]) -&gt; Series[bool]:\n    \"\"\"\n    Check if 'Unit' column values are strings for the rows where the 'Attribute' column matches specific attributes.\n\n    This function checks whether the values in the 'Unit' column are strings for rows where the 'Attribute' column\n    matches any of the specified attribute names. Rows that do not match the specified attributes are considered valid\n    by default. This function is commonly used by subclasses of 'AttributeMetadataSchema' to validate that a unit is\n    given for certain attributes in the metadata belonging to a Component.\n\n    Args:\n        df (pd.DataFrame): The DataFrame containing the columns to validate.\n        attribute_names (list[str]): A list with the names of the attributes to check in the 'Attribute' column.\n\n    Returns:\n        Series[bool]: A boolean Series indicating whether each row passes the validation. Rows where the 'Attribute'\n        column does not match the specified attribute are automatically marked as valid.\n\n    Example:\n        Given the following DataFrame:\n\n        | attribute   | unit       |\n        |-------------|------------|\n        | Volume      | MWh        |\n        | Temperature | None       |\n        | Capacity    | None       |\n\n        And `attribute_names = [\"Volume\", \"Capacity\"]`, the method will validate that the 'Unit' column contains strings\n        for rows where 'attribute' is \"Volume\" and \"Capacity\". The resulting Series will be:\n\n        | validation_result |\n        |-------------------|\n        | True              |\n        | True              |\n        | False             |\n\n    \"\"\"\n    is_attribute_rows = df[_AttributeMetadataNames.attribute].isin(attribute_names)\n    unit_is_str = df[_AttributeMetadataNames.unit].apply(lambda x: isinstance(x, str))\n    return ~is_attribute_rows | unit_is_str\n</code></pre>"},{"location":"reference/#framdata.database_names.validation_functions.dtype_str_int_float","title":"<code>dtype_str_int_float(series: Series[Any]) -&gt; Series[bool]</code>","text":"<p>Check if the series contains only str, int or float values.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>Series[Any]</code> <p>Series to check.</p> required <p>Returns:</p> Type Description <code>Series[bool]</code> <p>Series[bool]: Series of boolean values detonating if each element has passed the check.</p> Source code in <code>framdata/database_names/validation_functions.py</code> <pre><code>@extensions.register_check_method()\ndef dtype_str_int_float(series: Series[Any]) -&gt; Series[bool]:\n    \"\"\"\n    Check if the series contains only str, int or float values.\n\n    Args:\n        series (Series[Any]): Series to check.\n\n    Returns:\n        Series[bool]: Series of boolean values detonating if each element has passed the check.\n\n    \"\"\"\n    return series.apply(lambda value: isinstance(value, str | int | float))\n</code></pre>"},{"location":"reference/#framdata.database_names.validation_functions.dtype_str_int_float_none","title":"<code>dtype_str_int_float_none(series: Series[Any]) -&gt; Series[bool]</code>","text":"<p>Check if the series contains only str, int, float or None values.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>Series[Any]</code> <p>Series to check.</p> required <p>Returns:</p> Type Description <code>Series[bool]</code> <p>Series[bool]: Series of boolean values detonating if each element has passed the check.</p> Source code in <code>framdata/database_names/validation_functions.py</code> <pre><code>@extensions.register_check_method()\ndef dtype_str_int_float_none(series: Series[Any]) -&gt; Series[bool]:\n    \"\"\"\n    Check if the series contains only str, int, float or None values.\n\n    Args:\n        series (Series[Any]): Series to check.\n\n    Returns:\n        Series[bool]: Series of boolean values detonating if each element has passed the check.\n\n    \"\"\"\n    return series.apply(lambda value: isinstance(value, str | int | float | type(None)))\n</code></pre>"},{"location":"reference/#framdata.database_names.validation_functions.numeric_values_are_between_or_equal_to","title":"<code>numeric_values_are_between_or_equal_to(series: Series[Any], min_value: int | float, max_value: int | float) -&gt; Series[bool]</code>","text":"<p>Check if values are between or equal to a min and max value if they are of type int or float.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>Series[Any]</code> <p>Series to check.</p> required <code>min_value</code> <code>int | float</code> <p>Value that the elements in the series should be greater than or equal.</p> required <code>max_value</code> <code>int | float</code> <p>Value that the elements in the series should be less than or equal.</p> required <p>Returns:</p> Type Description <code>Series[bool]</code> <p>Series[bool]: Series of boolean values detonating if each element has passed the check.</p> Source code in <code>framdata/database_names/validation_functions.py</code> <pre><code>@extensions.register_check_method()\ndef numeric_values_are_between_or_equal_to(\n    series: Series[Any],\n    min_value: int | float,\n    max_value: int | float,\n) -&gt; Series[bool]:\n    \"\"\"\n    Check if values are between or equal to a min and max value if they are of type int or float.\n\n    Args:\n        series (Series[Any]): Series to check.\n        min_value (int | float): Value that the elements in the series should be greater than or equal.\n        max_value (int | float): Value that the elements in the series should be less than or equal.\n\n    Returns:\n        Series[bool]: Series of boolean values detonating if each element has passed the check.\n\n    \"\"\"\n    if not isinstance(min_value, (int | float)) and not isinstance(max_value, (int | float)):\n        message = \"min and max value must be of type int or float.\"\n        raise ValueError(message)\n    return series.apply(lambda x: min_value &lt;= x &lt;= max_value if isinstance(x, (int | float)) else True)\n</code></pre>"},{"location":"reference/#framdata.database_names.validation_functions.numeric_values_greater_than_or_equal_to","title":"<code>numeric_values_greater_than_or_equal_to(series: Series[Any], min_value: int | float) -&gt; Series[bool]</code>","text":"<p>Check if values are greater than or equal to min_value if they are of type int or float.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>Series[Any]</code> <p>Series to check.</p> required <code>min_value</code> <code>int | float</code> <p>Value that the elements in the series should be greater than or equal.</p> required <p>Returns:</p> Type Description <code>Series[bool]</code> <p>Series[bool]: Series of boolean values detonating if each element has passed the check.</p> Source code in <code>framdata/database_names/validation_functions.py</code> <pre><code>@extensions.register_check_method()\ndef numeric_values_greater_than_or_equal_to(series: Series[Any], min_value: int | float) -&gt; Series[bool]:\n    \"\"\"\n    Check if values are greater than or equal to min_value if they are of type int or float.\n\n    Args:\n        series (Series[Any]): Series to check.\n        min_value (int | float): Value that the elements in the series should be greater than or equal.\n\n    Returns:\n        Series[bool]: Series of boolean values detonating if each element has passed the check.\n\n    \"\"\"\n    if not isinstance(min_value, (int | float)):\n        message = \"min_value must be of type int or float.\"\n        raise ValueError(message)\n    return series.apply(lambda x: x &gt;= min_value if isinstance(x, (int | float)) else True)\n</code></pre>"},{"location":"reference/#framdata.database_names.validation_functions.numeric_values_less_than_or_equal_to","title":"<code>numeric_values_less_than_or_equal_to(series: Series[Any], max_value: int | float) -&gt; Series[bool]</code>","text":"<p>Check if values are less than or equal to max_value if they are of type int or float.</p> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>Series[Any]</code> <p>Series to check.</p> required <code>max_value</code> <code>int | float</code> <p>Value that the elements in the series should be greater than or equal.</p> required <p>Returns:</p> Type Description <code>Series[bool]</code> <p>Series[bool]: Series of boolean values detonating if each element has passed the check.</p> Source code in <code>framdata/database_names/validation_functions.py</code> <pre><code>@extensions.register_check_method()\ndef numeric_values_less_than_or_equal_to(series: Series[Any], max_value: int | float) -&gt; Series[bool]:\n    \"\"\"\n    Check if values are less than or equal to max_value if they are of type int or float.\n\n    Args:\n        series (Series[Any]): Series to check.\n        max_value (int | float): Value that the elements in the series should be greater than or equal.\n\n    Returns:\n        Series[bool]: Series of boolean values detonating if each element has passed the check.\n\n    \"\"\"\n    if not isinstance(max_value, (int | float)):\n        message = \"max_value must be of type int or float.\"\n        raise ValueError(message)\n    return series.apply(lambda x: x &lt;= max_value if isinstance(x, (int | float)) else True)\n</code></pre>"},{"location":"reference/#framdata.file_editors","title":"<code>file_editors</code>","text":""},{"location":"reference/#framdata.file_editors.NVEFileEditor","title":"<code>NVEFileEditor</code>","text":"<p>Contain class with common functionality for editing files.</p>"},{"location":"reference/#framdata.file_editors.NVEFileEditor.NVEFileEditor","title":"<code>NVEFileEditor</code>","text":"<p>               Bases: <code>Base</code></p> <p>Parent class with common functionality for classes concerned with editing FRAM files.</p> Source code in <code>framdata/file_editors/NVEFileEditor.py</code> <pre><code>class NVEFileEditor(Base):\n    \"\"\"Parent class with common functionality for classes concerned with editing FRAM files.\"\"\"\n\n    def __init__(self, source: Path | str | None = None) -&gt; None:\n        \"\"\"\n        Set path to parquet file if supplied, load/initialize table and metadata as pd.DataFrame and dictionary respectively.\n\n        Args:\n            source (Path | str | None, optional): Path to parquet file with timevectors. Defaults to None.\n\n        \"\"\"\n        super().__init__()\n\n        self._check_type(source, (Path, str, type(None)))\n        self._source = None if source is None else Path(source)\n\n    def get_source(self) -&gt; Path:\n        \"\"\"Get the source file path of the editor.\"\"\"\n        return self._source\n\n    def set_source(self, source: Path) -&gt; None:\n        \"\"\"Set the source file path of the editor.\"\"\"\n        self._check_type(source, (Path, str))\n        self._source = Path(source)\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEFileEditor.NVEFileEditor.__init__","title":"<code>__init__(source: Path | str | None = None) -&gt; None</code>","text":"<p>Set path to parquet file if supplied, load/initialize table and metadata as pd.DataFrame and dictionary respectively.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Path | str | None</code> <p>Path to parquet file with timevectors. Defaults to None.</p> <code>None</code> Source code in <code>framdata/file_editors/NVEFileEditor.py</code> <pre><code>def __init__(self, source: Path | str | None = None) -&gt; None:\n    \"\"\"\n    Set path to parquet file if supplied, load/initialize table and metadata as pd.DataFrame and dictionary respectively.\n\n    Args:\n        source (Path | str | None, optional): Path to parquet file with timevectors. Defaults to None.\n\n    \"\"\"\n    super().__init__()\n\n    self._check_type(source, (Path, str, type(None)))\n    self._source = None if source is None else Path(source)\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEFileEditor.NVEFileEditor.get_source","title":"<code>get_source() -&gt; Path</code>","text":"<p>Get the source file path of the editor.</p> Source code in <code>framdata/file_editors/NVEFileEditor.py</code> <pre><code>def get_source(self) -&gt; Path:\n    \"\"\"Get the source file path of the editor.\"\"\"\n    return self._source\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEFileEditor.NVEFileEditor.set_source","title":"<code>set_source(source: Path) -&gt; None</code>","text":"<p>Set the source file path of the editor.</p> Source code in <code>framdata/file_editors/NVEFileEditor.py</code> <pre><code>def set_source(self, source: Path) -&gt; None:\n    \"\"\"Set the source file path of the editor.\"\"\"\n    self._check_type(source, (Path, str))\n    self._source = Path(source)\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEH5TimeVectorEditor","title":"<code>NVEH5TimeVectorEditor</code>","text":"<p>Contains class for editing time vectors in H5 files.</p>"},{"location":"reference/#framdata.file_editors.NVEH5TimeVectorEditor.NVEH5TimeVectorEditor","title":"<code>NVEH5TimeVectorEditor</code>","text":"<p>               Bases: <code>NVEFileEditor</code></p> <p>Class with functionality concerned with editing time vectors and their metadata in H5 files.</p> Source code in <code>framdata/file_editors/NVEH5TimeVectorEditor.py</code> <pre><code>class NVEH5TimeVectorEditor(NVEFileEditor):\n    \"\"\"Class with functionality concerned with editing time vectors and their metadata in H5 files.\"\"\"\n\n    def __init__(self, source: Path | str | None = None) -&gt; None:\n        \"\"\"\n        Set path to parquet file if supplied, load/initialize table and metadata as pd.DataFrame and dictionary respectively.\n\n        Args:\n            source (Path | str | None, optional): Path to parquet file with timevectors. Defaults to None.\n\n        \"\"\"\n        super().__init__(source)\n\n        meta_tuple = ({}, None) if self._source is None or not self._source.exists() else self._read_data(H5Names.METADATA_GROUP, True)\n        self._metadata, self._common_metadata = meta_tuple\n        index_tuple = (defaultdict(NDArray), None) if self._source is None or not self._source.exists() else self._read_data(H5Names.INDEX_GROUP, False)\n        self._index, self._common_index = index_tuple\n        self._index = {k: v.astype(str) for k, v in self._index.items()}\n\n        vectors_tuple = (defaultdict(NDArray), None) if self._source is None or not self._source.exists() else self._read_data(H5Names.VECTORS_GROUP, False)\n        self._vectors, __ = vectors_tuple\n\n    def get_metadata(self, vector_id: str) -&gt; None | dict:\n        \"\"\"Get a copy of the metadata of the parquet file.\"\"\"\n        try:\n            return self._metadata[vector_id]\n        except KeyError as e:\n            f\"Found no ID '{vector_id}' in metadata.\"\n            raise KeyError from e\n\n    def set_metadata(self, vector_id: str, value: dict[str, METADATA_TYPES]) -&gt; None:\n        \"\"\"Set a field (new or overwrite) in the metadata.\"\"\"\n        self._check_type(vector_id, str)\n        self._check_type(value, dict)\n        self._metadata[vector_id] = value\n\n    def get_common_metadata(self) -&gt; None | dict:\n        \"\"\"Get a copy of the metadata of the parquet file.\"\"\"\n        return self._common_metadata if self._common_metadata is None else self._common_metadata.copy()\n\n    def set_common_metadata(self, value: dict[str, METADATA_TYPES]) -&gt; None:\n        \"\"\"Set a field (new or overwrite) in the metadata.\"\"\"\n        self._check_type(value, dict)\n        self._common_metadata = value\n\n    def set_index(self, vector_id: str, index: NDArray) -&gt; None:\n        \"\"\"Set a whole index in the time index table.\"\"\"\n        self._check_type(vector_id, str)\n        self._check_type(index, np.ndarray)\n        self._index[vector_id] = index\n\n    def get_index(self, vector_id: str) -&gt; NDArray:\n        \"\"\"Return a copy of a given index as a pandas series from the table.\"\"\"\n        try:\n            return self._index[vector_id]\n        except KeyError as e:\n            f\"Found no ID '{vector_id}' among indexes.\"\n            raise KeyError from e\n\n    def set_common_index(self, values: NDArray) -&gt; None:\n        \"\"\"Set a whole index in the time index table.\"\"\"\n        self._check_type(values, np.ndarray)\n        self._common_index = values\n\n    def get_common_index(self) -&gt; NDArray | None:\n        \"\"\"Return a copy of a given index as a pandas series from the table.\"\"\"\n        return self._common_index\n\n    def set_vector(self, vector_id: str, values: NDArray) -&gt; None:\n        \"\"\"Set a whole vector in the time vector table.\"\"\"\n        self._check_type(vector_id, str)\n        self._check_type(values, np.ndarray)\n        self._vectors[vector_id] = values\n\n    def get_vector(self, vector_id: str) -&gt; NDArray:\n        \"\"\"Return a copy of a given vector as a pandas series from the table.\"\"\"\n        try:\n            return self._vectors[vector_id]\n        except KeyError as e:\n            msg = f\"Found no ID '{vector_id}' among vectors.\"\n            raise KeyError(msg) from e\n\n    def get_vector_ids(self) -&gt; list[str]:\n        \"\"\"Get the IDs of all vectors.\"\"\"\n        return list(self._vectors.keys())\n\n    def save_to_h5(self, path: Path | str) -&gt; None:\n        self._check_type(path, (Path, str))\n        path = Path(path)\n\n        missing_index = {v for v in self._vectors if v not in self._index}\n        if self._common_index is None and len(missing_index) != 0:\n            msg = f\"Found vectors missing indexes and common index is not set: {missing_index}.\"\n            raise KeyError(msg)\n\n        missing_meta = {v for v in self._vectors if v not in self._metadata}\n        if self._common_metadata is None and len(missing_meta) != 0:\n            msg = f\"Found vectors missing metadata and common metadata is not set: {missing_meta}.\"\n            raise KeyError(msg)\n\n        with h5py.File(path, mode=\"w\") as f:\n            if self._common_metadata is not None:\n                common_meta_group = f.create_group(H5Names.COMMON_PREFIX + H5Names.METADATA_GROUP)\n                self._write_meta_to_group(common_meta_group, self._common_metadata)\n            if self._common_index is not None:\n                f.create_dataset(H5Names.COMMON_PREFIX + H5Names.INDEX_GROUP, data=self._common_index.astype(bytes))\n\n            if self._metadata:\n                meta_group = f.create_group(H5Names.METADATA_GROUP)\n                for vector_id, meta in self._metadata.items():\n                    vm_group = meta_group.create_group(vector_id)\n                    self._write_meta_to_group(vm_group, meta)\n\n            if self._index:\n                index_group = f.create_group(H5Names.INDEX_GROUP)\n                for vector_id, index in self._index.items():\n                    index_group.create_dataset(vector_id, data=index.astype(bytes))\n\n            if self._vectors:\n                vector_group = f.create_group(H5Names.VECTORS_GROUP)\n                for vector_id, vector in self._vectors.items():\n                    vector_group.create_dataset(vector_id, data=vector)\n\n    def _write_meta_to_group(self, meta_group: h5py.Group, metadata: dict) -&gt; None:\n        for k, v in metadata.items():\n            meta_group.create_dataset(k, data=str(v).encode(TvMn.ENCODING))\n\n    def _read_data(\n        self, group_name: str, cast_meta: bool\n    ) -&gt; tuple[dict[str, dict[str, METADATA_TYPES]] | dict[str, dict[str, NDArray]], dict[str, METADATA_TYPES] | dict[str, NDArray]]:\n        common_field = H5Names.COMMON_PREFIX + group_name\n        data = {}\n        common_data = None\n        with h5py.File(self._source, mode=\"r\") as f:\n            if group_name in f and isinstance(f[group_name], h5py.Group):\n                group = f[group_name]\n                data.update(\n                    {\n                        vector_id: TvMn.cast_meta(self._read_datasets(vector_data)) if cast_meta else self._read_datasets(vector_data)\n                        for vector_id, vector_data in group.items()\n                    },\n                )\n\n            if common_field in f and isinstance(f[common_field], h5py.Group):\n                datasets = self._read_datasets(f[common_field])\n                common_data, __ = TvMn.cast_meta(datasets) if cast_meta else (datasets, None)\n            elif common_field in f and isinstance(f[common_field], h5py.Dataset):\n                common_data = f[common_field][()]\n\n        return data, common_data\n\n    def _read_datasets(self, field: h5py.Group | h5py.Dataset) -&gt; dict | NDArray | bytes:\n        if isinstance(field, h5py.Dataset):\n            return field[()]\n        datasets = {}\n        for key, val in field.items():\n            if isinstance(val, h5py.Dataset):\n                datasets[key] = val[()]\n            else:\n                msg = f\"Expected only {h5py.Dataset} in field, but found {type(val)}\"\n                raise TypeError(msg)\n\n        return datasets\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEH5TimeVectorEditor.NVEH5TimeVectorEditor.__init__","title":"<code>__init__(source: Path | str | None = None) -&gt; None</code>","text":"<p>Set path to parquet file if supplied, load/initialize table and metadata as pd.DataFrame and dictionary respectively.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Path | str | None</code> <p>Path to parquet file with timevectors. Defaults to None.</p> <code>None</code> Source code in <code>framdata/file_editors/NVEH5TimeVectorEditor.py</code> <pre><code>def __init__(self, source: Path | str | None = None) -&gt; None:\n    \"\"\"\n    Set path to parquet file if supplied, load/initialize table and metadata as pd.DataFrame and dictionary respectively.\n\n    Args:\n        source (Path | str | None, optional): Path to parquet file with timevectors. Defaults to None.\n\n    \"\"\"\n    super().__init__(source)\n\n    meta_tuple = ({}, None) if self._source is None or not self._source.exists() else self._read_data(H5Names.METADATA_GROUP, True)\n    self._metadata, self._common_metadata = meta_tuple\n    index_tuple = (defaultdict(NDArray), None) if self._source is None or not self._source.exists() else self._read_data(H5Names.INDEX_GROUP, False)\n    self._index, self._common_index = index_tuple\n    self._index = {k: v.astype(str) for k, v in self._index.items()}\n\n    vectors_tuple = (defaultdict(NDArray), None) if self._source is None or not self._source.exists() else self._read_data(H5Names.VECTORS_GROUP, False)\n    self._vectors, __ = vectors_tuple\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEH5TimeVectorEditor.NVEH5TimeVectorEditor.get_common_index","title":"<code>get_common_index() -&gt; NDArray | None</code>","text":"<p>Return a copy of a given index as a pandas series from the table.</p> Source code in <code>framdata/file_editors/NVEH5TimeVectorEditor.py</code> <pre><code>def get_common_index(self) -&gt; NDArray | None:\n    \"\"\"Return a copy of a given index as a pandas series from the table.\"\"\"\n    return self._common_index\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEH5TimeVectorEditor.NVEH5TimeVectorEditor.get_common_metadata","title":"<code>get_common_metadata() -&gt; None | dict</code>","text":"<p>Get a copy of the metadata of the parquet file.</p> Source code in <code>framdata/file_editors/NVEH5TimeVectorEditor.py</code> <pre><code>def get_common_metadata(self) -&gt; None | dict:\n    \"\"\"Get a copy of the metadata of the parquet file.\"\"\"\n    return self._common_metadata if self._common_metadata is None else self._common_metadata.copy()\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEH5TimeVectorEditor.NVEH5TimeVectorEditor.get_index","title":"<code>get_index(vector_id: str) -&gt; NDArray</code>","text":"<p>Return a copy of a given index as a pandas series from the table.</p> Source code in <code>framdata/file_editors/NVEH5TimeVectorEditor.py</code> <pre><code>def get_index(self, vector_id: str) -&gt; NDArray:\n    \"\"\"Return a copy of a given index as a pandas series from the table.\"\"\"\n    try:\n        return self._index[vector_id]\n    except KeyError as e:\n        f\"Found no ID '{vector_id}' among indexes.\"\n        raise KeyError from e\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEH5TimeVectorEditor.NVEH5TimeVectorEditor.get_metadata","title":"<code>get_metadata(vector_id: str) -&gt; None | dict</code>","text":"<p>Get a copy of the metadata of the parquet file.</p> Source code in <code>framdata/file_editors/NVEH5TimeVectorEditor.py</code> <pre><code>def get_metadata(self, vector_id: str) -&gt; None | dict:\n    \"\"\"Get a copy of the metadata of the parquet file.\"\"\"\n    try:\n        return self._metadata[vector_id]\n    except KeyError as e:\n        f\"Found no ID '{vector_id}' in metadata.\"\n        raise KeyError from e\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEH5TimeVectorEditor.NVEH5TimeVectorEditor.get_vector","title":"<code>get_vector(vector_id: str) -&gt; NDArray</code>","text":"<p>Return a copy of a given vector as a pandas series from the table.</p> Source code in <code>framdata/file_editors/NVEH5TimeVectorEditor.py</code> <pre><code>def get_vector(self, vector_id: str) -&gt; NDArray:\n    \"\"\"Return a copy of a given vector as a pandas series from the table.\"\"\"\n    try:\n        return self._vectors[vector_id]\n    except KeyError as e:\n        msg = f\"Found no ID '{vector_id}' among vectors.\"\n        raise KeyError(msg) from e\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEH5TimeVectorEditor.NVEH5TimeVectorEditor.get_vector_ids","title":"<code>get_vector_ids() -&gt; list[str]</code>","text":"<p>Get the IDs of all vectors.</p> Source code in <code>framdata/file_editors/NVEH5TimeVectorEditor.py</code> <pre><code>def get_vector_ids(self) -&gt; list[str]:\n    \"\"\"Get the IDs of all vectors.\"\"\"\n    return list(self._vectors.keys())\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEH5TimeVectorEditor.NVEH5TimeVectorEditor.set_common_index","title":"<code>set_common_index(values: NDArray) -&gt; None</code>","text":"<p>Set a whole index in the time index table.</p> Source code in <code>framdata/file_editors/NVEH5TimeVectorEditor.py</code> <pre><code>def set_common_index(self, values: NDArray) -&gt; None:\n    \"\"\"Set a whole index in the time index table.\"\"\"\n    self._check_type(values, np.ndarray)\n    self._common_index = values\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEH5TimeVectorEditor.NVEH5TimeVectorEditor.set_common_metadata","title":"<code>set_common_metadata(value: dict[str, METADATA_TYPES]) -&gt; None</code>","text":"<p>Set a field (new or overwrite) in the metadata.</p> Source code in <code>framdata/file_editors/NVEH5TimeVectorEditor.py</code> <pre><code>def set_common_metadata(self, value: dict[str, METADATA_TYPES]) -&gt; None:\n    \"\"\"Set a field (new or overwrite) in the metadata.\"\"\"\n    self._check_type(value, dict)\n    self._common_metadata = value\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEH5TimeVectorEditor.NVEH5TimeVectorEditor.set_index","title":"<code>set_index(vector_id: str, index: NDArray) -&gt; None</code>","text":"<p>Set a whole index in the time index table.</p> Source code in <code>framdata/file_editors/NVEH5TimeVectorEditor.py</code> <pre><code>def set_index(self, vector_id: str, index: NDArray) -&gt; None:\n    \"\"\"Set a whole index in the time index table.\"\"\"\n    self._check_type(vector_id, str)\n    self._check_type(index, np.ndarray)\n    self._index[vector_id] = index\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEH5TimeVectorEditor.NVEH5TimeVectorEditor.set_metadata","title":"<code>set_metadata(vector_id: str, value: dict[str, METADATA_TYPES]) -&gt; None</code>","text":"<p>Set a field (new or overwrite) in the metadata.</p> Source code in <code>framdata/file_editors/NVEH5TimeVectorEditor.py</code> <pre><code>def set_metadata(self, vector_id: str, value: dict[str, METADATA_TYPES]) -&gt; None:\n    \"\"\"Set a field (new or overwrite) in the metadata.\"\"\"\n    self._check_type(vector_id, str)\n    self._check_type(value, dict)\n    self._metadata[vector_id] = value\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEH5TimeVectorEditor.NVEH5TimeVectorEditor.set_vector","title":"<code>set_vector(vector_id: str, values: NDArray) -&gt; None</code>","text":"<p>Set a whole vector in the time vector table.</p> Source code in <code>framdata/file_editors/NVEH5TimeVectorEditor.py</code> <pre><code>def set_vector(self, vector_id: str, values: NDArray) -&gt; None:\n    \"\"\"Set a whole vector in the time vector table.\"\"\"\n    self._check_type(vector_id, str)\n    self._check_type(values, np.ndarray)\n    self._vectors[vector_id] = values\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEParquetTimeVectorEditor","title":"<code>NVEParquetTimeVectorEditor</code>","text":"<p>Contains class for editing time vectors in parquet files.</p>"},{"location":"reference/#framdata.file_editors.NVEParquetTimeVectorEditor.NVEParquetTimeVectorEditor","title":"<code>NVEParquetTimeVectorEditor</code>","text":"<p>               Bases: <code>NVEFileEditor</code></p> <p>Class for managing time vectors and their metadata stored in parquet files.</p> Source code in <code>framdata/file_editors/NVEParquetTimeVectorEditor.py</code> <pre><code>class NVEParquetTimeVectorEditor(NVEFileEditor):\n    \"\"\"Class for managing time vectors and their metadata stored in parquet files.\"\"\"\n\n    def __init__(self, source: Path | str | None = None) -&gt; None:\n        \"\"\"\n        Set path to parquet file if supplied, load/initialize table and metadata as pd.DataFrame and dictionary respectively.\n\n        Args:\n            source (Path | str | None, optional): Path to parquet file with timevectors. Defaults to None.\n\n        \"\"\"\n        super().__init__(source)\n        self._metadata, __ = ({}, None) if self._source is None or not self._source.exists() else self._read_metadata()\n        self._data = pd.DataFrame() if self._source is None or not self._source.exists() else pd.read_parquet(self._source)\n\n    def save_to_parquet(self, path: Path | str) -&gt; None:\n        \"\"\"\n        Save the edited dataframe and metadata to parquet file.\n\n        Args:\n            path (Path): Path to save tha file to. Must be defined to force user to explicitly overwrite the original file if they want.\n\n        \"\"\"\n        self._check_type(path, (Path, str))\n        path = Path(path)\n        table = pa.Table.from_pandas(self._data)\n\n        # ensure binary strings with defined encoding, since parquet encodes metadata anyway\n        schema_with_meta = table.schema.with_metadata({str(k).encode(TvMn.ENCODING): str(v).encode(TvMn.ENCODING) for k, v in self._metadata.items()})\n        table = pa.Table.from_pandas(self._data, schema=schema_with_meta)\n\n        pq.write_table(table, path)\n\n    def get_metadata(self):\n        \"\"\"Get a copy of the metadata of the parquet file.\"\"\"\n        return self._metadata if self._metadata is None else self._metadata.copy()\n\n    def set_metadata(self, key: str, value: bool | int | str | datetime | timedelta | tzinfo | None) -&gt; None:\n        \"\"\"Set a field (new or overwrite) in the metadata.\"\"\"\n        self._check_type(key, str)\n        self._check_type(value, (bool, int, str, datetime, timedelta, tzinfo, type(None)))\n        self._metadata[key] = value\n\n    def set_vector(self, vector_id: str, values: pd.Series) -&gt; None:\n        \"\"\"Set a whole vector in the time vector table.\"\"\"\n        self._check_type(vector_id, str)\n        self._check_type(values, pd.Series)\n        if not self._data.empty and len(values) != len(self._data):\n            message = f\"Series values has different size than the other vectors in the table.\\nLength values: {len(values)}\\nLength vectors: {len(self._data)}\"\n            raise IndexError(message)\n        self._data[vector_id] = values\n\n    def get_vector(self, vector_id: str) -&gt; pd.Series:\n        \"\"\"Return a copy of a given vector as a pandas series from the table.\"\"\"\n        try:\n            return self._data[vector_id].copy()\n        except KeyError as e:\n            f\"Found no vector named '{vector_id}' in table at {self._source}.\"\n            raise KeyError from e\n\n    def get_dataframe(self) -&gt; pd.DataFrame:\n        \"\"\"Return a copy of all of the vector table as a pandas dataframe.\"\"\"\n        return self._data.copy()\n\n    def set_dataframe(self, dataframe: pd.DataFrame) -&gt; None:\n        \"\"\"Set the dataframe of the editor.\"\"\"\n        self._check_type(dataframe, pd.DataFrame)\n        self._data = dataframe\n\n    def get_vector_ids(self) -&gt; list[str]:\n        \"\"\"Get the IDs of all vectors.\"\"\"\n        return [c for c in self._data.columns if c != TvMn.DATETIME_COL]\n\n    def set_index_column(self, index: pd.Series) -&gt; None:\n        \"\"\"Set the index column.\"\"\"\n        self._check_type(index, pd.Series)\n        if not self._data.empty and len(index) != len(self._data):\n            message = f\"Series index has different size than the other vectors in the table.\\nLength index: {len(index)}\\nLength vectors: {len(self._data)}\"\n            raise IndexError(message)\n        self._data[TvMn.DATETIME_COL] = index\n\n    def get_index_column(self) -&gt; pd.Series:\n        \"\"\"Get the datetime column of the dataframe.\"\"\"\n        if TvMn.DATETIME_COL not in self._data:\n            message = f\"Table at {self._source} does not have an index column. Index column must exist and be named '{TvMn.DATETIME_COL}'.\"\n            raise KeyError(message)\n        return self._data[TvMn.DATETIME_COL].copy()\n\n    def _read_metadata(self) -&gt; tuple[dict[str, bool | int | str | datetime | timedelta | tzinfo | None], set[str]]:\n        if self._source is None:\n            message = \"Must set a source before reading file.\"\n            raise ValueError(message)\n        metadata = pq.ParquetFile(self._source).schema_arrow.metadata\n        return TvMn.cast_meta(metadata)\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEParquetTimeVectorEditor.NVEParquetTimeVectorEditor.__init__","title":"<code>__init__(source: Path | str | None = None) -&gt; None</code>","text":"<p>Set path to parquet file if supplied, load/initialize table and metadata as pd.DataFrame and dictionary respectively.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Path | str | None</code> <p>Path to parquet file with timevectors. Defaults to None.</p> <code>None</code> Source code in <code>framdata/file_editors/NVEParquetTimeVectorEditor.py</code> <pre><code>def __init__(self, source: Path | str | None = None) -&gt; None:\n    \"\"\"\n    Set path to parquet file if supplied, load/initialize table and metadata as pd.DataFrame and dictionary respectively.\n\n    Args:\n        source (Path | str | None, optional): Path to parquet file with timevectors. Defaults to None.\n\n    \"\"\"\n    super().__init__(source)\n    self._metadata, __ = ({}, None) if self._source is None or not self._source.exists() else self._read_metadata()\n    self._data = pd.DataFrame() if self._source is None or not self._source.exists() else pd.read_parquet(self._source)\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEParquetTimeVectorEditor.NVEParquetTimeVectorEditor.get_dataframe","title":"<code>get_dataframe() -&gt; pd.DataFrame</code>","text":"<p>Return a copy of all of the vector table as a pandas dataframe.</p> Source code in <code>framdata/file_editors/NVEParquetTimeVectorEditor.py</code> <pre><code>def get_dataframe(self) -&gt; pd.DataFrame:\n    \"\"\"Return a copy of all of the vector table as a pandas dataframe.\"\"\"\n    return self._data.copy()\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEParquetTimeVectorEditor.NVEParquetTimeVectorEditor.get_index_column","title":"<code>get_index_column() -&gt; pd.Series</code>","text":"<p>Get the datetime column of the dataframe.</p> Source code in <code>framdata/file_editors/NVEParquetTimeVectorEditor.py</code> <pre><code>def get_index_column(self) -&gt; pd.Series:\n    \"\"\"Get the datetime column of the dataframe.\"\"\"\n    if TvMn.DATETIME_COL not in self._data:\n        message = f\"Table at {self._source} does not have an index column. Index column must exist and be named '{TvMn.DATETIME_COL}'.\"\n        raise KeyError(message)\n    return self._data[TvMn.DATETIME_COL].copy()\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEParquetTimeVectorEditor.NVEParquetTimeVectorEditor.get_metadata","title":"<code>get_metadata()</code>","text":"<p>Get a copy of the metadata of the parquet file.</p> Source code in <code>framdata/file_editors/NVEParquetTimeVectorEditor.py</code> <pre><code>def get_metadata(self):\n    \"\"\"Get a copy of the metadata of the parquet file.\"\"\"\n    return self._metadata if self._metadata is None else self._metadata.copy()\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEParquetTimeVectorEditor.NVEParquetTimeVectorEditor.get_vector","title":"<code>get_vector(vector_id: str) -&gt; pd.Series</code>","text":"<p>Return a copy of a given vector as a pandas series from the table.</p> Source code in <code>framdata/file_editors/NVEParquetTimeVectorEditor.py</code> <pre><code>def get_vector(self, vector_id: str) -&gt; pd.Series:\n    \"\"\"Return a copy of a given vector as a pandas series from the table.\"\"\"\n    try:\n        return self._data[vector_id].copy()\n    except KeyError as e:\n        f\"Found no vector named '{vector_id}' in table at {self._source}.\"\n        raise KeyError from e\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEParquetTimeVectorEditor.NVEParquetTimeVectorEditor.get_vector_ids","title":"<code>get_vector_ids() -&gt; list[str]</code>","text":"<p>Get the IDs of all vectors.</p> Source code in <code>framdata/file_editors/NVEParquetTimeVectorEditor.py</code> <pre><code>def get_vector_ids(self) -&gt; list[str]:\n    \"\"\"Get the IDs of all vectors.\"\"\"\n    return [c for c in self._data.columns if c != TvMn.DATETIME_COL]\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEParquetTimeVectorEditor.NVEParquetTimeVectorEditor.save_to_parquet","title":"<code>save_to_parquet(path: Path | str) -&gt; None</code>","text":"<p>Save the edited dataframe and metadata to parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to save tha file to. Must be defined to force user to explicitly overwrite the original file if they want.</p> required Source code in <code>framdata/file_editors/NVEParquetTimeVectorEditor.py</code> <pre><code>def save_to_parquet(self, path: Path | str) -&gt; None:\n    \"\"\"\n    Save the edited dataframe and metadata to parquet file.\n\n    Args:\n        path (Path): Path to save tha file to. Must be defined to force user to explicitly overwrite the original file if they want.\n\n    \"\"\"\n    self._check_type(path, (Path, str))\n    path = Path(path)\n    table = pa.Table.from_pandas(self._data)\n\n    # ensure binary strings with defined encoding, since parquet encodes metadata anyway\n    schema_with_meta = table.schema.with_metadata({str(k).encode(TvMn.ENCODING): str(v).encode(TvMn.ENCODING) for k, v in self._metadata.items()})\n    table = pa.Table.from_pandas(self._data, schema=schema_with_meta)\n\n    pq.write_table(table, path)\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEParquetTimeVectorEditor.NVEParquetTimeVectorEditor.set_dataframe","title":"<code>set_dataframe(dataframe: pd.DataFrame) -&gt; None</code>","text":"<p>Set the dataframe of the editor.</p> Source code in <code>framdata/file_editors/NVEParquetTimeVectorEditor.py</code> <pre><code>def set_dataframe(self, dataframe: pd.DataFrame) -&gt; None:\n    \"\"\"Set the dataframe of the editor.\"\"\"\n    self._check_type(dataframe, pd.DataFrame)\n    self._data = dataframe\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEParquetTimeVectorEditor.NVEParquetTimeVectorEditor.set_index_column","title":"<code>set_index_column(index: pd.Series) -&gt; None</code>","text":"<p>Set the index column.</p> Source code in <code>framdata/file_editors/NVEParquetTimeVectorEditor.py</code> <pre><code>def set_index_column(self, index: pd.Series) -&gt; None:\n    \"\"\"Set the index column.\"\"\"\n    self._check_type(index, pd.Series)\n    if not self._data.empty and len(index) != len(self._data):\n        message = f\"Series index has different size than the other vectors in the table.\\nLength index: {len(index)}\\nLength vectors: {len(self._data)}\"\n        raise IndexError(message)\n    self._data[TvMn.DATETIME_COL] = index\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEParquetTimeVectorEditor.NVEParquetTimeVectorEditor.set_metadata","title":"<code>set_metadata(key: str, value: bool | int | str | datetime | timedelta | tzinfo | None) -&gt; None</code>","text":"<p>Set a field (new or overwrite) in the metadata.</p> Source code in <code>framdata/file_editors/NVEParquetTimeVectorEditor.py</code> <pre><code>def set_metadata(self, key: str, value: bool | int | str | datetime | timedelta | tzinfo | None) -&gt; None:\n    \"\"\"Set a field (new or overwrite) in the metadata.\"\"\"\n    self._check_type(key, str)\n    self._check_type(value, (bool, int, str, datetime, timedelta, tzinfo, type(None)))\n    self._metadata[key] = value\n</code></pre>"},{"location":"reference/#framdata.file_editors.NVEParquetTimeVectorEditor.NVEParquetTimeVectorEditor.set_vector","title":"<code>set_vector(vector_id: str, values: pd.Series) -&gt; None</code>","text":"<p>Set a whole vector in the time vector table.</p> Source code in <code>framdata/file_editors/NVEParquetTimeVectorEditor.py</code> <pre><code>def set_vector(self, vector_id: str, values: pd.Series) -&gt; None:\n    \"\"\"Set a whole vector in the time vector table.\"\"\"\n    self._check_type(vector_id, str)\n    self._check_type(values, pd.Series)\n    if not self._data.empty and len(values) != len(self._data):\n        message = f\"Series values has different size than the other vectors in the table.\\nLength values: {len(values)}\\nLength vectors: {len(self._data)}\"\n        raise IndexError(message)\n    self._data[vector_id] = values\n</code></pre>"},{"location":"reference/#framdata.loaders","title":"<code>loaders</code>","text":""},{"location":"reference/#framdata.loaders.NVEExcelTimeVectorLoader","title":"<code>NVEExcelTimeVectorLoader</code>","text":"<p>               Bases: <code>NVETimeVectorLoader</code></p> <p>Class for loading time vector data from NVE excel file sources.</p> <p>Meant for short time vectors (e.g. yearly volumes or installed capacities) which are desireable to view and edit easily through Excel. Supports the followinf formats:     - 'Horizontal': One column containing IDs, the other column names represents the index. Vector values as rows     - 'Vertical': One column as index (DateTime), the oher columns names are vector IDs. Vectors as column values.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>class NVEExcelTimeVectorLoader(NVETimeVectorLoader):\n    \"\"\"\n    Class for loading time vector data from NVE excel file sources.\n\n    Meant for short time vectors (e.g. yearly volumes or installed capacities) which are desireable to view and edit easily through Excel.\n    Supports the followinf formats:\n        - 'Horizontal': One column containing IDs, the other column names represents the index. Vector values as rows\n        - 'Vertical': One column as index (DateTime), the oher columns names are vector IDs. Vectors as column values.\n\n    \"\"\"\n\n    _SUPPORTED_SUFFIXES: ClassVar[list] = [\".xlsx\"]\n    _DATA_SHEET = \"Data\"\n    _METADATA_SHEET = \"Metadata\"\n\n    def __init__(self, source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None:\n        \"\"\"\n        Intitialize loader instance and connect it to an Excel file containing time vector data.\n\n        Args:\n            source (Path | str): Absolute Path to database or excel file.\n            require_whole_years (bool): Flag for validating that the time vectors in the source contain data for complete years.\n            relative_loc (Path | str | None, optional): Path to excel file relative to source. Defaults to None.\n            validate (bool, optional): Flag to turn on validation of timevectors. NB! Loads all data into memory at once. Defaults to True.\n\n        \"\"\"\n        super().__init__(source, require_whole_years, relative_loc)\n        self._index: TimeIndex = None\n\n        if validate:\n            self.validate_vectors()\n\n    def get_unit(self, vector_id: str) -&gt; str:\n        \"\"\"\n        Get the unit of the given time vector.\n\n        Args:\n            vector_id (str): ID of a time vector. Not used since all time vectors in the NVE excel files have the same\n                             unit.\n\n        Returns:\n            str: Unit of the time vector.\n\n        \"\"\"\n        return self.get_metadata(\"\")[TvMn.UNIT]\n\n    def get_values(self, vector_id: str) -&gt; NDArray:\n        \"\"\"\n        Get numpy array with all the values of a given vector in the Loader's excel file.\n\n        Args:\n            vector_id (str): Unique id of the vector in the file.\n\n        Returns:\n            NDArray: Numpy array with values.\n\n        \"\"\"\n        if self._data is None:\n            self._data = pd.DataFrame()\n        if vector_id not in self._data.columns:\n            issmallformat = self._is_horizontal_format()\n            column_filter = [vector_id]\n            usecols = None\n            if not issmallformat:\n                usecols = column_filter\n\n            values_df = pd.read_excel(self.get_source(), sheet_name=self._DATA_SHEET, usecols=usecols)\n\n            if issmallformat:  # Convert the table to large time series format\n                values_df = self._process_horizontal_format(values_df)\n                values_df = self._enforce_dtypes(values_df, issmallformat)\n                self._data = values_df\n            else:\n                values_df = self._enforce_dtypes(values_df, issmallformat)\n                self._data[vector_id] = values_df\n        return self._data[vector_id].to_numpy()\n\n    def get_index(self, vector_id: str) -&gt; ListTimeIndex:\n        \"\"\"\n        Get the TimeIndex describing the time dimension of the vectors in the file.\n\n        Args:\n            vector_id (str): Not used since all vectors in the NVE excel files have the same index.\n\n        Returns:\n            TimeIndex: TimeIndex object describing the excel file's index.\n\n        \"\"\"\n        meta = self.get_metadata(\"\")\n        if self._index is None:\n            self.get_values(TvMn.DATETIME_COL)\n            self._index = ListTimeIndex(\n                self._data[TvMn.DATETIME_COL].tolist(),\n                is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n                extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n                extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n            )\n        return self._index\n\n    def get_metadata(self, vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]:\n        \"\"\"\n        Read Excel file metadata.\n\n        Args:\n            vector_id (str): Not used.\n\n        Raises:\n            KeyError: If an expected metadata key is missing.\n\n        Returns:\n            dict[str, bool|int|str|datetime|timedelta|tzinfo|None]: Metadata dictionary.\n\n        \"\"\"\n        if self._meta is None:\n            path = self.get_source()\n            raw_meta = pd.read_excel(path, sheet_name=self._METADATA_SHEET, na_values=[\"\"]).replace([np.nan], [None]).to_dict(\"records\")[0]\n\n            self._meta = self._process_meta(raw_meta)\n        return self._meta\n\n    def _enforce_dtypes(self, values_df: pd.DataFrame | pd.Series, issmallformat: bool) -&gt; pd.DataFrame:\n        set_dtypes = \"float\"\n        if isinstance(values_df, pd.DataFrame):\n            set_dtypes = {c: \"float\" for c in values_df.columns if c != TvMn.DATETIME_COL}\n\n        # ensure correct dtypes\n        try:\n            return values_df.astype(set_dtypes)\n        except ValueError as e:\n            index_column = TvMn.ID_COLUMN_NAME if issmallformat else TvMn.DATETIME_COL\n            message = f\"Error in {self} while reading file. All columns except '{index_column}' must consist of only float or integer numbers.\"\n            raise RuntimeError(message) from e\n\n    def _process_horizontal_format(self, horizontal_format_df: pd.DataFrame) -&gt; pd.DataFrame:\n        # We have to read the whole file to find the correct series\n\n        # Rename the id column name and then transpose to get the correct format\n        # Since the columns are counted as indices when transposing, we need to reset the index (but keep the DateTime\n        # column)\n        reformat_df = horizontal_format_df.rename(columns={TvMn.ID_COLUMN_NAME: TvMn.DATETIME_COL}).T.reset_index(drop=False)\n\n        # after transposing, column names are set a the first row, which is DateTime, IDs\n        reformat_df.columns = reformat_df.iloc[0]\n        # We reindex by dropping the first row, thus removing the row of DateTime, IDs\n        reformat_df = reformat_df.reindex(reformat_df.index.drop(0)).reset_index(drop=True)\n\n        # Since It is possible to write only year or year-month as timestamp in the table,\n        # we need to reformat to correct datetime format\n        reformat_df[TvMn.DATETIME_COL] = self._to_iso_datetimes(reformat_df[TvMn.DATETIME_COL])\n\n        return reformat_df\n\n    def _to_iso_datetimes(self, series: pd.Series) -&gt; list[datetime]:\n        \"\"\"\n        Convert a series of dates to ISO datetime format.\n\n        Args:\n            series (pd.Series): Series which values will be converted to ISO format.\n\n        Raises:\n            RuntimeError: When an input value which cannot be converted is encountered.\n\n        Returns:\n            list[datetime]: List of formatted datetimes.\n\n        \"\"\"\n        reformatted = []\n        three_segments = 3\n        two_segments = 2\n        one_segment = 1\n        for i in series:\n            new_i = str(i)\n            date_split = len(new_i.split(\"-\"))\n            space_split = len(new_i.split(\" \"))\n            time_split = len(new_i.split(\":\"))\n            try:\n                if date_split == one_segment:  # Only year is defined\n                    # get datetime for first week first day\n                    new_i = datetime.fromisocalendar(int(new_i), 1, 1)\n                elif date_split == two_segments:\n                    # Year and month is defined\n                    new_i = datetime.strptime(new_i + \"-01\", \"%Y-%m-%d\")  # Add first day\n                elif date_split == three_segments and space_split == one_segment and time_split == one_segment:\n                    # days defined but not time\n                    new_i = datetime.strptime(new_i, \"%Y-%m-%d\")\n                elif date_split == three_segments and space_split == two_segments and time_split == one_segment:\n                    new_i = datetime.strptime(new_i, \"%Y-%m-%d %H\")\n                elif date_split == three_segments and space_split == two_segments and time_split == two_segments:\n                    new_i = datetime.strptime(new_i, \"%Y-%m-%d %H:%M\")\n                elif date_split == three_segments and space_split == two_segments and time_split == three_segments:\n                    # Assume time is defined\n                    new_i = datetime.strptime(new_i, \"%Y-%m-%d %H:%M:%S\")\n                else:\n                    msg = f\"Could not convert value '{new_i}' to datetime format.\"\n                    raise ValueError(msg)\n            except Exception as e:\n                msg = f\"Loader {self} could not convert value '{new_i}' to datetime format. Check formatting, for example number of spaces.\"\n                raise RuntimeError(msg) from e\n            reformatted.append(new_i)\n        return sorted(reformatted)\n\n    def _is_horizontal_format(self) -&gt; bool:\n        \"\"\"Determine if the file strucure is the NVE small format.\"\"\"\n        column_names = pd.read_excel(self.get_source(), nrows=0, sheet_name=self._DATA_SHEET).columns.tolist()\n        return TvMn.ID_COLUMN_NAME in column_names\n\n    def _get_ids(self) -&gt; list[str]:\n        if self._content_ids is not None:\n            return self._content_ids\n        try:\n            if self._is_horizontal_format():\n                self._content_ids = pd.read_excel(\n                    self.get_source(),\n                    usecols=[TvMn.ID_COLUMN_NAME],\n                    sheet_name=self._DATA_SHEET,\n                )[TvMn.ID_COLUMN_NAME].tolist()\n            else:\n                columns_list = pd.read_excel(self.get_source(), nrows=0, sheet_name=self._DATA_SHEET).columns.tolist()\n                columns_list.remove(TvMn.DATETIME_COL)\n                self._content_ids = columns_list\n        except ValueError as e:\n            message = f\"{self}: found problem with TimeVector IDs.\"\n            raise RuntimeError(message) from e\n\n        return self._content_ids\n\n    def clear_cache(self) -&gt; None:\n        \"\"\"Clear cached data.\"\"\"\n        self._data = None\n        self._meta = None\n        self._index = None\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEExcelTimeVectorLoader.__init__","title":"<code>__init__(source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None</code>","text":"<p>Intitialize loader instance and connect it to an Excel file containing time vector data.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Path | str</code> <p>Absolute Path to database or excel file.</p> required <code>require_whole_years</code> <code>bool</code> <p>Flag for validating that the time vectors in the source contain data for complete years.</p> required <code>relative_loc</code> <code>Path | str | None</code> <p>Path to excel file relative to source. Defaults to None.</p> <code>None</code> <code>validate</code> <code>bool</code> <p>Flag to turn on validation of timevectors. NB! Loads all data into memory at once. Defaults to True.</p> <code>True</code> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def __init__(self, source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None:\n    \"\"\"\n    Intitialize loader instance and connect it to an Excel file containing time vector data.\n\n    Args:\n        source (Path | str): Absolute Path to database or excel file.\n        require_whole_years (bool): Flag for validating that the time vectors in the source contain data for complete years.\n        relative_loc (Path | str | None, optional): Path to excel file relative to source. Defaults to None.\n        validate (bool, optional): Flag to turn on validation of timevectors. NB! Loads all data into memory at once. Defaults to True.\n\n    \"\"\"\n    super().__init__(source, require_whole_years, relative_loc)\n    self._index: TimeIndex = None\n\n    if validate:\n        self.validate_vectors()\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEExcelTimeVectorLoader.clear_cache","title":"<code>clear_cache() -&gt; None</code>","text":"<p>Clear cached data.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def clear_cache(self) -&gt; None:\n    \"\"\"Clear cached data.\"\"\"\n    self._data = None\n    self._meta = None\n    self._index = None\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEExcelTimeVectorLoader.get_index","title":"<code>get_index(vector_id: str) -&gt; ListTimeIndex</code>","text":"<p>Get the TimeIndex describing the time dimension of the vectors in the file.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Not used since all vectors in the NVE excel files have the same index.</p> required <p>Returns:</p> Name Type Description <code>TimeIndex</code> <code>ListTimeIndex</code> <p>TimeIndex object describing the excel file's index.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_index(self, vector_id: str) -&gt; ListTimeIndex:\n    \"\"\"\n    Get the TimeIndex describing the time dimension of the vectors in the file.\n\n    Args:\n        vector_id (str): Not used since all vectors in the NVE excel files have the same index.\n\n    Returns:\n        TimeIndex: TimeIndex object describing the excel file's index.\n\n    \"\"\"\n    meta = self.get_metadata(\"\")\n    if self._index is None:\n        self.get_values(TvMn.DATETIME_COL)\n        self._index = ListTimeIndex(\n            self._data[TvMn.DATETIME_COL].tolist(),\n            is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n            extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n            extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n        )\n    return self._index\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEExcelTimeVectorLoader.get_metadata","title":"<code>get_metadata(vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]</code>","text":"<p>Read Excel file metadata.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Not used.</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>If an expected metadata key is missing.</p> <p>Returns:</p> Type Description <code>dict[str, bool | int | str | datetime | timedelta | tzinfo | None]</code> <p>dict[str, bool|int|str|datetime|timedelta|tzinfo|None]: Metadata dictionary.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_metadata(self, vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]:\n    \"\"\"\n    Read Excel file metadata.\n\n    Args:\n        vector_id (str): Not used.\n\n    Raises:\n        KeyError: If an expected metadata key is missing.\n\n    Returns:\n        dict[str, bool|int|str|datetime|timedelta|tzinfo|None]: Metadata dictionary.\n\n    \"\"\"\n    if self._meta is None:\n        path = self.get_source()\n        raw_meta = pd.read_excel(path, sheet_name=self._METADATA_SHEET, na_values=[\"\"]).replace([np.nan], [None]).to_dict(\"records\")[0]\n\n        self._meta = self._process_meta(raw_meta)\n    return self._meta\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEExcelTimeVectorLoader.get_unit","title":"<code>get_unit(vector_id: str) -&gt; str</code>","text":"<p>Get the unit of the given time vector.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>ID of a time vector. Not used since all time vectors in the NVE excel files have the same              unit.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Unit of the time vector.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_unit(self, vector_id: str) -&gt; str:\n    \"\"\"\n    Get the unit of the given time vector.\n\n    Args:\n        vector_id (str): ID of a time vector. Not used since all time vectors in the NVE excel files have the same\n                         unit.\n\n    Returns:\n        str: Unit of the time vector.\n\n    \"\"\"\n    return self.get_metadata(\"\")[TvMn.UNIT]\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEExcelTimeVectorLoader.get_values","title":"<code>get_values(vector_id: str) -&gt; NDArray</code>","text":"<p>Get numpy array with all the values of a given vector in the Loader's excel file.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Unique id of the vector in the file.</p> required <p>Returns:</p> Name Type Description <code>NDArray</code> <code>NDArray</code> <p>Numpy array with values.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_values(self, vector_id: str) -&gt; NDArray:\n    \"\"\"\n    Get numpy array with all the values of a given vector in the Loader's excel file.\n\n    Args:\n        vector_id (str): Unique id of the vector in the file.\n\n    Returns:\n        NDArray: Numpy array with values.\n\n    \"\"\"\n    if self._data is None:\n        self._data = pd.DataFrame()\n    if vector_id not in self._data.columns:\n        issmallformat = self._is_horizontal_format()\n        column_filter = [vector_id]\n        usecols = None\n        if not issmallformat:\n            usecols = column_filter\n\n        values_df = pd.read_excel(self.get_source(), sheet_name=self._DATA_SHEET, usecols=usecols)\n\n        if issmallformat:  # Convert the table to large time series format\n            values_df = self._process_horizontal_format(values_df)\n            values_df = self._enforce_dtypes(values_df, issmallformat)\n            self._data = values_df\n        else:\n            values_df = self._enforce_dtypes(values_df, issmallformat)\n            self._data[vector_id] = values_df\n    return self._data[vector_id].to_numpy()\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEH5TimeVectorLoader","title":"<code>NVEH5TimeVectorLoader</code>","text":"<p>               Bases: <code>NVETimeVectorLoader</code></p> <p>Class for loading time vector data from NVE HDF5 file sources.</p> <p>Meant for large time vectors (e.g. hourly data over multiple years). Supports differing lengths and metadata of vectors stored in the file.</p> Specialized to the following format <ul> <li>index (h5py.Group, optional): Used to define indexes for vectors if index is supposed to only apply to that vector.</li> <li>common_index (h5py.Dataset): Contains one numpy array for all vectors. This is a fallback index for vectors which have not defined their own index in                                the index group. Also used on purpose if many or all vectors have the same index.</li> <li>metadata (h5py.Group): Used connect a specific set of metadata to a particular vector.</li> <li>common_metadata (h5py.Group): Contains one set of metadata fields for all vectors. Used in a similar way as common_index.</li> <li>vectors (h5py.Group): Contains numpy arrays containing the vector values connected to a unique ID. The same ID is used to connect the vector to an                         index or metadata.</li> </ul> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>class NVEH5TimeVectorLoader(NVETimeVectorLoader):\n    \"\"\"\n    Class for loading time vector data from NVE HDF5 file sources.\n\n    Meant for large time vectors (e.g. hourly data over multiple years). Supports differing lengths and metadata of vectors stored in the file.\n\n    Specialized to the following format:\n        - index (h5py.Group, optional): Used to define indexes for vectors if index is supposed to only apply to that vector.\n        - common_index (h5py.Dataset): Contains one numpy array for all vectors. This is a fallback index for vectors which have not defined their own index in\n                                       the index group. Also used on purpose if many or all vectors have the same index.\n        - metadata (h5py.Group): Used connect a specific set of metadata to a particular vector.\n        - common_metadata (h5py.Group): Contains one set of metadata fields for all vectors. Used in a similar way as common_index.\n        - vectors (h5py.Group): Contains numpy arrays containing the vector values connected to a unique ID. The same ID is used to connect the vector to an\n                                index or metadata.\n\n    \"\"\"\n\n    _SUPPORTED_SUFFIXES: ClassVar[list] = [\".h5\", \".hdf5\"]\n\n    def __init__(self, source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None:\n        \"\"\"\n        Intitialize loader instance and connect it to a H5 file containing time vector data.\n\n        Args:\n            source (Path | str): Absolute Path to database or HDF5 file.\n            require_whole_years (bool): Flag for validating that the time vectors in the source contain data for complete years.\n            relative_loc (Path | str | None, optional): Path to HDF5 file relative to source. Defaults to None.\n            validate (bool, optional): Whether to validate vectors after loading. NB! Loads all data into memory at once. Defaults to True.\n\n        \"\"\"\n        super().__init__(source, require_whole_years, relative_loc)\n        self._index: TimeIndex = None\n        self._file_pointer = None\n\n        if validate:\n            self.validate_vectors()\n\n    def get_values(self, vector_id: str) -&gt; NDArray:\n        \"\"\"\n        Get numpy array with all the values of a given vector in the Loader's HDF5 file.\n\n        Args:\n            vector_id (str): Unique id of the vector in the file.\n\n        Returns:\n            NDArray: Numpy array with values.\n\n        \"\"\"\n        if self._data is None:\n            self._data = dict()\n        if vector_id not in self._data:\n            with h5py.File(self.get_source(), mode=\"r\") as h5f:\n                self._data[vector_id] = self._read_vector_field(h5f, H5Names.VECTORS_GROUP, vector_id, field_type=h5py.Dataset, use_fallback=False)[()]\n        return self._data[vector_id]\n\n    def get_index(self, vector_id: str) -&gt; TimeIndex:\n        \"\"\"\n        Get the TimeIndex describing the time dimension of the vectors in the file.\n\n        Args:\n            vector_id (str): Not used since all vectors in the NVE parquet files have the same index.\n\n        Returns:\n            TimeIndex: TimeIndex object describing the parquet file's index.\n\n        \"\"\"\n        if self._index is None:\n            meta = self.get_metadata(\"\")\n\n            if meta[TvMn.FREQUENCY] is None:\n                self._index = ListTimeIndex(\n                    datetime_list=self._read_index(vector_id),\n                    is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n                    extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n                    extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n                )\n                return self._index\n\n            start = pd.to_datetime(self._read_index(vector_id)[0]) if meta[TvMn.START] is None else meta[TvMn.START]\n            num_points = self._read_index(vector_id).size if meta[TvMn.NUM_POINTS] is None else meta[TvMn.NUM_POINTS]\n\n            self._index = FixedFrequencyTimeIndex(\n                start,\n                meta[TvMn.FREQUENCY],\n                num_points,\n                is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n                extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n                extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n            )\n\n        return self._index\n\n    def _read_index(self, vector_id: str) -&gt; NDArray:\n        with h5py.File(self.get_source(), mode=\"r\") as h5f:\n            return np.char.decode(self._read_vector_field(h5f, H5Names.INDEX_GROUP, vector_id, h5py.Dataset)[()], encoding=\"utf-8\").astype(datetime)\n\n    def _read_vector_field(\n        self,\n        h5file: h5py.File,\n        field_name: str,\n        vector_id: str,\n        field_type: type[h5py.Dataset | h5py.Group],\n        use_fallback: bool = True,\n    ) -&gt; h5py.Dataset | h5py.Group:\n        error = \"\"\n        if field_name in h5file:  # check if group_name exists\n            main_group = h5file[field_name]\n            if not isinstance(main_group, h5py.Group):\n                message = f\"{self} expected '{field_name}' to be a {h5py.Group} in {h5file}. Got {type(main_group)}.\"\n                raise TypeError(message)\n\n            if vector_id in main_group:\n                vector_field = main_group[vector_id]\n                if not isinstance(vector_field, field_type):\n                    message = f\"{self} expected '{vector_id}' to be a {field_type} in {h5file}. Got {type(vector_field)}\"\n                    raise TypeError(message)\n                return vector_field\n            error = f\"'{vector_id}' was not found in '{field_name}' group\"\n        else:\n            error = f\"'{field_name}' was not found in file\"\n\n        no_fallback_message = f\"{self} expected '{vector_id}' in {h5py.Group} '{field_name}' \"\n        if not use_fallback:\n            no_fallback_message += f\"but {error}.\"\n            raise KeyError(no_fallback_message)\n\n        fallback_name = H5Names.COMMON_PREFIX + field_name\n        if fallback_name in h5file:  # check if common_ + group_name exists\n            fallback_field = h5file[fallback_name]\n            if not isinstance(fallback_field, field_type):\n                message = f\"{self} expected '{fallback_field}' to be a {field_type} in {h5file}. Got {type(fallback_field)}.\"\n                raise TypeError(message)\n            return fallback_field\n\n        message = (\n            no_fallback_message\n            + f\"or a fallback {field_type} '{fallback_name}' in H5 file but \"\n            + f\"{error},\"\n            + f\" and fallback {field_type} '{fallback_name}' not found in file.\"\n        )\n        raise KeyError(message)\n\n    def get_metadata(self, vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]:\n        \"\"\"\n        Retrieve and decodes custom metadata from parquet file.\n\n        Args:\n            vector_id (str): Not used\n\n        Raises:\n            KeyError: If any of the expected metadata keys is not found in file.\n\n        Returns:\n            dict: Dictionary with decoded metadata.\n\n        \"\"\"\n        if self._meta is None:\n            errors = set()\n            meta = {}\n            with h5py.File(self.get_source(), mode=\"r\") as h5f:\n                meta_group = self._read_vector_field(h5f, H5Names.METADATA_GROUP, vector_id, h5py.Group)\n                for k, m in meta_group.items():\n                    if isinstance(m, h5py.Dataset):\n                        meta[k] = m[()]\n                    else:\n                        errors.add(f\"Improper metadata format: Metadata key {k} exists but is a h5 group when it should be a h5 dataset.\")\n            self._report_errors(errors)\n            self._meta = self._process_meta(meta)\n        return self._meta\n\n    def _get_ids(self) -&gt; list[str]:\n        with h5py.File(self.get_source(), mode=\"r\") as h5f:\n            if H5Names.VECTORS_GROUP in h5f:\n                return list(h5f[H5Names.VECTORS_GROUP].keys())\n            message = f\"{self} required key '{H5Names.VECTORS_GROUP}' was not found in file.\"\n            raise KeyError(message)\n\n    def clear_cache(self) -&gt; None:\n        \"\"\"Clear cached data.\"\"\"\n        self._data = None\n        self._meta = None\n        self._index = None\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEH5TimeVectorLoader.__init__","title":"<code>__init__(source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None</code>","text":"<p>Intitialize loader instance and connect it to a H5 file containing time vector data.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Path | str</code> <p>Absolute Path to database or HDF5 file.</p> required <code>require_whole_years</code> <code>bool</code> <p>Flag for validating that the time vectors in the source contain data for complete years.</p> required <code>relative_loc</code> <code>Path | str | None</code> <p>Path to HDF5 file relative to source. Defaults to None.</p> <code>None</code> <code>validate</code> <code>bool</code> <p>Whether to validate vectors after loading. NB! Loads all data into memory at once. Defaults to True.</p> <code>True</code> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def __init__(self, source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None:\n    \"\"\"\n    Intitialize loader instance and connect it to a H5 file containing time vector data.\n\n    Args:\n        source (Path | str): Absolute Path to database or HDF5 file.\n        require_whole_years (bool): Flag for validating that the time vectors in the source contain data for complete years.\n        relative_loc (Path | str | None, optional): Path to HDF5 file relative to source. Defaults to None.\n        validate (bool, optional): Whether to validate vectors after loading. NB! Loads all data into memory at once. Defaults to True.\n\n    \"\"\"\n    super().__init__(source, require_whole_years, relative_loc)\n    self._index: TimeIndex = None\n    self._file_pointer = None\n\n    if validate:\n        self.validate_vectors()\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEH5TimeVectorLoader.clear_cache","title":"<code>clear_cache() -&gt; None</code>","text":"<p>Clear cached data.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def clear_cache(self) -&gt; None:\n    \"\"\"Clear cached data.\"\"\"\n    self._data = None\n    self._meta = None\n    self._index = None\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEH5TimeVectorLoader.get_index","title":"<code>get_index(vector_id: str) -&gt; TimeIndex</code>","text":"<p>Get the TimeIndex describing the time dimension of the vectors in the file.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Not used since all vectors in the NVE parquet files have the same index.</p> required <p>Returns:</p> Name Type Description <code>TimeIndex</code> <code>TimeIndex</code> <p>TimeIndex object describing the parquet file's index.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_index(self, vector_id: str) -&gt; TimeIndex:\n    \"\"\"\n    Get the TimeIndex describing the time dimension of the vectors in the file.\n\n    Args:\n        vector_id (str): Not used since all vectors in the NVE parquet files have the same index.\n\n    Returns:\n        TimeIndex: TimeIndex object describing the parquet file's index.\n\n    \"\"\"\n    if self._index is None:\n        meta = self.get_metadata(\"\")\n\n        if meta[TvMn.FREQUENCY] is None:\n            self._index = ListTimeIndex(\n                datetime_list=self._read_index(vector_id),\n                is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n                extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n                extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n            )\n            return self._index\n\n        start = pd.to_datetime(self._read_index(vector_id)[0]) if meta[TvMn.START] is None else meta[TvMn.START]\n        num_points = self._read_index(vector_id).size if meta[TvMn.NUM_POINTS] is None else meta[TvMn.NUM_POINTS]\n\n        self._index = FixedFrequencyTimeIndex(\n            start,\n            meta[TvMn.FREQUENCY],\n            num_points,\n            is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n            extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n            extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n        )\n\n    return self._index\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEH5TimeVectorLoader.get_metadata","title":"<code>get_metadata(vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]</code>","text":"<p>Retrieve and decodes custom metadata from parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Not used</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>If any of the expected metadata keys is not found in file.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, bool | int | str | datetime | timedelta | tzinfo | None]</code> <p>Dictionary with decoded metadata.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_metadata(self, vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]:\n    \"\"\"\n    Retrieve and decodes custom metadata from parquet file.\n\n    Args:\n        vector_id (str): Not used\n\n    Raises:\n        KeyError: If any of the expected metadata keys is not found in file.\n\n    Returns:\n        dict: Dictionary with decoded metadata.\n\n    \"\"\"\n    if self._meta is None:\n        errors = set()\n        meta = {}\n        with h5py.File(self.get_source(), mode=\"r\") as h5f:\n            meta_group = self._read_vector_field(h5f, H5Names.METADATA_GROUP, vector_id, h5py.Group)\n            for k, m in meta_group.items():\n                if isinstance(m, h5py.Dataset):\n                    meta[k] = m[()]\n                else:\n                    errors.add(f\"Improper metadata format: Metadata key {k} exists but is a h5 group when it should be a h5 dataset.\")\n        self._report_errors(errors)\n        self._meta = self._process_meta(meta)\n    return self._meta\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEH5TimeVectorLoader.get_values","title":"<code>get_values(vector_id: str) -&gt; NDArray</code>","text":"<p>Get numpy array with all the values of a given vector in the Loader's HDF5 file.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Unique id of the vector in the file.</p> required <p>Returns:</p> Name Type Description <code>NDArray</code> <code>NDArray</code> <p>Numpy array with values.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_values(self, vector_id: str) -&gt; NDArray:\n    \"\"\"\n    Get numpy array with all the values of a given vector in the Loader's HDF5 file.\n\n    Args:\n        vector_id (str): Unique id of the vector in the file.\n\n    Returns:\n        NDArray: Numpy array with values.\n\n    \"\"\"\n    if self._data is None:\n        self._data = dict()\n    if vector_id not in self._data:\n        with h5py.File(self.get_source(), mode=\"r\") as h5f:\n            self._data[vector_id] = self._read_vector_field(h5f, H5Names.VECTORS_GROUP, vector_id, field_type=h5py.Dataset, use_fallback=False)[()]\n    return self._data[vector_id]\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEParquetTimeVectorLoader","title":"<code>NVEParquetTimeVectorLoader</code>","text":"<p>               Bases: <code>NVETimeVectorLoader</code></p> <p>Class for loading time vector data from NVE parquet file sources.</p> <p>Meant for large time vectors. All vectors in the file must have the same lenghts and metadata. Supports format:     - 'Vertical' with one index collumn (DateTime) and the others containing vector values.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>class NVEParquetTimeVectorLoader(NVETimeVectorLoader):\n    \"\"\"\n    Class for loading time vector data from NVE parquet file sources.\n\n    Meant for large time vectors. All vectors in the file must have the same lenghts and metadata.\n    Supports format:\n        - 'Vertical' with one index collumn (DateTime) and the others containing vector values.\n\n    \"\"\"\n\n    _SUPPORTED_SUFFIXES: ClassVar[list] = [\".parquet\"]\n\n    def __init__(self, source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None:\n        \"\"\"\n        Intitialize loader instance and connect it to an Parquet file containing time vector data.\n\n        Args:\n            source (Path | str): Absolute Path to database or parquet file.\n            require_whole_years (bool): Flag for validating that the time vectors in the source contain data for complete years.\n            relative_loc (Path | str | None, optional): Path to parquet file relative to source. Defaults to None.\n            validate (bool, optional): Flag to turn on validation of timevectors. NB! Loads all data into memory at once. Defaults to True.\n\n        \"\"\"\n        super().__init__(source, require_whole_years, relative_loc)\n        self._index: TimeIndex = None\n        if validate:\n            self.validate_vectors()\n\n    def get_values(self, vector_id: str) -&gt; NDArray:\n        \"\"\"\n        Get numpy array with all the values of a given vector in the Loader's parquet file.\n\n        Args:\n            vector_id (str): Unique id of the vector in the file.\n\n        Returns:\n            NDArray: Numpy array with values.\n\n        \"\"\"\n        if self._data is None:\n            self._data = dict()\n        if vector_id not in self._data:\n            table = pq.read_table(self.get_source(), columns=[vector_id])\n            self._data[vector_id] = table[vector_id].to_numpy()\n        # if self._data is None:\n        #     self._data = pq.read_table(self.get_source())\n        return self._data[vector_id]  # .to_numpy()\n\n    def get_index(self, vector_id: str) -&gt; TimeIndex:  # Could be more types of indexes?\n        \"\"\"\n        Get the TimeIndex describing the time dimension of the vectors in the file.\n\n        Args:\n            vector_id (str): Not used since all vectors in the NVE parquet files have the same index.\n\n        Returns:\n            TimeIndex: TimeIndex object describing the parquet file's index.\n\n        \"\"\"\n        if self._index is None:\n            meta = self.get_metadata(\"\")\n\n            if TvMn.FREQUENCY not in meta or (TvMn.FREQUENCY in meta and meta[TvMn.FREQUENCY] is None):\n                datetime_index = pd.DatetimeIndex(\n                    pd.read_parquet(self.get_source(), columns=[TvMn.DATETIME_COL])[TvMn.DATETIME_COL],\n                    tz=meta[TvMn.TIMEZONE],\n                ).tolist()\n                self._index = ListTimeIndex(\n                    datetime_list=datetime_index,\n                    is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n                    extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n                    extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n                )\n                return self._index\n\n            parquet_file = None\n            if TvMn.START not in meta or (TvMn.START in meta and meta[TvMn.START] is None):\n                parquet_file = pq.ParquetFile(self.get_source())\n                start = pd.to_datetime(next(parquet_file.iter_batches(batch_size=1, columns=[TvMn.DATETIME_COL])))\n            else:\n                start = meta[TvMn.START]\n\n            if TvMn.NUM_POINTS not in meta or (TvMn.NUM_POINTS in meta and meta[TvMn.NUM_POINTS] is None):\n                if parquet_file is None:\n                    parquet_file = pq.ParquetFile(self.get_source())\n                num_points = parquet_file.metadata.num_rows\n            else:\n                num_points = meta[TvMn.NUM_POINTS]\n            self._index = FixedFrequencyTimeIndex(\n                start,\n                meta[TvMn.FREQUENCY],\n                num_points,\n                is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n                extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n                extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n            )\n\n        return self._index\n\n    def get_metadata(self, vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]:\n        \"\"\"\n        Retrieve and decodes custom metadata from parquet file.\n\n        Args:\n            vector_id (str): Not used\n\n        Raises:\n            KeyError: If any of the expected metadata keys is not found in file.\n\n        Returns:\n            dict: Dictionary with decoded metadata.\n\n        \"\"\"\n        if self._meta is None:\n            path = self.get_source()\n            raw_meta = pq.ParquetFile(path).schema_arrow.metadata\n\n            self._meta = self._process_meta(raw_meta)\n        return self._meta\n\n    def _get_ids(self) -&gt; list[str]:\n        parquet_file = pq.ParquetFile(self.get_source())\n        time_vector_ids: list[str] = parquet_file.schema_arrow.names\n        time_vector_ids.remove(TvMn.DATETIME_COL)\n        return time_vector_ids\n\n    def clear_cache(self) -&gt; None:\n        \"\"\"Clear cached data.\"\"\"\n        self._data = None\n        self._meta = None\n        self._index = None\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEParquetTimeVectorLoader.__init__","title":"<code>__init__(source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None</code>","text":"<p>Intitialize loader instance and connect it to an Parquet file containing time vector data.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Path | str</code> <p>Absolute Path to database or parquet file.</p> required <code>require_whole_years</code> <code>bool</code> <p>Flag for validating that the time vectors in the source contain data for complete years.</p> required <code>relative_loc</code> <code>Path | str | None</code> <p>Path to parquet file relative to source. Defaults to None.</p> <code>None</code> <code>validate</code> <code>bool</code> <p>Flag to turn on validation of timevectors. NB! Loads all data into memory at once. Defaults to True.</p> <code>True</code> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def __init__(self, source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None:\n    \"\"\"\n    Intitialize loader instance and connect it to an Parquet file containing time vector data.\n\n    Args:\n        source (Path | str): Absolute Path to database or parquet file.\n        require_whole_years (bool): Flag for validating that the time vectors in the source contain data for complete years.\n        relative_loc (Path | str | None, optional): Path to parquet file relative to source. Defaults to None.\n        validate (bool, optional): Flag to turn on validation of timevectors. NB! Loads all data into memory at once. Defaults to True.\n\n    \"\"\"\n    super().__init__(source, require_whole_years, relative_loc)\n    self._index: TimeIndex = None\n    if validate:\n        self.validate_vectors()\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEParquetTimeVectorLoader.clear_cache","title":"<code>clear_cache() -&gt; None</code>","text":"<p>Clear cached data.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def clear_cache(self) -&gt; None:\n    \"\"\"Clear cached data.\"\"\"\n    self._data = None\n    self._meta = None\n    self._index = None\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEParquetTimeVectorLoader.get_index","title":"<code>get_index(vector_id: str) -&gt; TimeIndex</code>","text":"<p>Get the TimeIndex describing the time dimension of the vectors in the file.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Not used since all vectors in the NVE parquet files have the same index.</p> required <p>Returns:</p> Name Type Description <code>TimeIndex</code> <code>TimeIndex</code> <p>TimeIndex object describing the parquet file's index.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_index(self, vector_id: str) -&gt; TimeIndex:  # Could be more types of indexes?\n    \"\"\"\n    Get the TimeIndex describing the time dimension of the vectors in the file.\n\n    Args:\n        vector_id (str): Not used since all vectors in the NVE parquet files have the same index.\n\n    Returns:\n        TimeIndex: TimeIndex object describing the parquet file's index.\n\n    \"\"\"\n    if self._index is None:\n        meta = self.get_metadata(\"\")\n\n        if TvMn.FREQUENCY not in meta or (TvMn.FREQUENCY in meta and meta[TvMn.FREQUENCY] is None):\n            datetime_index = pd.DatetimeIndex(\n                pd.read_parquet(self.get_source(), columns=[TvMn.DATETIME_COL])[TvMn.DATETIME_COL],\n                tz=meta[TvMn.TIMEZONE],\n            ).tolist()\n            self._index = ListTimeIndex(\n                datetime_list=datetime_index,\n                is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n                extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n                extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n            )\n            return self._index\n\n        parquet_file = None\n        if TvMn.START not in meta or (TvMn.START in meta and meta[TvMn.START] is None):\n            parquet_file = pq.ParquetFile(self.get_source())\n            start = pd.to_datetime(next(parquet_file.iter_batches(batch_size=1, columns=[TvMn.DATETIME_COL])))\n        else:\n            start = meta[TvMn.START]\n\n        if TvMn.NUM_POINTS not in meta or (TvMn.NUM_POINTS in meta and meta[TvMn.NUM_POINTS] is None):\n            if parquet_file is None:\n                parquet_file = pq.ParquetFile(self.get_source())\n            num_points = parquet_file.metadata.num_rows\n        else:\n            num_points = meta[TvMn.NUM_POINTS]\n        self._index = FixedFrequencyTimeIndex(\n            start,\n            meta[TvMn.FREQUENCY],\n            num_points,\n            is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n            extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n            extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n        )\n\n    return self._index\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEParquetTimeVectorLoader.get_metadata","title":"<code>get_metadata(vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]</code>","text":"<p>Retrieve and decodes custom metadata from parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Not used</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>If any of the expected metadata keys is not found in file.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, bool | int | str | datetime | timedelta | tzinfo | None]</code> <p>Dictionary with decoded metadata.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_metadata(self, vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]:\n    \"\"\"\n    Retrieve and decodes custom metadata from parquet file.\n\n    Args:\n        vector_id (str): Not used\n\n    Raises:\n        KeyError: If any of the expected metadata keys is not found in file.\n\n    Returns:\n        dict: Dictionary with decoded metadata.\n\n    \"\"\"\n    if self._meta is None:\n        path = self.get_source()\n        raw_meta = pq.ParquetFile(path).schema_arrow.metadata\n\n        self._meta = self._process_meta(raw_meta)\n    return self._meta\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEParquetTimeVectorLoader.get_values","title":"<code>get_values(vector_id: str) -&gt; NDArray</code>","text":"<p>Get numpy array with all the values of a given vector in the Loader's parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Unique id of the vector in the file.</p> required <p>Returns:</p> Name Type Description <code>NDArray</code> <code>NDArray</code> <p>Numpy array with values.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_values(self, vector_id: str) -&gt; NDArray:\n    \"\"\"\n    Get numpy array with all the values of a given vector in the Loader's parquet file.\n\n    Args:\n        vector_id (str): Unique id of the vector in the file.\n\n    Returns:\n        NDArray: Numpy array with values.\n\n    \"\"\"\n    if self._data is None:\n        self._data = dict()\n    if vector_id not in self._data:\n        table = pq.read_table(self.get_source(), columns=[vector_id])\n        self._data[vector_id] = table[vector_id].to_numpy()\n    # if self._data is None:\n    #     self._data = pq.read_table(self.get_source())\n    return self._data[vector_id]  # .to_numpy()\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEYamlTimeVectoroader","title":"<code>NVEYamlTimeVectoroader</code>","text":"<p>               Bases: <code>NVETimeVectorLoader</code></p> <p>Class for loading time vector data from NVE YAML file sources.</p> <p>Meant for very sparse time vector data, where the vectors have varying lengths and indexes. Currently all vectors must have the same metadata within each file. Supported format:     - Metadata: field containing dictionary with metadata for all vectors.     - Other fields are vector IDs with lists for x and y axes.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>class NVEYamlTimeVectoroader(NVETimeVectorLoader):\n    \"\"\"\n    Class for loading time vector data from NVE YAML file sources.\n\n    Meant for very sparse time vector data, where the vectors have varying lengths and indexes. Currently all vectors must have the same metadata within each\n    file.\n    Supported format:\n        - Metadata: field containing dictionary with metadata for all vectors.\n        - Other fields are vector IDs with lists for x and y axes.\n\n    \"\"\"\n\n    _SUPPORTED_SUFFIXES: ClassVar[list] = [\".yaml\", \".yml\"]\n\n    def __init__(self, source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None:\n        \"\"\"\n        Intitialize loader instance and connect it to an Yaml file containing time vector data.\n\n        Args:\n            source (Path | str): Absolute Path to database or excel file.\n            require_whole_years (bool): Flag for validating that the time vectors in the source contain data for complete years.\n            relative_loc (Path | str | None, optional): Path to excel file relative to source. Defaults to None.\n            validate (bool, optional): Flag to turn on validation of timevectors. NB! Loads all data into memory at once. Defaults to True.\n\n        \"\"\"\n        super().__init__(source, require_whole_years, relative_loc)\n        self._content_ids: list[str] = None\n\n        self._values_label: str = None\n        self._index_label: str = None\n\n        if validate:\n            self.validate_vectors()\n\n    def get_values(self, vector_id: str) -&gt; NDArray:\n        \"\"\"\n        Get values of vector.\n\n        Args:\n            vector_id (str): Unique id of the curve in the Loader source.\n\n        Returns:\n            NDArray: Numpy array with values of vector.\n\n        \"\"\"\n        if self._data is None:\n            self._parse_file()\n        values_list = self._data[vector_id][self._values_label]\n        if len(values_list) == 0:\n            message = f\"Time vector {vector_id} in {self} contains no points.\"\n            raise ValueError(message)\n        return np.asarray(values_list)\n\n    def get_index(self, vector_id: str) -&gt; TimeIndex:\n        \"\"\"\n        Get index of vector.\n\n        Args:\n            vector_id (str): Unique id of the curve in the Loader source.\n\n        Returns:\n            NDArray: Numpy array with index of vector.\n\n        \"\"\"\n        meta = self.get_metadata(\"\")\n        try:\n            datetime_list = [self._date_to_datetime(index_val) for index_val in self._data[vector_id][self._index_label]]\n        except ValueError as e:\n            message = f\"{self} got non date or none datetime values in index field of vector {vector_id}.\"\n            raise ValueError(message) from e\n\n        if len(datetime_list) == 0:\n            message = f\"Index of {vector_id} in {self} contains no points.\"\n            raise ValueError(message)\n        if len(datetime_list) == 1:\n            if not meta[TvMn.IS_ZERO_ONE_PROFILE]:\n                message = (\n                    f\"Index of {vector_id} in {self} contains a single point but is classified as an average index with a reference period.\"\n                    \" Can only create an extrapolatable index when it does not have a reference period.\"\n                )\n                raise ValueError(message)\n            # if not (meta[TvMn.EXTRAPOLATE_FISRT_POINT] and meta[TvMn.EXTRAPOLATE_LAST_POINT]):\n            #     message = (\n            #         f\"Index of {vector_id} in {self} contains a single point but is classified as not extrapolatable.\"\n            #         \" Can only create an extrapolatable index when it is both forward and backwards extrapolatable.\"\n            #     )\n            #     raise ValueError(message)\n            return ConstantTimeIndex()\n\n        return ListTimeIndex(\n            datetime_list=datetime_list,\n            is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n            extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n            extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n        )\n\n    def get_metadata(self, vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]:\n        \"\"\"\n        Read YAML file metadata.\n\n        Args:\n            vector_id (str): Not used.\n\n        Raises:\n            KeyError: If an expected metadata key is missing.\n\n        Returns:\n            dict[str, bool|int|str|datetime|timedelta|tzinfo|None]: Metadata dictionary.\n\n        \"\"\"\n        if self._meta is None:\n            raw_meta = self._data[YamlNames.metadata_field][YamlNames.x_field]\n\n            self._meta = self._process_meta(raw_meta)\n        return self._meta\n\n    def _get_ids(self) -&gt; list[str]:\n        if self._content_ids is None:\n            if self._data is None:\n                self._parse_file()\n            ids_list = list(self._data.keys())\n            ids_list.remove(YamlNames.metadata_field)\n            self._content_ids = ids_list\n        return self._content_ids\n\n    def _parse_file(self) -&gt; None:\n        with self.get_source().open(encoding=YamlNames.encoding) as f:\n            d = yaml.safe_load(f)\n            self._x_meta = d[YamlNames.metadata_field][YamlNames.x_field]\n            self._y_meta = d[YamlNames.metadata_field][YamlNames.y_field]\n\n            self._values_label = self._x_meta[YamlNames.attribute]\n            self._index_label = self._y_meta[YamlNames.attribute]\n\n            self._data = d\n\n    def _date_to_datetime(self, value: date | datetime) -&gt; datetime:\n        if isinstance(value, date):\n            value = datetime(value.year, value.month, value.day)\n        elif not isinstance(value, datetime):\n            message = \"Value must be date or datetime.\"\n            raise ValueError(message)\n        return value\n\n    def clear_cache(self) -&gt; None:\n        \"\"\"Clear cached data.\"\"\"\n        self._data = None\n        self._meta = None\n\n        self._content_ids = None\n\n        self._values_label = None\n        self._index_label = None\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEYamlTimeVectoroader.__init__","title":"<code>__init__(source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None</code>","text":"<p>Intitialize loader instance and connect it to an Yaml file containing time vector data.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Path | str</code> <p>Absolute Path to database or excel file.</p> required <code>require_whole_years</code> <code>bool</code> <p>Flag for validating that the time vectors in the source contain data for complete years.</p> required <code>relative_loc</code> <code>Path | str | None</code> <p>Path to excel file relative to source. Defaults to None.</p> <code>None</code> <code>validate</code> <code>bool</code> <p>Flag to turn on validation of timevectors. NB! Loads all data into memory at once. Defaults to True.</p> <code>True</code> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def __init__(self, source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None:\n    \"\"\"\n    Intitialize loader instance and connect it to an Yaml file containing time vector data.\n\n    Args:\n        source (Path | str): Absolute Path to database or excel file.\n        require_whole_years (bool): Flag for validating that the time vectors in the source contain data for complete years.\n        relative_loc (Path | str | None, optional): Path to excel file relative to source. Defaults to None.\n        validate (bool, optional): Flag to turn on validation of timevectors. NB! Loads all data into memory at once. Defaults to True.\n\n    \"\"\"\n    super().__init__(source, require_whole_years, relative_loc)\n    self._content_ids: list[str] = None\n\n    self._values_label: str = None\n    self._index_label: str = None\n\n    if validate:\n        self.validate_vectors()\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEYamlTimeVectoroader.clear_cache","title":"<code>clear_cache() -&gt; None</code>","text":"<p>Clear cached data.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def clear_cache(self) -&gt; None:\n    \"\"\"Clear cached data.\"\"\"\n    self._data = None\n    self._meta = None\n\n    self._content_ids = None\n\n    self._values_label = None\n    self._index_label = None\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEYamlTimeVectoroader.get_index","title":"<code>get_index(vector_id: str) -&gt; TimeIndex</code>","text":"<p>Get index of vector.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Unique id of the curve in the Loader source.</p> required <p>Returns:</p> Name Type Description <code>NDArray</code> <code>TimeIndex</code> <p>Numpy array with index of vector.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_index(self, vector_id: str) -&gt; TimeIndex:\n    \"\"\"\n    Get index of vector.\n\n    Args:\n        vector_id (str): Unique id of the curve in the Loader source.\n\n    Returns:\n        NDArray: Numpy array with index of vector.\n\n    \"\"\"\n    meta = self.get_metadata(\"\")\n    try:\n        datetime_list = [self._date_to_datetime(index_val) for index_val in self._data[vector_id][self._index_label]]\n    except ValueError as e:\n        message = f\"{self} got non date or none datetime values in index field of vector {vector_id}.\"\n        raise ValueError(message) from e\n\n    if len(datetime_list) == 0:\n        message = f\"Index of {vector_id} in {self} contains no points.\"\n        raise ValueError(message)\n    if len(datetime_list) == 1:\n        if not meta[TvMn.IS_ZERO_ONE_PROFILE]:\n            message = (\n                f\"Index of {vector_id} in {self} contains a single point but is classified as an average index with a reference period.\"\n                \" Can only create an extrapolatable index when it does not have a reference period.\"\n            )\n            raise ValueError(message)\n        # if not (meta[TvMn.EXTRAPOLATE_FISRT_POINT] and meta[TvMn.EXTRAPOLATE_LAST_POINT]):\n        #     message = (\n        #         f\"Index of {vector_id} in {self} contains a single point but is classified as not extrapolatable.\"\n        #         \" Can only create an extrapolatable index when it is both forward and backwards extrapolatable.\"\n        #     )\n        #     raise ValueError(message)\n        return ConstantTimeIndex()\n\n    return ListTimeIndex(\n        datetime_list=datetime_list,\n        is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n        extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n        extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n    )\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEYamlTimeVectoroader.get_metadata","title":"<code>get_metadata(vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]</code>","text":"<p>Read YAML file metadata.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Not used.</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>If an expected metadata key is missing.</p> <p>Returns:</p> Type Description <code>dict[str, bool | int | str | datetime | timedelta | tzinfo | None]</code> <p>dict[str, bool|int|str|datetime|timedelta|tzinfo|None]: Metadata dictionary.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_metadata(self, vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]:\n    \"\"\"\n    Read YAML file metadata.\n\n    Args:\n        vector_id (str): Not used.\n\n    Raises:\n        KeyError: If an expected metadata key is missing.\n\n    Returns:\n        dict[str, bool|int|str|datetime|timedelta|tzinfo|None]: Metadata dictionary.\n\n    \"\"\"\n    if self._meta is None:\n        raw_meta = self._data[YamlNames.metadata_field][YamlNames.x_field]\n\n        self._meta = self._process_meta(raw_meta)\n    return self._meta\n</code></pre>"},{"location":"reference/#framdata.loaders.NVEYamlTimeVectoroader.get_values","title":"<code>get_values(vector_id: str) -&gt; NDArray</code>","text":"<p>Get values of vector.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Unique id of the curve in the Loader source.</p> required <p>Returns:</p> Name Type Description <code>NDArray</code> <code>NDArray</code> <p>Numpy array with values of vector.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_values(self, vector_id: str) -&gt; NDArray:\n    \"\"\"\n    Get values of vector.\n\n    Args:\n        vector_id (str): Unique id of the curve in the Loader source.\n\n    Returns:\n        NDArray: Numpy array with values of vector.\n\n    \"\"\"\n    if self._data is None:\n        self._parse_file()\n    values_list = self._data[vector_id][self._values_label]\n    if len(values_list) == 0:\n        message = f\"Time vector {vector_id} in {self} contains no points.\"\n        raise ValueError(message)\n    return np.asarray(values_list)\n</code></pre>"},{"location":"reference/#framdata.loaders.NVETimeVectorLoader","title":"<code>NVETimeVectorLoader</code>","text":"<p>Loader for NVE time vector data.</p> <p>This module provides the NVETimeVectorLoader class, which extends FileLoader and TimeVectorLoader to handle metadata and validation for time vector data from NVE parquet files.</p>"},{"location":"reference/#framdata.loaders.NVETimeVectorLoader.NVETimeVectorLoader","title":"<code>NVETimeVectorLoader</code>","text":"<p>               Bases: <code>FileLoader</code>, <code>TimeVectorLoader</code></p> <p>Common interface for metadata in NVE TimeVectorLoaders.</p> Source code in <code>framdata/loaders/NVETimeVectorLoader.py</code> <pre><code>class NVETimeVectorLoader(FileLoader, TimeVectorLoader):\n    \"\"\"Common interface for metadata in NVE TimeVectorLoaders.\"\"\"\n\n    def __init__(self, source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None) -&gt; None:\n        \"\"\"\n        Initialize NVETimeVectorLoader with source and optional relative location.\n\n        Args:\n            source (Path | str): Path or string to the source file.\n            require_whole_years (bool): Flag for validating that the time vectors in the source contain data for complete years.\n            relative_loc (Path | str | None, optional): Relative location, defaults to None.\n\n        \"\"\"\n        super().__init__(source, relative_loc)\n\n        self._data: dict[str, NDArray] = None\n        self._meta: dict[str, bool | int | str | datetime | timedelta | tzinfo] = None\n\n        self._require_whole_years = require_whole_years\n\n    def is_max_level(self, vector_id: str) -&gt; bool | None:\n        \"\"\"\n        Check if the time vector is classified as a max level vector.\n\n        Args:\n            vector_id (str): ID of the time vector.\n\n        Returns:\n            bool | None: True if max level, False otherwise, or None if not specified.\n\n        \"\"\"\n        return self.get_metadata(vector_id)[TvMn.IS_MAX_LEVEL]\n\n    def is_zero_one_profile(self, vector_id: str) -&gt; bool | None:\n        \"\"\"\n        Check if the time vector is classified as a zero-one profile vector.\n\n        Args:\n            vector_id (str): ID of the time vector.\n\n        Returns:\n            bool | None: True if zero-one profile, False otherwise, or None if not specified.\n\n        \"\"\"\n        return self.get_metadata(vector_id)[TvMn.IS_ZERO_ONE_PROFILE]\n\n    def get_unit(self, vector_id: str) -&gt; str:\n        \"\"\"\n        Get the unit of the given time vector.\n\n        Args:\n            vector_id (str): ID of a time vector. Not used since all time vectors in the NVE parquet files have the same\n                             unit.\n\n        Returns:\n            str: Unit of the time vector.\n\n        \"\"\"\n        return self.get_metadata(vector_id)[TvMn.UNIT]\n\n    def get_reference_period(self, vector_id: str) -&gt; ReferencePeriod | None:\n        \"\"\"\n        Get Reference perod from metadata.\n\n        Args:\n            vector_id (str): Not used.\n\n        Raises:\n            ValueError: If only one of start year or number of years are set in metadata.\n\n        Returns:\n            ReferencePeriod | None\n\n        \"\"\"\n        start_year = self.get_metadata(vector_id)[TvMn.REF_PERIOD_START_YEAR]\n        num_years = self.get_metadata(vector_id)[TvMn.REF_PERIOD_NUM_YEARS]\n\n        ref_period = None\n        if start_year and num_years:\n            ref_period = ReferencePeriod(start_year=start_year, num_years=num_years)\n        elif start_year or num_years:\n            message = (\n                f\"{self}: Both {TvMn.REF_PERIOD_START_YEAR} and {TvMn.REF_PERIOD_NUM_YEARS} must be provided for a valid reference period.\"\n                \"Alternatively, both must be None for undefined reference period.\"\n            )\n            raise ValueError(message)\n        return ref_period\n\n    def validate_vectors(self) -&gt; None:\n        \"\"\"\n        Validate data in all vectors contained in the Loader.\n\n        Conditions validated:\n            - If vector contains negative values.\n            (- If vector is a zero one profile and contains values outside the unit interval.) * not in use currently\n\n        Raises:\n            ValueError: When conditions are violated.\n\n        \"\"\"\n        errors = set()\n        for vector_id in self.get_ids():\n            errors |= self._validate_vector(vector_id)\n\n        if errors:\n            message = f\"Found errors in {self}:\"\n            for e in errors:\n                message += f\"\\n - {e}.\"\n\n            raise ValueError(message)\n\n    def _process_meta(self, raw_meta: dict[str | bytes, str | bytes | int | bool | None]) -&gt; dict[str, Any]:\n        processed_meta, missing_keys = TvMn.cast_meta(raw_meta)\n\n        optional_keys = {TvMn.ID_COLUMN_NAME, TvMn.FREQUENCY, TvMn.NUM_POINTS, TvMn.START}\n        missing_keys -= optional_keys\n\n        if missing_keys:\n            msg = f\"{self} could not find keys: {missing_keys} in metadata of file {self.get_source()}. Metadata: {processed_meta}\"\n            raise KeyError(msg)\n\n        return processed_meta\n\n    def _validate_vector(self, vector_id: str) -&gt; set[str]:\n        index = self.get_index(vector_id)\n        values = self.get_values(vector_id)\n\n        errors = set()\n\n        # validate index length\n        if index.get_num_periods() not in range(values.size - 1, values.size + 1):  # Since ListTimeIndex objects' num_periods can vary.\n            errors.add(f\"{vector_id} - {type(index)} with {index.get_num_periods()} periods and vector with size ({values.size}) do not match.\")\n\n        # validate negative and missing values\n        negatives = values &lt; 0\n        if np.any(negatives):\n            errors.add(f\"{vector_id} contains {negatives.sum()} negative values.\")\n        nans = np.isnan(values)\n        if np.any(nans):\n            errors.add(f\"{vector_id} contains {nans.sum()} nan values.\")\n\n        # validate that index is whole years if required\n        if self._require_whole_years and not index.is_whole_years():\n            errors.add(f\"{vector_id} is required to contain whole years but its index ({index}) is not classified as is_whole_years.\")\n\n        # outside_unit_interval = ((0 &lt;= values) &amp; (values &lt;= 1))\n        # if self.is_zero_one_profile(vector_id) and outside_unit_interval.any():\n        #     num_outside_range = outside_unit_interval.sum()\n        #     errors.add(f\"{vector_id} is classified as a zero one vector but contains {num_outside_range} values outside the range 0, 1.\")\n\n        # if not self.is_zero_one_profile(vector_id):\n        #     ref_period = self.get_reference_period(vector_id)\n        #     ref_start_date = ref_period.get_start_year()\n\n        #     index = self.get_index(vector_id)\n\n        return errors\n</code></pre>"},{"location":"reference/#framdata.loaders.NVETimeVectorLoader.NVETimeVectorLoader.__init__","title":"<code>__init__(source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None) -&gt; None</code>","text":"<p>Initialize NVETimeVectorLoader with source and optional relative location.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Path | str</code> <p>Path or string to the source file.</p> required <code>require_whole_years</code> <code>bool</code> <p>Flag for validating that the time vectors in the source contain data for complete years.</p> required <code>relative_loc</code> <code>Path | str | None</code> <p>Relative location, defaults to None.</p> <code>None</code> Source code in <code>framdata/loaders/NVETimeVectorLoader.py</code> <pre><code>def __init__(self, source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None) -&gt; None:\n    \"\"\"\n    Initialize NVETimeVectorLoader with source and optional relative location.\n\n    Args:\n        source (Path | str): Path or string to the source file.\n        require_whole_years (bool): Flag for validating that the time vectors in the source contain data for complete years.\n        relative_loc (Path | str | None, optional): Relative location, defaults to None.\n\n    \"\"\"\n    super().__init__(source, relative_loc)\n\n    self._data: dict[str, NDArray] = None\n    self._meta: dict[str, bool | int | str | datetime | timedelta | tzinfo] = None\n\n    self._require_whole_years = require_whole_years\n</code></pre>"},{"location":"reference/#framdata.loaders.NVETimeVectorLoader.NVETimeVectorLoader.get_reference_period","title":"<code>get_reference_period(vector_id: str) -&gt; ReferencePeriod | None</code>","text":"<p>Get Reference perod from metadata.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Not used.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If only one of start year or number of years are set in metadata.</p> <p>Returns:</p> Type Description <code>ReferencePeriod | None</code> <p>ReferencePeriod | None</p> Source code in <code>framdata/loaders/NVETimeVectorLoader.py</code> <pre><code>def get_reference_period(self, vector_id: str) -&gt; ReferencePeriod | None:\n    \"\"\"\n    Get Reference perod from metadata.\n\n    Args:\n        vector_id (str): Not used.\n\n    Raises:\n        ValueError: If only one of start year or number of years are set in metadata.\n\n    Returns:\n        ReferencePeriod | None\n\n    \"\"\"\n    start_year = self.get_metadata(vector_id)[TvMn.REF_PERIOD_START_YEAR]\n    num_years = self.get_metadata(vector_id)[TvMn.REF_PERIOD_NUM_YEARS]\n\n    ref_period = None\n    if start_year and num_years:\n        ref_period = ReferencePeriod(start_year=start_year, num_years=num_years)\n    elif start_year or num_years:\n        message = (\n            f\"{self}: Both {TvMn.REF_PERIOD_START_YEAR} and {TvMn.REF_PERIOD_NUM_YEARS} must be provided for a valid reference period.\"\n            \"Alternatively, both must be None for undefined reference period.\"\n        )\n        raise ValueError(message)\n    return ref_period\n</code></pre>"},{"location":"reference/#framdata.loaders.NVETimeVectorLoader.NVETimeVectorLoader.get_unit","title":"<code>get_unit(vector_id: str) -&gt; str</code>","text":"<p>Get the unit of the given time vector.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>ID of a time vector. Not used since all time vectors in the NVE parquet files have the same              unit.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Unit of the time vector.</p> Source code in <code>framdata/loaders/NVETimeVectorLoader.py</code> <pre><code>def get_unit(self, vector_id: str) -&gt; str:\n    \"\"\"\n    Get the unit of the given time vector.\n\n    Args:\n        vector_id (str): ID of a time vector. Not used since all time vectors in the NVE parquet files have the same\n                         unit.\n\n    Returns:\n        str: Unit of the time vector.\n\n    \"\"\"\n    return self.get_metadata(vector_id)[TvMn.UNIT]\n</code></pre>"},{"location":"reference/#framdata.loaders.NVETimeVectorLoader.NVETimeVectorLoader.is_max_level","title":"<code>is_max_level(vector_id: str) -&gt; bool | None</code>","text":"<p>Check if the time vector is classified as a max level vector.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>ID of the time vector.</p> required <p>Returns:</p> Type Description <code>bool | None</code> <p>bool | None: True if max level, False otherwise, or None if not specified.</p> Source code in <code>framdata/loaders/NVETimeVectorLoader.py</code> <pre><code>def is_max_level(self, vector_id: str) -&gt; bool | None:\n    \"\"\"\n    Check if the time vector is classified as a max level vector.\n\n    Args:\n        vector_id (str): ID of the time vector.\n\n    Returns:\n        bool | None: True if max level, False otherwise, or None if not specified.\n\n    \"\"\"\n    return self.get_metadata(vector_id)[TvMn.IS_MAX_LEVEL]\n</code></pre>"},{"location":"reference/#framdata.loaders.NVETimeVectorLoader.NVETimeVectorLoader.is_zero_one_profile","title":"<code>is_zero_one_profile(vector_id: str) -&gt; bool | None</code>","text":"<p>Check if the time vector is classified as a zero-one profile vector.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>ID of the time vector.</p> required <p>Returns:</p> Type Description <code>bool | None</code> <p>bool | None: True if zero-one profile, False otherwise, or None if not specified.</p> Source code in <code>framdata/loaders/NVETimeVectorLoader.py</code> <pre><code>def is_zero_one_profile(self, vector_id: str) -&gt; bool | None:\n    \"\"\"\n    Check if the time vector is classified as a zero-one profile vector.\n\n    Args:\n        vector_id (str): ID of the time vector.\n\n    Returns:\n        bool | None: True if zero-one profile, False otherwise, or None if not specified.\n\n    \"\"\"\n    return self.get_metadata(vector_id)[TvMn.IS_ZERO_ONE_PROFILE]\n</code></pre>"},{"location":"reference/#framdata.loaders.NVETimeVectorLoader.NVETimeVectorLoader.validate_vectors","title":"<code>validate_vectors() -&gt; None</code>","text":"<p>Validate data in all vectors contained in the Loader.</p> Conditions validated <ul> <li>If vector contains negative values. (- If vector is a zero one profile and contains values outside the unit interval.) * not in use currently</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>When conditions are violated.</p> Source code in <code>framdata/loaders/NVETimeVectorLoader.py</code> <pre><code>def validate_vectors(self) -&gt; None:\n    \"\"\"\n    Validate data in all vectors contained in the Loader.\n\n    Conditions validated:\n        - If vector contains negative values.\n        (- If vector is a zero one profile and contains values outside the unit interval.) * not in use currently\n\n    Raises:\n        ValueError: When conditions are violated.\n\n    \"\"\"\n    errors = set()\n    for vector_id in self.get_ids():\n        errors |= self._validate_vector(vector_id)\n\n    if errors:\n        message = f\"Found errors in {self}:\"\n        for e in errors:\n            message += f\"\\n - {e}.\"\n\n        raise ValueError(message)\n</code></pre>"},{"location":"reference/#framdata.loaders.curve_loaders","title":"<code>curve_loaders</code>","text":"<p>Contains class for loading Curve data from NVE yaml files.</p>"},{"location":"reference/#framdata.loaders.curve_loaders.NVEYamlCurveLoader","title":"<code>NVEYamlCurveLoader</code>","text":"<p>               Bases: <code>FileLoader</code>, <code>CurveLoader</code></p> <p>Handle reading of Curve data from a yaml File of NVE specific format.</p> Source code in <code>framdata/loaders/curve_loaders.py</code> <pre><code>class NVEYamlCurveLoader(FileLoader, CurveLoader):\n    \"\"\"Handle reading of Curve data from a yaml File of NVE specific format.\"\"\"\n\n    _SUPPORTED_SUFFIXES: ClassVar[list[str]] = [\".yaml\", \".yml\"]\n\n    def __init__(self, source: Path | str, relative_loc: Path | str | None = None) -&gt; None:\n        \"\"\"\n        Handle reading of curves from a single yaml file.\n\n        Args:\n            source (Path | str): Absolute Path to database or yaml file path.\n            relative_loc (Optional[Union[Path, str]], optional): Path to yaml file relative to source. Defaults to None.\n\n        \"\"\"\n        super().__init__(source, relative_loc)\n\n        self._data = None\n        self._x_meta: str = None\n        self._y_meta: str = None\n\n        self._x_label: str = None\n        self._y_label: str = None\n\n    def get_x_axis(self, curve_id: str) -&gt; NDArray:\n        \"\"\"\n        Get values of x axis.\n\n        Args:\n            curve_id (str): Unique id of the curve in the Loader source.\n\n        Returns:\n            NDArray: Numpy array with values of x axis.\n\n        \"\"\"\n        if self._data is None:\n            self._parse_file()\n        return np.asarray(self._data[curve_id][self._x_label])\n\n    def get_y_axis(self, curve_id: str) -&gt; NDArray:\n        \"\"\"\n        Get values of y axis.\n\n        Args:\n            curve_id (str): Unique id of the curve in the Loader source.\n\n        Returns:\n            NDArray: Numpy array with values of y axis.\n\n        \"\"\"\n        if self._data is None:\n            self._parse_file()\n        return np.asarray(self._data[curve_id][self._y_label])\n\n    def get_x_unit(self, curve_id: str) -&gt; str:\n        \"\"\"\n        Get the unit of the x axis for the specified curve.\n\n        Args:\n            curve_id (str): Unique id of the curve in the Loader source.\n\n        Returns:\n            str: Unit of the x axis.\n\n        \"\"\"\n        if self._data is None:\n            self._parse_file()\n        return self._x_meta[YamlNames.unit]\n\n    def get_y_unit(self, curve_id: str) -&gt; str:\n        \"\"\"\n        Get the unit of the y axis for the specified curve.\n\n        Args:\n            curve_id (str): Unique id of the curve in the Loader source.\n\n        Returns:\n            str: Unit of the y axis.\n\n        \"\"\"\n        if self._data is None:\n            self._parse_file()\n        return self._y_meta[YamlNames.unit]\n\n    def get_metadata(self, content_id: str) -&gt; dict:\n        \"\"\"\n        Retrieve metadata for the specified content ID.\n\n        Args:\n            content_id (str): Unique identifier for the content.\n\n        Returns:\n            dict: Metadata associated with the content.\n\n        \"\"\"\n        if self._data is None:\n            self._parse_file()\n        return self._data[YamlNames.metadata_field]\n\n    def _get_ids(self) -&gt; list[str]:\n        if self._content_ids is None:\n            if self._data is None:\n                self._parse_file()\n            ids_list = list(self._data.keys())\n            ids_list.remove(YamlNames.metadata_field)\n            self._content_ids = ids_list\n        return self._content_ids\n\n    def _parse_file(self) -&gt; None:\n        with self.get_source().open(encoding=YamlNames.encoding) as f:\n            d = yaml.safe_load(f)\n            self._x_meta = d[YamlNames.metadata_field][YamlNames.x_field]\n            self._y_meta = d[YamlNames.metadata_field][YamlNames.y_field]\n\n            self._x_label = self._x_meta[YamlNames.attribute]\n            self._y_label = self._y_meta[YamlNames.attribute]\n\n            self._data = d\n\n    def clear_cache(self) -&gt; None:\n        \"\"\"Clear cached data.\"\"\"\n        self._data = None\n        self._x_meta = None\n        self._y_meta = None\n\n        self._x_label = None\n        self._y_label = None\n</code></pre>"},{"location":"reference/#framdata.loaders.curve_loaders.NVEYamlCurveLoader.__init__","title":"<code>__init__(source: Path | str, relative_loc: Path | str | None = None) -&gt; None</code>","text":"<p>Handle reading of curves from a single yaml file.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Path | str</code> <p>Absolute Path to database or yaml file path.</p> required <code>relative_loc</code> <code>Optional[Union[Path, str]]</code> <p>Path to yaml file relative to source. Defaults to None.</p> <code>None</code> Source code in <code>framdata/loaders/curve_loaders.py</code> <pre><code>def __init__(self, source: Path | str, relative_loc: Path | str | None = None) -&gt; None:\n    \"\"\"\n    Handle reading of curves from a single yaml file.\n\n    Args:\n        source (Path | str): Absolute Path to database or yaml file path.\n        relative_loc (Optional[Union[Path, str]], optional): Path to yaml file relative to source. Defaults to None.\n\n    \"\"\"\n    super().__init__(source, relative_loc)\n\n    self._data = None\n    self._x_meta: str = None\n    self._y_meta: str = None\n\n    self._x_label: str = None\n    self._y_label: str = None\n</code></pre>"},{"location":"reference/#framdata.loaders.curve_loaders.NVEYamlCurveLoader.clear_cache","title":"<code>clear_cache() -&gt; None</code>","text":"<p>Clear cached data.</p> Source code in <code>framdata/loaders/curve_loaders.py</code> <pre><code>def clear_cache(self) -&gt; None:\n    \"\"\"Clear cached data.\"\"\"\n    self._data = None\n    self._x_meta = None\n    self._y_meta = None\n\n    self._x_label = None\n    self._y_label = None\n</code></pre>"},{"location":"reference/#framdata.loaders.curve_loaders.NVEYamlCurveLoader.get_metadata","title":"<code>get_metadata(content_id: str) -&gt; dict</code>","text":"<p>Retrieve metadata for the specified content ID.</p> <p>Parameters:</p> Name Type Description Default <code>content_id</code> <code>str</code> <p>Unique identifier for the content.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Metadata associated with the content.</p> Source code in <code>framdata/loaders/curve_loaders.py</code> <pre><code>def get_metadata(self, content_id: str) -&gt; dict:\n    \"\"\"\n    Retrieve metadata for the specified content ID.\n\n    Args:\n        content_id (str): Unique identifier for the content.\n\n    Returns:\n        dict: Metadata associated with the content.\n\n    \"\"\"\n    if self._data is None:\n        self._parse_file()\n    return self._data[YamlNames.metadata_field]\n</code></pre>"},{"location":"reference/#framdata.loaders.curve_loaders.NVEYamlCurveLoader.get_x_axis","title":"<code>get_x_axis(curve_id: str) -&gt; NDArray</code>","text":"<p>Get values of x axis.</p> <p>Parameters:</p> Name Type Description Default <code>curve_id</code> <code>str</code> <p>Unique id of the curve in the Loader source.</p> required <p>Returns:</p> Name Type Description <code>NDArray</code> <code>NDArray</code> <p>Numpy array with values of x axis.</p> Source code in <code>framdata/loaders/curve_loaders.py</code> <pre><code>def get_x_axis(self, curve_id: str) -&gt; NDArray:\n    \"\"\"\n    Get values of x axis.\n\n    Args:\n        curve_id (str): Unique id of the curve in the Loader source.\n\n    Returns:\n        NDArray: Numpy array with values of x axis.\n\n    \"\"\"\n    if self._data is None:\n        self._parse_file()\n    return np.asarray(self._data[curve_id][self._x_label])\n</code></pre>"},{"location":"reference/#framdata.loaders.curve_loaders.NVEYamlCurveLoader.get_x_unit","title":"<code>get_x_unit(curve_id: str) -&gt; str</code>","text":"<p>Get the unit of the x axis for the specified curve.</p> <p>Parameters:</p> Name Type Description Default <code>curve_id</code> <code>str</code> <p>Unique id of the curve in the Loader source.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Unit of the x axis.</p> Source code in <code>framdata/loaders/curve_loaders.py</code> <pre><code>def get_x_unit(self, curve_id: str) -&gt; str:\n    \"\"\"\n    Get the unit of the x axis for the specified curve.\n\n    Args:\n        curve_id (str): Unique id of the curve in the Loader source.\n\n    Returns:\n        str: Unit of the x axis.\n\n    \"\"\"\n    if self._data is None:\n        self._parse_file()\n    return self._x_meta[YamlNames.unit]\n</code></pre>"},{"location":"reference/#framdata.loaders.curve_loaders.NVEYamlCurveLoader.get_y_axis","title":"<code>get_y_axis(curve_id: str) -&gt; NDArray</code>","text":"<p>Get values of y axis.</p> <p>Parameters:</p> Name Type Description Default <code>curve_id</code> <code>str</code> <p>Unique id of the curve in the Loader source.</p> required <p>Returns:</p> Name Type Description <code>NDArray</code> <code>NDArray</code> <p>Numpy array with values of y axis.</p> Source code in <code>framdata/loaders/curve_loaders.py</code> <pre><code>def get_y_axis(self, curve_id: str) -&gt; NDArray:\n    \"\"\"\n    Get values of y axis.\n\n    Args:\n        curve_id (str): Unique id of the curve in the Loader source.\n\n    Returns:\n        NDArray: Numpy array with values of y axis.\n\n    \"\"\"\n    if self._data is None:\n        self._parse_file()\n    return np.asarray(self._data[curve_id][self._y_label])\n</code></pre>"},{"location":"reference/#framdata.loaders.curve_loaders.NVEYamlCurveLoader.get_y_unit","title":"<code>get_y_unit(curve_id: str) -&gt; str</code>","text":"<p>Get the unit of the y axis for the specified curve.</p> <p>Parameters:</p> Name Type Description Default <code>curve_id</code> <code>str</code> <p>Unique id of the curve in the Loader source.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Unit of the y axis.</p> Source code in <code>framdata/loaders/curve_loaders.py</code> <pre><code>def get_y_unit(self, curve_id: str) -&gt; str:\n    \"\"\"\n    Get the unit of the y axis for the specified curve.\n\n    Args:\n        curve_id (str): Unique id of the curve in the Loader source.\n\n    Returns:\n        str: Unit of the y axis.\n\n    \"\"\"\n    if self._data is None:\n        self._parse_file()\n    return self._y_meta[YamlNames.unit]\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders","title":"<code>time_vector_loaders</code>","text":"<p>Contain classes for reading time vector data from various file types with formats specific to NVE.</p> This module provides <ul> <li>NVEExcelTimeVectorLoader: Handle time vectors in excel files.</li> <li>NVEH5TimeVectorLoader: Handle time vectors in HDF5 files.</li> <li>NVEYamlTimeVectorLoader: Handle time vectors in Yaml files.</li> <li>NVEParquetTieVectorLoader: Handle time vectors in Parquet files.</li> </ul>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEExcelTimeVectorLoader","title":"<code>NVEExcelTimeVectorLoader</code>","text":"<p>               Bases: <code>NVETimeVectorLoader</code></p> <p>Class for loading time vector data from NVE excel file sources.</p> <p>Meant for short time vectors (e.g. yearly volumes or installed capacities) which are desireable to view and edit easily through Excel. Supports the followinf formats:     - 'Horizontal': One column containing IDs, the other column names represents the index. Vector values as rows     - 'Vertical': One column as index (DateTime), the oher columns names are vector IDs. Vectors as column values.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>class NVEExcelTimeVectorLoader(NVETimeVectorLoader):\n    \"\"\"\n    Class for loading time vector data from NVE excel file sources.\n\n    Meant for short time vectors (e.g. yearly volumes or installed capacities) which are desireable to view and edit easily through Excel.\n    Supports the followinf formats:\n        - 'Horizontal': One column containing IDs, the other column names represents the index. Vector values as rows\n        - 'Vertical': One column as index (DateTime), the oher columns names are vector IDs. Vectors as column values.\n\n    \"\"\"\n\n    _SUPPORTED_SUFFIXES: ClassVar[list] = [\".xlsx\"]\n    _DATA_SHEET = \"Data\"\n    _METADATA_SHEET = \"Metadata\"\n\n    def __init__(self, source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None:\n        \"\"\"\n        Intitialize loader instance and connect it to an Excel file containing time vector data.\n\n        Args:\n            source (Path | str): Absolute Path to database or excel file.\n            require_whole_years (bool): Flag for validating that the time vectors in the source contain data for complete years.\n            relative_loc (Path | str | None, optional): Path to excel file relative to source. Defaults to None.\n            validate (bool, optional): Flag to turn on validation of timevectors. NB! Loads all data into memory at once. Defaults to True.\n\n        \"\"\"\n        super().__init__(source, require_whole_years, relative_loc)\n        self._index: TimeIndex = None\n\n        if validate:\n            self.validate_vectors()\n\n    def get_unit(self, vector_id: str) -&gt; str:\n        \"\"\"\n        Get the unit of the given time vector.\n\n        Args:\n            vector_id (str): ID of a time vector. Not used since all time vectors in the NVE excel files have the same\n                             unit.\n\n        Returns:\n            str: Unit of the time vector.\n\n        \"\"\"\n        return self.get_metadata(\"\")[TvMn.UNIT]\n\n    def get_values(self, vector_id: str) -&gt; NDArray:\n        \"\"\"\n        Get numpy array with all the values of a given vector in the Loader's excel file.\n\n        Args:\n            vector_id (str): Unique id of the vector in the file.\n\n        Returns:\n            NDArray: Numpy array with values.\n\n        \"\"\"\n        if self._data is None:\n            self._data = pd.DataFrame()\n        if vector_id not in self._data.columns:\n            issmallformat = self._is_horizontal_format()\n            column_filter = [vector_id]\n            usecols = None\n            if not issmallformat:\n                usecols = column_filter\n\n            values_df = pd.read_excel(self.get_source(), sheet_name=self._DATA_SHEET, usecols=usecols)\n\n            if issmallformat:  # Convert the table to large time series format\n                values_df = self._process_horizontal_format(values_df)\n                values_df = self._enforce_dtypes(values_df, issmallformat)\n                self._data = values_df\n            else:\n                values_df = self._enforce_dtypes(values_df, issmallformat)\n                self._data[vector_id] = values_df\n        return self._data[vector_id].to_numpy()\n\n    def get_index(self, vector_id: str) -&gt; ListTimeIndex:\n        \"\"\"\n        Get the TimeIndex describing the time dimension of the vectors in the file.\n\n        Args:\n            vector_id (str): Not used since all vectors in the NVE excel files have the same index.\n\n        Returns:\n            TimeIndex: TimeIndex object describing the excel file's index.\n\n        \"\"\"\n        meta = self.get_metadata(\"\")\n        if self._index is None:\n            self.get_values(TvMn.DATETIME_COL)\n            self._index = ListTimeIndex(\n                self._data[TvMn.DATETIME_COL].tolist(),\n                is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n                extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n                extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n            )\n        return self._index\n\n    def get_metadata(self, vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]:\n        \"\"\"\n        Read Excel file metadata.\n\n        Args:\n            vector_id (str): Not used.\n\n        Raises:\n            KeyError: If an expected metadata key is missing.\n\n        Returns:\n            dict[str, bool|int|str|datetime|timedelta|tzinfo|None]: Metadata dictionary.\n\n        \"\"\"\n        if self._meta is None:\n            path = self.get_source()\n            raw_meta = pd.read_excel(path, sheet_name=self._METADATA_SHEET, na_values=[\"\"]).replace([np.nan], [None]).to_dict(\"records\")[0]\n\n            self._meta = self._process_meta(raw_meta)\n        return self._meta\n\n    def _enforce_dtypes(self, values_df: pd.DataFrame | pd.Series, issmallformat: bool) -&gt; pd.DataFrame:\n        set_dtypes = \"float\"\n        if isinstance(values_df, pd.DataFrame):\n            set_dtypes = {c: \"float\" for c in values_df.columns if c != TvMn.DATETIME_COL}\n\n        # ensure correct dtypes\n        try:\n            return values_df.astype(set_dtypes)\n        except ValueError as e:\n            index_column = TvMn.ID_COLUMN_NAME if issmallformat else TvMn.DATETIME_COL\n            message = f\"Error in {self} while reading file. All columns except '{index_column}' must consist of only float or integer numbers.\"\n            raise RuntimeError(message) from e\n\n    def _process_horizontal_format(self, horizontal_format_df: pd.DataFrame) -&gt; pd.DataFrame:\n        # We have to read the whole file to find the correct series\n\n        # Rename the id column name and then transpose to get the correct format\n        # Since the columns are counted as indices when transposing, we need to reset the index (but keep the DateTime\n        # column)\n        reformat_df = horizontal_format_df.rename(columns={TvMn.ID_COLUMN_NAME: TvMn.DATETIME_COL}).T.reset_index(drop=False)\n\n        # after transposing, column names are set a the first row, which is DateTime, IDs\n        reformat_df.columns = reformat_df.iloc[0]\n        # We reindex by dropping the first row, thus removing the row of DateTime, IDs\n        reformat_df = reformat_df.reindex(reformat_df.index.drop(0)).reset_index(drop=True)\n\n        # Since It is possible to write only year or year-month as timestamp in the table,\n        # we need to reformat to correct datetime format\n        reformat_df[TvMn.DATETIME_COL] = self._to_iso_datetimes(reformat_df[TvMn.DATETIME_COL])\n\n        return reformat_df\n\n    def _to_iso_datetimes(self, series: pd.Series) -&gt; list[datetime]:\n        \"\"\"\n        Convert a series of dates to ISO datetime format.\n\n        Args:\n            series (pd.Series): Series which values will be converted to ISO format.\n\n        Raises:\n            RuntimeError: When an input value which cannot be converted is encountered.\n\n        Returns:\n            list[datetime]: List of formatted datetimes.\n\n        \"\"\"\n        reformatted = []\n        three_segments = 3\n        two_segments = 2\n        one_segment = 1\n        for i in series:\n            new_i = str(i)\n            date_split = len(new_i.split(\"-\"))\n            space_split = len(new_i.split(\" \"))\n            time_split = len(new_i.split(\":\"))\n            try:\n                if date_split == one_segment:  # Only year is defined\n                    # get datetime for first week first day\n                    new_i = datetime.fromisocalendar(int(new_i), 1, 1)\n                elif date_split == two_segments:\n                    # Year and month is defined\n                    new_i = datetime.strptime(new_i + \"-01\", \"%Y-%m-%d\")  # Add first day\n                elif date_split == three_segments and space_split == one_segment and time_split == one_segment:\n                    # days defined but not time\n                    new_i = datetime.strptime(new_i, \"%Y-%m-%d\")\n                elif date_split == three_segments and space_split == two_segments and time_split == one_segment:\n                    new_i = datetime.strptime(new_i, \"%Y-%m-%d %H\")\n                elif date_split == three_segments and space_split == two_segments and time_split == two_segments:\n                    new_i = datetime.strptime(new_i, \"%Y-%m-%d %H:%M\")\n                elif date_split == three_segments and space_split == two_segments and time_split == three_segments:\n                    # Assume time is defined\n                    new_i = datetime.strptime(new_i, \"%Y-%m-%d %H:%M:%S\")\n                else:\n                    msg = f\"Could not convert value '{new_i}' to datetime format.\"\n                    raise ValueError(msg)\n            except Exception as e:\n                msg = f\"Loader {self} could not convert value '{new_i}' to datetime format. Check formatting, for example number of spaces.\"\n                raise RuntimeError(msg) from e\n            reformatted.append(new_i)\n        return sorted(reformatted)\n\n    def _is_horizontal_format(self) -&gt; bool:\n        \"\"\"Determine if the file strucure is the NVE small format.\"\"\"\n        column_names = pd.read_excel(self.get_source(), nrows=0, sheet_name=self._DATA_SHEET).columns.tolist()\n        return TvMn.ID_COLUMN_NAME in column_names\n\n    def _get_ids(self) -&gt; list[str]:\n        if self._content_ids is not None:\n            return self._content_ids\n        try:\n            if self._is_horizontal_format():\n                self._content_ids = pd.read_excel(\n                    self.get_source(),\n                    usecols=[TvMn.ID_COLUMN_NAME],\n                    sheet_name=self._DATA_SHEET,\n                )[TvMn.ID_COLUMN_NAME].tolist()\n            else:\n                columns_list = pd.read_excel(self.get_source(), nrows=0, sheet_name=self._DATA_SHEET).columns.tolist()\n                columns_list.remove(TvMn.DATETIME_COL)\n                self._content_ids = columns_list\n        except ValueError as e:\n            message = f\"{self}: found problem with TimeVector IDs.\"\n            raise RuntimeError(message) from e\n\n        return self._content_ids\n\n    def clear_cache(self) -&gt; None:\n        \"\"\"Clear cached data.\"\"\"\n        self._data = None\n        self._meta = None\n        self._index = None\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEExcelTimeVectorLoader.__init__","title":"<code>__init__(source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None</code>","text":"<p>Intitialize loader instance and connect it to an Excel file containing time vector data.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Path | str</code> <p>Absolute Path to database or excel file.</p> required <code>require_whole_years</code> <code>bool</code> <p>Flag for validating that the time vectors in the source contain data for complete years.</p> required <code>relative_loc</code> <code>Path | str | None</code> <p>Path to excel file relative to source. Defaults to None.</p> <code>None</code> <code>validate</code> <code>bool</code> <p>Flag to turn on validation of timevectors. NB! Loads all data into memory at once. Defaults to True.</p> <code>True</code> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def __init__(self, source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None:\n    \"\"\"\n    Intitialize loader instance and connect it to an Excel file containing time vector data.\n\n    Args:\n        source (Path | str): Absolute Path to database or excel file.\n        require_whole_years (bool): Flag for validating that the time vectors in the source contain data for complete years.\n        relative_loc (Path | str | None, optional): Path to excel file relative to source. Defaults to None.\n        validate (bool, optional): Flag to turn on validation of timevectors. NB! Loads all data into memory at once. Defaults to True.\n\n    \"\"\"\n    super().__init__(source, require_whole_years, relative_loc)\n    self._index: TimeIndex = None\n\n    if validate:\n        self.validate_vectors()\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEExcelTimeVectorLoader.clear_cache","title":"<code>clear_cache() -&gt; None</code>","text":"<p>Clear cached data.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def clear_cache(self) -&gt; None:\n    \"\"\"Clear cached data.\"\"\"\n    self._data = None\n    self._meta = None\n    self._index = None\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEExcelTimeVectorLoader.get_index","title":"<code>get_index(vector_id: str) -&gt; ListTimeIndex</code>","text":"<p>Get the TimeIndex describing the time dimension of the vectors in the file.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Not used since all vectors in the NVE excel files have the same index.</p> required <p>Returns:</p> Name Type Description <code>TimeIndex</code> <code>ListTimeIndex</code> <p>TimeIndex object describing the excel file's index.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_index(self, vector_id: str) -&gt; ListTimeIndex:\n    \"\"\"\n    Get the TimeIndex describing the time dimension of the vectors in the file.\n\n    Args:\n        vector_id (str): Not used since all vectors in the NVE excel files have the same index.\n\n    Returns:\n        TimeIndex: TimeIndex object describing the excel file's index.\n\n    \"\"\"\n    meta = self.get_metadata(\"\")\n    if self._index is None:\n        self.get_values(TvMn.DATETIME_COL)\n        self._index = ListTimeIndex(\n            self._data[TvMn.DATETIME_COL].tolist(),\n            is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n            extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n            extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n        )\n    return self._index\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEExcelTimeVectorLoader.get_metadata","title":"<code>get_metadata(vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]</code>","text":"<p>Read Excel file metadata.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Not used.</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>If an expected metadata key is missing.</p> <p>Returns:</p> Type Description <code>dict[str, bool | int | str | datetime | timedelta | tzinfo | None]</code> <p>dict[str, bool|int|str|datetime|timedelta|tzinfo|None]: Metadata dictionary.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_metadata(self, vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]:\n    \"\"\"\n    Read Excel file metadata.\n\n    Args:\n        vector_id (str): Not used.\n\n    Raises:\n        KeyError: If an expected metadata key is missing.\n\n    Returns:\n        dict[str, bool|int|str|datetime|timedelta|tzinfo|None]: Metadata dictionary.\n\n    \"\"\"\n    if self._meta is None:\n        path = self.get_source()\n        raw_meta = pd.read_excel(path, sheet_name=self._METADATA_SHEET, na_values=[\"\"]).replace([np.nan], [None]).to_dict(\"records\")[0]\n\n        self._meta = self._process_meta(raw_meta)\n    return self._meta\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEExcelTimeVectorLoader.get_unit","title":"<code>get_unit(vector_id: str) -&gt; str</code>","text":"<p>Get the unit of the given time vector.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>ID of a time vector. Not used since all time vectors in the NVE excel files have the same              unit.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Unit of the time vector.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_unit(self, vector_id: str) -&gt; str:\n    \"\"\"\n    Get the unit of the given time vector.\n\n    Args:\n        vector_id (str): ID of a time vector. Not used since all time vectors in the NVE excel files have the same\n                         unit.\n\n    Returns:\n        str: Unit of the time vector.\n\n    \"\"\"\n    return self.get_metadata(\"\")[TvMn.UNIT]\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEExcelTimeVectorLoader.get_values","title":"<code>get_values(vector_id: str) -&gt; NDArray</code>","text":"<p>Get numpy array with all the values of a given vector in the Loader's excel file.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Unique id of the vector in the file.</p> required <p>Returns:</p> Name Type Description <code>NDArray</code> <code>NDArray</code> <p>Numpy array with values.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_values(self, vector_id: str) -&gt; NDArray:\n    \"\"\"\n    Get numpy array with all the values of a given vector in the Loader's excel file.\n\n    Args:\n        vector_id (str): Unique id of the vector in the file.\n\n    Returns:\n        NDArray: Numpy array with values.\n\n    \"\"\"\n    if self._data is None:\n        self._data = pd.DataFrame()\n    if vector_id not in self._data.columns:\n        issmallformat = self._is_horizontal_format()\n        column_filter = [vector_id]\n        usecols = None\n        if not issmallformat:\n            usecols = column_filter\n\n        values_df = pd.read_excel(self.get_source(), sheet_name=self._DATA_SHEET, usecols=usecols)\n\n        if issmallformat:  # Convert the table to large time series format\n            values_df = self._process_horizontal_format(values_df)\n            values_df = self._enforce_dtypes(values_df, issmallformat)\n            self._data = values_df\n        else:\n            values_df = self._enforce_dtypes(values_df, issmallformat)\n            self._data[vector_id] = values_df\n    return self._data[vector_id].to_numpy()\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEH5TimeVectorLoader","title":"<code>NVEH5TimeVectorLoader</code>","text":"<p>               Bases: <code>NVETimeVectorLoader</code></p> <p>Class for loading time vector data from NVE HDF5 file sources.</p> <p>Meant for large time vectors (e.g. hourly data over multiple years). Supports differing lengths and metadata of vectors stored in the file.</p> Specialized to the following format <ul> <li>index (h5py.Group, optional): Used to define indexes for vectors if index is supposed to only apply to that vector.</li> <li>common_index (h5py.Dataset): Contains one numpy array for all vectors. This is a fallback index for vectors which have not defined their own index in                                the index group. Also used on purpose if many or all vectors have the same index.</li> <li>metadata (h5py.Group): Used connect a specific set of metadata to a particular vector.</li> <li>common_metadata (h5py.Group): Contains one set of metadata fields for all vectors. Used in a similar way as common_index.</li> <li>vectors (h5py.Group): Contains numpy arrays containing the vector values connected to a unique ID. The same ID is used to connect the vector to an                         index or metadata.</li> </ul> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>class NVEH5TimeVectorLoader(NVETimeVectorLoader):\n    \"\"\"\n    Class for loading time vector data from NVE HDF5 file sources.\n\n    Meant for large time vectors (e.g. hourly data over multiple years). Supports differing lengths and metadata of vectors stored in the file.\n\n    Specialized to the following format:\n        - index (h5py.Group, optional): Used to define indexes for vectors if index is supposed to only apply to that vector.\n        - common_index (h5py.Dataset): Contains one numpy array for all vectors. This is a fallback index for vectors which have not defined their own index in\n                                       the index group. Also used on purpose if many or all vectors have the same index.\n        - metadata (h5py.Group): Used connect a specific set of metadata to a particular vector.\n        - common_metadata (h5py.Group): Contains one set of metadata fields for all vectors. Used in a similar way as common_index.\n        - vectors (h5py.Group): Contains numpy arrays containing the vector values connected to a unique ID. The same ID is used to connect the vector to an\n                                index or metadata.\n\n    \"\"\"\n\n    _SUPPORTED_SUFFIXES: ClassVar[list] = [\".h5\", \".hdf5\"]\n\n    def __init__(self, source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None:\n        \"\"\"\n        Intitialize loader instance and connect it to a H5 file containing time vector data.\n\n        Args:\n            source (Path | str): Absolute Path to database or HDF5 file.\n            require_whole_years (bool): Flag for validating that the time vectors in the source contain data for complete years.\n            relative_loc (Path | str | None, optional): Path to HDF5 file relative to source. Defaults to None.\n            validate (bool, optional): Whether to validate vectors after loading. NB! Loads all data into memory at once. Defaults to True.\n\n        \"\"\"\n        super().__init__(source, require_whole_years, relative_loc)\n        self._index: TimeIndex = None\n        self._file_pointer = None\n\n        if validate:\n            self.validate_vectors()\n\n    def get_values(self, vector_id: str) -&gt; NDArray:\n        \"\"\"\n        Get numpy array with all the values of a given vector in the Loader's HDF5 file.\n\n        Args:\n            vector_id (str): Unique id of the vector in the file.\n\n        Returns:\n            NDArray: Numpy array with values.\n\n        \"\"\"\n        if self._data is None:\n            self._data = dict()\n        if vector_id not in self._data:\n            with h5py.File(self.get_source(), mode=\"r\") as h5f:\n                self._data[vector_id] = self._read_vector_field(h5f, H5Names.VECTORS_GROUP, vector_id, field_type=h5py.Dataset, use_fallback=False)[()]\n        return self._data[vector_id]\n\n    def get_index(self, vector_id: str) -&gt; TimeIndex:\n        \"\"\"\n        Get the TimeIndex describing the time dimension of the vectors in the file.\n\n        Args:\n            vector_id (str): Not used since all vectors in the NVE parquet files have the same index.\n\n        Returns:\n            TimeIndex: TimeIndex object describing the parquet file's index.\n\n        \"\"\"\n        if self._index is None:\n            meta = self.get_metadata(\"\")\n\n            if meta[TvMn.FREQUENCY] is None:\n                self._index = ListTimeIndex(\n                    datetime_list=self._read_index(vector_id),\n                    is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n                    extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n                    extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n                )\n                return self._index\n\n            start = pd.to_datetime(self._read_index(vector_id)[0]) if meta[TvMn.START] is None else meta[TvMn.START]\n            num_points = self._read_index(vector_id).size if meta[TvMn.NUM_POINTS] is None else meta[TvMn.NUM_POINTS]\n\n            self._index = FixedFrequencyTimeIndex(\n                start,\n                meta[TvMn.FREQUENCY],\n                num_points,\n                is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n                extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n                extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n            )\n\n        return self._index\n\n    def _read_index(self, vector_id: str) -&gt; NDArray:\n        with h5py.File(self.get_source(), mode=\"r\") as h5f:\n            return np.char.decode(self._read_vector_field(h5f, H5Names.INDEX_GROUP, vector_id, h5py.Dataset)[()], encoding=\"utf-8\").astype(datetime)\n\n    def _read_vector_field(\n        self,\n        h5file: h5py.File,\n        field_name: str,\n        vector_id: str,\n        field_type: type[h5py.Dataset | h5py.Group],\n        use_fallback: bool = True,\n    ) -&gt; h5py.Dataset | h5py.Group:\n        error = \"\"\n        if field_name in h5file:  # check if group_name exists\n            main_group = h5file[field_name]\n            if not isinstance(main_group, h5py.Group):\n                message = f\"{self} expected '{field_name}' to be a {h5py.Group} in {h5file}. Got {type(main_group)}.\"\n                raise TypeError(message)\n\n            if vector_id in main_group:\n                vector_field = main_group[vector_id]\n                if not isinstance(vector_field, field_type):\n                    message = f\"{self} expected '{vector_id}' to be a {field_type} in {h5file}. Got {type(vector_field)}\"\n                    raise TypeError(message)\n                return vector_field\n            error = f\"'{vector_id}' was not found in '{field_name}' group\"\n        else:\n            error = f\"'{field_name}' was not found in file\"\n\n        no_fallback_message = f\"{self} expected '{vector_id}' in {h5py.Group} '{field_name}' \"\n        if not use_fallback:\n            no_fallback_message += f\"but {error}.\"\n            raise KeyError(no_fallback_message)\n\n        fallback_name = H5Names.COMMON_PREFIX + field_name\n        if fallback_name in h5file:  # check if common_ + group_name exists\n            fallback_field = h5file[fallback_name]\n            if not isinstance(fallback_field, field_type):\n                message = f\"{self} expected '{fallback_field}' to be a {field_type} in {h5file}. Got {type(fallback_field)}.\"\n                raise TypeError(message)\n            return fallback_field\n\n        message = (\n            no_fallback_message\n            + f\"or a fallback {field_type} '{fallback_name}' in H5 file but \"\n            + f\"{error},\"\n            + f\" and fallback {field_type} '{fallback_name}' not found in file.\"\n        )\n        raise KeyError(message)\n\n    def get_metadata(self, vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]:\n        \"\"\"\n        Retrieve and decodes custom metadata from parquet file.\n\n        Args:\n            vector_id (str): Not used\n\n        Raises:\n            KeyError: If any of the expected metadata keys is not found in file.\n\n        Returns:\n            dict: Dictionary with decoded metadata.\n\n        \"\"\"\n        if self._meta is None:\n            errors = set()\n            meta = {}\n            with h5py.File(self.get_source(), mode=\"r\") as h5f:\n                meta_group = self._read_vector_field(h5f, H5Names.METADATA_GROUP, vector_id, h5py.Group)\n                for k, m in meta_group.items():\n                    if isinstance(m, h5py.Dataset):\n                        meta[k] = m[()]\n                    else:\n                        errors.add(f\"Improper metadata format: Metadata key {k} exists but is a h5 group when it should be a h5 dataset.\")\n            self._report_errors(errors)\n            self._meta = self._process_meta(meta)\n        return self._meta\n\n    def _get_ids(self) -&gt; list[str]:\n        with h5py.File(self.get_source(), mode=\"r\") as h5f:\n            if H5Names.VECTORS_GROUP in h5f:\n                return list(h5f[H5Names.VECTORS_GROUP].keys())\n            message = f\"{self} required key '{H5Names.VECTORS_GROUP}' was not found in file.\"\n            raise KeyError(message)\n\n    def clear_cache(self) -&gt; None:\n        \"\"\"Clear cached data.\"\"\"\n        self._data = None\n        self._meta = None\n        self._index = None\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEH5TimeVectorLoader.__init__","title":"<code>__init__(source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None</code>","text":"<p>Intitialize loader instance and connect it to a H5 file containing time vector data.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Path | str</code> <p>Absolute Path to database or HDF5 file.</p> required <code>require_whole_years</code> <code>bool</code> <p>Flag for validating that the time vectors in the source contain data for complete years.</p> required <code>relative_loc</code> <code>Path | str | None</code> <p>Path to HDF5 file relative to source. Defaults to None.</p> <code>None</code> <code>validate</code> <code>bool</code> <p>Whether to validate vectors after loading. NB! Loads all data into memory at once. Defaults to True.</p> <code>True</code> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def __init__(self, source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None:\n    \"\"\"\n    Intitialize loader instance and connect it to a H5 file containing time vector data.\n\n    Args:\n        source (Path | str): Absolute Path to database or HDF5 file.\n        require_whole_years (bool): Flag for validating that the time vectors in the source contain data for complete years.\n        relative_loc (Path | str | None, optional): Path to HDF5 file relative to source. Defaults to None.\n        validate (bool, optional): Whether to validate vectors after loading. NB! Loads all data into memory at once. Defaults to True.\n\n    \"\"\"\n    super().__init__(source, require_whole_years, relative_loc)\n    self._index: TimeIndex = None\n    self._file_pointer = None\n\n    if validate:\n        self.validate_vectors()\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEH5TimeVectorLoader.clear_cache","title":"<code>clear_cache() -&gt; None</code>","text":"<p>Clear cached data.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def clear_cache(self) -&gt; None:\n    \"\"\"Clear cached data.\"\"\"\n    self._data = None\n    self._meta = None\n    self._index = None\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEH5TimeVectorLoader.get_index","title":"<code>get_index(vector_id: str) -&gt; TimeIndex</code>","text":"<p>Get the TimeIndex describing the time dimension of the vectors in the file.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Not used since all vectors in the NVE parquet files have the same index.</p> required <p>Returns:</p> Name Type Description <code>TimeIndex</code> <code>TimeIndex</code> <p>TimeIndex object describing the parquet file's index.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_index(self, vector_id: str) -&gt; TimeIndex:\n    \"\"\"\n    Get the TimeIndex describing the time dimension of the vectors in the file.\n\n    Args:\n        vector_id (str): Not used since all vectors in the NVE parquet files have the same index.\n\n    Returns:\n        TimeIndex: TimeIndex object describing the parquet file's index.\n\n    \"\"\"\n    if self._index is None:\n        meta = self.get_metadata(\"\")\n\n        if meta[TvMn.FREQUENCY] is None:\n            self._index = ListTimeIndex(\n                datetime_list=self._read_index(vector_id),\n                is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n                extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n                extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n            )\n            return self._index\n\n        start = pd.to_datetime(self._read_index(vector_id)[0]) if meta[TvMn.START] is None else meta[TvMn.START]\n        num_points = self._read_index(vector_id).size if meta[TvMn.NUM_POINTS] is None else meta[TvMn.NUM_POINTS]\n\n        self._index = FixedFrequencyTimeIndex(\n            start,\n            meta[TvMn.FREQUENCY],\n            num_points,\n            is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n            extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n            extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n        )\n\n    return self._index\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEH5TimeVectorLoader.get_metadata","title":"<code>get_metadata(vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]</code>","text":"<p>Retrieve and decodes custom metadata from parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Not used</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>If any of the expected metadata keys is not found in file.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, bool | int | str | datetime | timedelta | tzinfo | None]</code> <p>Dictionary with decoded metadata.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_metadata(self, vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]:\n    \"\"\"\n    Retrieve and decodes custom metadata from parquet file.\n\n    Args:\n        vector_id (str): Not used\n\n    Raises:\n        KeyError: If any of the expected metadata keys is not found in file.\n\n    Returns:\n        dict: Dictionary with decoded metadata.\n\n    \"\"\"\n    if self._meta is None:\n        errors = set()\n        meta = {}\n        with h5py.File(self.get_source(), mode=\"r\") as h5f:\n            meta_group = self._read_vector_field(h5f, H5Names.METADATA_GROUP, vector_id, h5py.Group)\n            for k, m in meta_group.items():\n                if isinstance(m, h5py.Dataset):\n                    meta[k] = m[()]\n                else:\n                    errors.add(f\"Improper metadata format: Metadata key {k} exists but is a h5 group when it should be a h5 dataset.\")\n        self._report_errors(errors)\n        self._meta = self._process_meta(meta)\n    return self._meta\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEH5TimeVectorLoader.get_values","title":"<code>get_values(vector_id: str) -&gt; NDArray</code>","text":"<p>Get numpy array with all the values of a given vector in the Loader's HDF5 file.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Unique id of the vector in the file.</p> required <p>Returns:</p> Name Type Description <code>NDArray</code> <code>NDArray</code> <p>Numpy array with values.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_values(self, vector_id: str) -&gt; NDArray:\n    \"\"\"\n    Get numpy array with all the values of a given vector in the Loader's HDF5 file.\n\n    Args:\n        vector_id (str): Unique id of the vector in the file.\n\n    Returns:\n        NDArray: Numpy array with values.\n\n    \"\"\"\n    if self._data is None:\n        self._data = dict()\n    if vector_id not in self._data:\n        with h5py.File(self.get_source(), mode=\"r\") as h5f:\n            self._data[vector_id] = self._read_vector_field(h5f, H5Names.VECTORS_GROUP, vector_id, field_type=h5py.Dataset, use_fallback=False)[()]\n    return self._data[vector_id]\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEParquetTimeVectorLoader","title":"<code>NVEParquetTimeVectorLoader</code>","text":"<p>               Bases: <code>NVETimeVectorLoader</code></p> <p>Class for loading time vector data from NVE parquet file sources.</p> <p>Meant for large time vectors. All vectors in the file must have the same lenghts and metadata. Supports format:     - 'Vertical' with one index collumn (DateTime) and the others containing vector values.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>class NVEParquetTimeVectorLoader(NVETimeVectorLoader):\n    \"\"\"\n    Class for loading time vector data from NVE parquet file sources.\n\n    Meant for large time vectors. All vectors in the file must have the same lenghts and metadata.\n    Supports format:\n        - 'Vertical' with one index collumn (DateTime) and the others containing vector values.\n\n    \"\"\"\n\n    _SUPPORTED_SUFFIXES: ClassVar[list] = [\".parquet\"]\n\n    def __init__(self, source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None:\n        \"\"\"\n        Intitialize loader instance and connect it to an Parquet file containing time vector data.\n\n        Args:\n            source (Path | str): Absolute Path to database or parquet file.\n            require_whole_years (bool): Flag for validating that the time vectors in the source contain data for complete years.\n            relative_loc (Path | str | None, optional): Path to parquet file relative to source. Defaults to None.\n            validate (bool, optional): Flag to turn on validation of timevectors. NB! Loads all data into memory at once. Defaults to True.\n\n        \"\"\"\n        super().__init__(source, require_whole_years, relative_loc)\n        self._index: TimeIndex = None\n        if validate:\n            self.validate_vectors()\n\n    def get_values(self, vector_id: str) -&gt; NDArray:\n        \"\"\"\n        Get numpy array with all the values of a given vector in the Loader's parquet file.\n\n        Args:\n            vector_id (str): Unique id of the vector in the file.\n\n        Returns:\n            NDArray: Numpy array with values.\n\n        \"\"\"\n        if self._data is None:\n            self._data = dict()\n        if vector_id not in self._data:\n            table = pq.read_table(self.get_source(), columns=[vector_id])\n            self._data[vector_id] = table[vector_id].to_numpy()\n        # if self._data is None:\n        #     self._data = pq.read_table(self.get_source())\n        return self._data[vector_id]  # .to_numpy()\n\n    def get_index(self, vector_id: str) -&gt; TimeIndex:  # Could be more types of indexes?\n        \"\"\"\n        Get the TimeIndex describing the time dimension of the vectors in the file.\n\n        Args:\n            vector_id (str): Not used since all vectors in the NVE parquet files have the same index.\n\n        Returns:\n            TimeIndex: TimeIndex object describing the parquet file's index.\n\n        \"\"\"\n        if self._index is None:\n            meta = self.get_metadata(\"\")\n\n            if TvMn.FREQUENCY not in meta or (TvMn.FREQUENCY in meta and meta[TvMn.FREQUENCY] is None):\n                datetime_index = pd.DatetimeIndex(\n                    pd.read_parquet(self.get_source(), columns=[TvMn.DATETIME_COL])[TvMn.DATETIME_COL],\n                    tz=meta[TvMn.TIMEZONE],\n                ).tolist()\n                self._index = ListTimeIndex(\n                    datetime_list=datetime_index,\n                    is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n                    extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n                    extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n                )\n                return self._index\n\n            parquet_file = None\n            if TvMn.START not in meta or (TvMn.START in meta and meta[TvMn.START] is None):\n                parquet_file = pq.ParquetFile(self.get_source())\n                start = pd.to_datetime(next(parquet_file.iter_batches(batch_size=1, columns=[TvMn.DATETIME_COL])))\n            else:\n                start = meta[TvMn.START]\n\n            if TvMn.NUM_POINTS not in meta or (TvMn.NUM_POINTS in meta and meta[TvMn.NUM_POINTS] is None):\n                if parquet_file is None:\n                    parquet_file = pq.ParquetFile(self.get_source())\n                num_points = parquet_file.metadata.num_rows\n            else:\n                num_points = meta[TvMn.NUM_POINTS]\n            self._index = FixedFrequencyTimeIndex(\n                start,\n                meta[TvMn.FREQUENCY],\n                num_points,\n                is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n                extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n                extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n            )\n\n        return self._index\n\n    def get_metadata(self, vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]:\n        \"\"\"\n        Retrieve and decodes custom metadata from parquet file.\n\n        Args:\n            vector_id (str): Not used\n\n        Raises:\n            KeyError: If any of the expected metadata keys is not found in file.\n\n        Returns:\n            dict: Dictionary with decoded metadata.\n\n        \"\"\"\n        if self._meta is None:\n            path = self.get_source()\n            raw_meta = pq.ParquetFile(path).schema_arrow.metadata\n\n            self._meta = self._process_meta(raw_meta)\n        return self._meta\n\n    def _get_ids(self) -&gt; list[str]:\n        parquet_file = pq.ParquetFile(self.get_source())\n        time_vector_ids: list[str] = parquet_file.schema_arrow.names\n        time_vector_ids.remove(TvMn.DATETIME_COL)\n        return time_vector_ids\n\n    def clear_cache(self) -&gt; None:\n        \"\"\"Clear cached data.\"\"\"\n        self._data = None\n        self._meta = None\n        self._index = None\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEParquetTimeVectorLoader.__init__","title":"<code>__init__(source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None</code>","text":"<p>Intitialize loader instance and connect it to an Parquet file containing time vector data.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Path | str</code> <p>Absolute Path to database or parquet file.</p> required <code>require_whole_years</code> <code>bool</code> <p>Flag for validating that the time vectors in the source contain data for complete years.</p> required <code>relative_loc</code> <code>Path | str | None</code> <p>Path to parquet file relative to source. Defaults to None.</p> <code>None</code> <code>validate</code> <code>bool</code> <p>Flag to turn on validation of timevectors. NB! Loads all data into memory at once. Defaults to True.</p> <code>True</code> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def __init__(self, source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None:\n    \"\"\"\n    Intitialize loader instance and connect it to an Parquet file containing time vector data.\n\n    Args:\n        source (Path | str): Absolute Path to database or parquet file.\n        require_whole_years (bool): Flag for validating that the time vectors in the source contain data for complete years.\n        relative_loc (Path | str | None, optional): Path to parquet file relative to source. Defaults to None.\n        validate (bool, optional): Flag to turn on validation of timevectors. NB! Loads all data into memory at once. Defaults to True.\n\n    \"\"\"\n    super().__init__(source, require_whole_years, relative_loc)\n    self._index: TimeIndex = None\n    if validate:\n        self.validate_vectors()\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEParquetTimeVectorLoader.clear_cache","title":"<code>clear_cache() -&gt; None</code>","text":"<p>Clear cached data.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def clear_cache(self) -&gt; None:\n    \"\"\"Clear cached data.\"\"\"\n    self._data = None\n    self._meta = None\n    self._index = None\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEParquetTimeVectorLoader.get_index","title":"<code>get_index(vector_id: str) -&gt; TimeIndex</code>","text":"<p>Get the TimeIndex describing the time dimension of the vectors in the file.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Not used since all vectors in the NVE parquet files have the same index.</p> required <p>Returns:</p> Name Type Description <code>TimeIndex</code> <code>TimeIndex</code> <p>TimeIndex object describing the parquet file's index.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_index(self, vector_id: str) -&gt; TimeIndex:  # Could be more types of indexes?\n    \"\"\"\n    Get the TimeIndex describing the time dimension of the vectors in the file.\n\n    Args:\n        vector_id (str): Not used since all vectors in the NVE parquet files have the same index.\n\n    Returns:\n        TimeIndex: TimeIndex object describing the parquet file's index.\n\n    \"\"\"\n    if self._index is None:\n        meta = self.get_metadata(\"\")\n\n        if TvMn.FREQUENCY not in meta or (TvMn.FREQUENCY in meta and meta[TvMn.FREQUENCY] is None):\n            datetime_index = pd.DatetimeIndex(\n                pd.read_parquet(self.get_source(), columns=[TvMn.DATETIME_COL])[TvMn.DATETIME_COL],\n                tz=meta[TvMn.TIMEZONE],\n            ).tolist()\n            self._index = ListTimeIndex(\n                datetime_list=datetime_index,\n                is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n                extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n                extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n            )\n            return self._index\n\n        parquet_file = None\n        if TvMn.START not in meta or (TvMn.START in meta and meta[TvMn.START] is None):\n            parquet_file = pq.ParquetFile(self.get_source())\n            start = pd.to_datetime(next(parquet_file.iter_batches(batch_size=1, columns=[TvMn.DATETIME_COL])))\n        else:\n            start = meta[TvMn.START]\n\n        if TvMn.NUM_POINTS not in meta or (TvMn.NUM_POINTS in meta and meta[TvMn.NUM_POINTS] is None):\n            if parquet_file is None:\n                parquet_file = pq.ParquetFile(self.get_source())\n            num_points = parquet_file.metadata.num_rows\n        else:\n            num_points = meta[TvMn.NUM_POINTS]\n        self._index = FixedFrequencyTimeIndex(\n            start,\n            meta[TvMn.FREQUENCY],\n            num_points,\n            is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n            extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n            extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n        )\n\n    return self._index\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEParquetTimeVectorLoader.get_metadata","title":"<code>get_metadata(vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]</code>","text":"<p>Retrieve and decodes custom metadata from parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Not used</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>If any of the expected metadata keys is not found in file.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, bool | int | str | datetime | timedelta | tzinfo | None]</code> <p>Dictionary with decoded metadata.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_metadata(self, vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]:\n    \"\"\"\n    Retrieve and decodes custom metadata from parquet file.\n\n    Args:\n        vector_id (str): Not used\n\n    Raises:\n        KeyError: If any of the expected metadata keys is not found in file.\n\n    Returns:\n        dict: Dictionary with decoded metadata.\n\n    \"\"\"\n    if self._meta is None:\n        path = self.get_source()\n        raw_meta = pq.ParquetFile(path).schema_arrow.metadata\n\n        self._meta = self._process_meta(raw_meta)\n    return self._meta\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEParquetTimeVectorLoader.get_values","title":"<code>get_values(vector_id: str) -&gt; NDArray</code>","text":"<p>Get numpy array with all the values of a given vector in the Loader's parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Unique id of the vector in the file.</p> required <p>Returns:</p> Name Type Description <code>NDArray</code> <code>NDArray</code> <p>Numpy array with values.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_values(self, vector_id: str) -&gt; NDArray:\n    \"\"\"\n    Get numpy array with all the values of a given vector in the Loader's parquet file.\n\n    Args:\n        vector_id (str): Unique id of the vector in the file.\n\n    Returns:\n        NDArray: Numpy array with values.\n\n    \"\"\"\n    if self._data is None:\n        self._data = dict()\n    if vector_id not in self._data:\n        table = pq.read_table(self.get_source(), columns=[vector_id])\n        self._data[vector_id] = table[vector_id].to_numpy()\n    # if self._data is None:\n    #     self._data = pq.read_table(self.get_source())\n    return self._data[vector_id]  # .to_numpy()\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEYamlTimeVectoroader","title":"<code>NVEYamlTimeVectoroader</code>","text":"<p>               Bases: <code>NVETimeVectorLoader</code></p> <p>Class for loading time vector data from NVE YAML file sources.</p> <p>Meant for very sparse time vector data, where the vectors have varying lengths and indexes. Currently all vectors must have the same metadata within each file. Supported format:     - Metadata: field containing dictionary with metadata for all vectors.     - Other fields are vector IDs with lists for x and y axes.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>class NVEYamlTimeVectoroader(NVETimeVectorLoader):\n    \"\"\"\n    Class for loading time vector data from NVE YAML file sources.\n\n    Meant for very sparse time vector data, where the vectors have varying lengths and indexes. Currently all vectors must have the same metadata within each\n    file.\n    Supported format:\n        - Metadata: field containing dictionary with metadata for all vectors.\n        - Other fields are vector IDs with lists for x and y axes.\n\n    \"\"\"\n\n    _SUPPORTED_SUFFIXES: ClassVar[list] = [\".yaml\", \".yml\"]\n\n    def __init__(self, source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None:\n        \"\"\"\n        Intitialize loader instance and connect it to an Yaml file containing time vector data.\n\n        Args:\n            source (Path | str): Absolute Path to database or excel file.\n            require_whole_years (bool): Flag for validating that the time vectors in the source contain data for complete years.\n            relative_loc (Path | str | None, optional): Path to excel file relative to source. Defaults to None.\n            validate (bool, optional): Flag to turn on validation of timevectors. NB! Loads all data into memory at once. Defaults to True.\n\n        \"\"\"\n        super().__init__(source, require_whole_years, relative_loc)\n        self._content_ids: list[str] = None\n\n        self._values_label: str = None\n        self._index_label: str = None\n\n        if validate:\n            self.validate_vectors()\n\n    def get_values(self, vector_id: str) -&gt; NDArray:\n        \"\"\"\n        Get values of vector.\n\n        Args:\n            vector_id (str): Unique id of the curve in the Loader source.\n\n        Returns:\n            NDArray: Numpy array with values of vector.\n\n        \"\"\"\n        if self._data is None:\n            self._parse_file()\n        values_list = self._data[vector_id][self._values_label]\n        if len(values_list) == 0:\n            message = f\"Time vector {vector_id} in {self} contains no points.\"\n            raise ValueError(message)\n        return np.asarray(values_list)\n\n    def get_index(self, vector_id: str) -&gt; TimeIndex:\n        \"\"\"\n        Get index of vector.\n\n        Args:\n            vector_id (str): Unique id of the curve in the Loader source.\n\n        Returns:\n            NDArray: Numpy array with index of vector.\n\n        \"\"\"\n        meta = self.get_metadata(\"\")\n        try:\n            datetime_list = [self._date_to_datetime(index_val) for index_val in self._data[vector_id][self._index_label]]\n        except ValueError as e:\n            message = f\"{self} got non date or none datetime values in index field of vector {vector_id}.\"\n            raise ValueError(message) from e\n\n        if len(datetime_list) == 0:\n            message = f\"Index of {vector_id} in {self} contains no points.\"\n            raise ValueError(message)\n        if len(datetime_list) == 1:\n            if not meta[TvMn.IS_ZERO_ONE_PROFILE]:\n                message = (\n                    f\"Index of {vector_id} in {self} contains a single point but is classified as an average index with a reference period.\"\n                    \" Can only create an extrapolatable index when it does not have a reference period.\"\n                )\n                raise ValueError(message)\n            # if not (meta[TvMn.EXTRAPOLATE_FISRT_POINT] and meta[TvMn.EXTRAPOLATE_LAST_POINT]):\n            #     message = (\n            #         f\"Index of {vector_id} in {self} contains a single point but is classified as not extrapolatable.\"\n            #         \" Can only create an extrapolatable index when it is both forward and backwards extrapolatable.\"\n            #     )\n            #     raise ValueError(message)\n            return ConstantTimeIndex()\n\n        return ListTimeIndex(\n            datetime_list=datetime_list,\n            is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n            extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n            extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n        )\n\n    def get_metadata(self, vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]:\n        \"\"\"\n        Read YAML file metadata.\n\n        Args:\n            vector_id (str): Not used.\n\n        Raises:\n            KeyError: If an expected metadata key is missing.\n\n        Returns:\n            dict[str, bool|int|str|datetime|timedelta|tzinfo|None]: Metadata dictionary.\n\n        \"\"\"\n        if self._meta is None:\n            raw_meta = self._data[YamlNames.metadata_field][YamlNames.x_field]\n\n            self._meta = self._process_meta(raw_meta)\n        return self._meta\n\n    def _get_ids(self) -&gt; list[str]:\n        if self._content_ids is None:\n            if self._data is None:\n                self._parse_file()\n            ids_list = list(self._data.keys())\n            ids_list.remove(YamlNames.metadata_field)\n            self._content_ids = ids_list\n        return self._content_ids\n\n    def _parse_file(self) -&gt; None:\n        with self.get_source().open(encoding=YamlNames.encoding) as f:\n            d = yaml.safe_load(f)\n            self._x_meta = d[YamlNames.metadata_field][YamlNames.x_field]\n            self._y_meta = d[YamlNames.metadata_field][YamlNames.y_field]\n\n            self._values_label = self._x_meta[YamlNames.attribute]\n            self._index_label = self._y_meta[YamlNames.attribute]\n\n            self._data = d\n\n    def _date_to_datetime(self, value: date | datetime) -&gt; datetime:\n        if isinstance(value, date):\n            value = datetime(value.year, value.month, value.day)\n        elif not isinstance(value, datetime):\n            message = \"Value must be date or datetime.\"\n            raise ValueError(message)\n        return value\n\n    def clear_cache(self) -&gt; None:\n        \"\"\"Clear cached data.\"\"\"\n        self._data = None\n        self._meta = None\n\n        self._content_ids = None\n\n        self._values_label = None\n        self._index_label = None\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEYamlTimeVectoroader.__init__","title":"<code>__init__(source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None</code>","text":"<p>Intitialize loader instance and connect it to an Yaml file containing time vector data.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Path | str</code> <p>Absolute Path to database or excel file.</p> required <code>require_whole_years</code> <code>bool</code> <p>Flag for validating that the time vectors in the source contain data for complete years.</p> required <code>relative_loc</code> <code>Path | str | None</code> <p>Path to excel file relative to source. Defaults to None.</p> <code>None</code> <code>validate</code> <code>bool</code> <p>Flag to turn on validation of timevectors. NB! Loads all data into memory at once. Defaults to True.</p> <code>True</code> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def __init__(self, source: Path | str, require_whole_years: bool, relative_loc: Path | str | None = None, validate: bool = True) -&gt; None:\n    \"\"\"\n    Intitialize loader instance and connect it to an Yaml file containing time vector data.\n\n    Args:\n        source (Path | str): Absolute Path to database or excel file.\n        require_whole_years (bool): Flag for validating that the time vectors in the source contain data for complete years.\n        relative_loc (Path | str | None, optional): Path to excel file relative to source. Defaults to None.\n        validate (bool, optional): Flag to turn on validation of timevectors. NB! Loads all data into memory at once. Defaults to True.\n\n    \"\"\"\n    super().__init__(source, require_whole_years, relative_loc)\n    self._content_ids: list[str] = None\n\n    self._values_label: str = None\n    self._index_label: str = None\n\n    if validate:\n        self.validate_vectors()\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEYamlTimeVectoroader.clear_cache","title":"<code>clear_cache() -&gt; None</code>","text":"<p>Clear cached data.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def clear_cache(self) -&gt; None:\n    \"\"\"Clear cached data.\"\"\"\n    self._data = None\n    self._meta = None\n\n    self._content_ids = None\n\n    self._values_label = None\n    self._index_label = None\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEYamlTimeVectoroader.get_index","title":"<code>get_index(vector_id: str) -&gt; TimeIndex</code>","text":"<p>Get index of vector.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Unique id of the curve in the Loader source.</p> required <p>Returns:</p> Name Type Description <code>NDArray</code> <code>TimeIndex</code> <p>Numpy array with index of vector.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_index(self, vector_id: str) -&gt; TimeIndex:\n    \"\"\"\n    Get index of vector.\n\n    Args:\n        vector_id (str): Unique id of the curve in the Loader source.\n\n    Returns:\n        NDArray: Numpy array with index of vector.\n\n    \"\"\"\n    meta = self.get_metadata(\"\")\n    try:\n        datetime_list = [self._date_to_datetime(index_val) for index_val in self._data[vector_id][self._index_label]]\n    except ValueError as e:\n        message = f\"{self} got non date or none datetime values in index field of vector {vector_id}.\"\n        raise ValueError(message) from e\n\n    if len(datetime_list) == 0:\n        message = f\"Index of {vector_id} in {self} contains no points.\"\n        raise ValueError(message)\n    if len(datetime_list) == 1:\n        if not meta[TvMn.IS_ZERO_ONE_PROFILE]:\n            message = (\n                f\"Index of {vector_id} in {self} contains a single point but is classified as an average index with a reference period.\"\n                \" Can only create an extrapolatable index when it does not have a reference period.\"\n            )\n            raise ValueError(message)\n        # if not (meta[TvMn.EXTRAPOLATE_FISRT_POINT] and meta[TvMn.EXTRAPOLATE_LAST_POINT]):\n        #     message = (\n        #         f\"Index of {vector_id} in {self} contains a single point but is classified as not extrapolatable.\"\n        #         \" Can only create an extrapolatable index when it is both forward and backwards extrapolatable.\"\n        #     )\n        #     raise ValueError(message)\n        return ConstantTimeIndex()\n\n    return ListTimeIndex(\n        datetime_list=datetime_list,\n        is_52_week_years=meta[TvMn.IS_52_WEEK_YEARS],\n        extrapolate_first_point=meta[TvMn.EXTRAPOLATE_FISRT_POINT],\n        extrapolate_last_point=meta[TvMn.EXTRAPOLATE_LAST_POINT],\n    )\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEYamlTimeVectoroader.get_metadata","title":"<code>get_metadata(vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]</code>","text":"<p>Read YAML file metadata.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Not used.</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>If an expected metadata key is missing.</p> <p>Returns:</p> Type Description <code>dict[str, bool | int | str | datetime | timedelta | tzinfo | None]</code> <p>dict[str, bool|int|str|datetime|timedelta|tzinfo|None]: Metadata dictionary.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_metadata(self, vector_id: str) -&gt; dict[str, bool | int | str | datetime | timedelta | tzinfo | None]:\n    \"\"\"\n    Read YAML file metadata.\n\n    Args:\n        vector_id (str): Not used.\n\n    Raises:\n        KeyError: If an expected metadata key is missing.\n\n    Returns:\n        dict[str, bool|int|str|datetime|timedelta|tzinfo|None]: Metadata dictionary.\n\n    \"\"\"\n    if self._meta is None:\n        raw_meta = self._data[YamlNames.metadata_field][YamlNames.x_field]\n\n        self._meta = self._process_meta(raw_meta)\n    return self._meta\n</code></pre>"},{"location":"reference/#framdata.loaders.time_vector_loaders.NVEYamlTimeVectoroader.get_values","title":"<code>get_values(vector_id: str) -&gt; NDArray</code>","text":"<p>Get values of vector.</p> <p>Parameters:</p> Name Type Description Default <code>vector_id</code> <code>str</code> <p>Unique id of the curve in the Loader source.</p> required <p>Returns:</p> Name Type Description <code>NDArray</code> <code>NDArray</code> <p>Numpy array with values of vector.</p> Source code in <code>framdata/loaders/time_vector_loaders.py</code> <pre><code>def get_values(self, vector_id: str) -&gt; NDArray:\n    \"\"\"\n    Get values of vector.\n\n    Args:\n        vector_id (str): Unique id of the curve in the Loader source.\n\n    Returns:\n        NDArray: Numpy array with values of vector.\n\n    \"\"\"\n    if self._data is None:\n        self._parse_file()\n    values_list = self._data[vector_id][self._values_label]\n    if len(values_list) == 0:\n        message = f\"Time vector {vector_id} in {self} contains no points.\"\n        raise ValueError(message)\n    return np.asarray(values_list)\n</code></pre>"}]}